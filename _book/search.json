[
  {
    "objectID": "setup.html#finding-your-way",
    "href": "setup.html#finding-your-way",
    "title": "2  Getting Going",
    "section": "2.1 Finding your way",
    "text": "2.1 Finding your way\nEveryone finds their own workflow for coding, depending on their preferred language, editor, how they run their code, and so on. The aim of the sections below is to give a roundup of some popular tools in the Python ecosystem."
  },
  {
    "objectID": "setup.html#your-coding-environment",
    "href": "setup.html#your-coding-environment",
    "title": "2  Getting Going",
    "section": "2.2 Your coding environment",
    "text": "2.2 Your coding environment\nTo run Python code on your computer you will need to have installed the Python language. I recommend the Anaconda distribution as it comes with all the parts of the toolkit we’ll need such as Jupyter notebooks and the major libraries NumPy and SciPy.\nTry running python at the command line. You should get something like\nPython 3.9.12 (main, Apr  5 2022, 01:53:17) \n[Clang 12.0.0 ] :: Anaconda, Inc. on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \nYou should confirm that you are using Python 3 (the command python3 will also work and guarantee this if you happen to have Python 2 as the default). The prompt >>> indicates that you have started the Python interactive shell or REPL and are good to go:\n\nprint(\"Hello world!\")\n1 + 2\n\nHello world!\n\n\n3\n\n\nTo leave and return to the command line, you can run quit() or exit()."
  },
  {
    "objectID": "setup.html#ipython",
    "href": "setup.html#ipython",
    "title": "2  Getting Going",
    "section": "2.3 IPython",
    "text": "2.3 IPython\nIf you ran the above command from within python you may have noticed that the nice colour scheme that you see above was absent. This is called syntax highlighting and provides a visual guide to the syntax of the language.\nIPython is an interactive shell that provides syntax highlighting and much more. If you have installed IPython (it comes with Anaconda) you can start it from the command line with ipython.\nAmong the most helpful features of IPython are:\n\nTab completion: hit tab to autocomplete. This is particularly useful for viewing all properties or methods of an object: \nTyping ?word or word? prints detailed information about an object (?? provides additional detail).\nCertain magic commands prefixed by % that provide certain additional functionality. For example, %timeit finds the executation time of a single line statement, which is useful when profiling the performance of code:\n\n\n%timeit L = [n ** 2 for n in range(1000)]\n\n238 µs ± 10.1 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n%timeit automatically runs several times to give some statistics on the execution time. For multiple lines you can use the %%timeit magic.\nYou can find much more exploring the documentation."
  },
  {
    "objectID": "setup.html#running-a-python-program",
    "href": "setup.html#running-a-python-program",
    "title": "2  Getting Going",
    "section": "2.4 Running a Python program",
    "text": "2.4 Running a Python program\nPython code in a file with a .py extension can be run from the command line with python hello_world.py or python -m hello_world. In the latter case the -m option tells the interpreter to look for a module called hello_world. More on modules below.\nFrom the IPython shell you can instead use run hello_world.py or just run hello_world.\nTODO: These magics are normally documented with a %. When is it necessary?"
  },
  {
    "objectID": "setup.html#importing-code",
    "href": "setup.html#importing-code",
    "title": "2  Getting Going",
    "section": "2.5 Importing code",
    "text": "2.5 Importing code\nA Python module is just a file containing definition and statements. Breaking long code into modules is good practice for writing clear and reusable software. Users may not want to delve into the details of some function you have written in order to be able to us it, and separating the corresponding code into a separate file is a hygienic way to handle this.\nThus if I make the file hello_world.py containing the function:\n\ndef hello():\n    print(\"Hello world!\")\n\nI can run this function by first importing the module:\n\nimport hello_world\nhello_world.hello()\n\nHello world!\n\n\nNotice that the function hello is accessed from the hello_world namespace. This is to avoid any confusion that may arise if more that one imported module has a function of the same name. If you are confident that’s not an issue and want more concise code you can do this:\n\nfrom hello_world import hello\nhello()\n\nHello world!\n\n\nor even:\n\nfrom hello_world import *\nhello()\n\nHello world!\n\n\nThe issue with the latter is that it may introduce a whole bunch of names that may interfere with things you already defined.\nA collection of modules in a folder is called a package. You can import a package in the same way and access all the modules using the same . notation i.e. package.module1, package.module2, etc..\nSince explicit namespaces are preferred to avoid ambiguity it’s common to introduce shorthand names for the package or module you are importing, hence the ubiquitous:\n\nimport numpy as np\nnp.arange(10)\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n(You can call it what you like, of course!)\nFor details about where the interpreter looks to find modules you try to import are in the documentation."
  },
  {
    "objectID": "setup.html#installing-libraries",
    "href": "setup.html#installing-libraries",
    "title": "2  Getting Going",
    "section": "2.6 Installing libraries",
    "text": "2.6 Installing libraries\n99% of the code 1 you run will have been written by somebody else in the form of a library (a collection of modules or packages). Package installation is handled by the command line utilities pip or conda, the latter being the package manager for the Anaconda distribution. If you have NumPy and SciPy installed you won’t need to worry about this too much in this course."
  },
  {
    "objectID": "setup.html#editors",
    "href": "setup.html#editors",
    "title": "2  Getting Going",
    "section": "2.7 Editors",
    "text": "2.7 Editors\nModern editors come with a huge number of tools that make writing code much easier, and you would be crazy not to take advantage of them. These range from the visual cues provided by syntax highlighting – which we’ve already met – to code completion, parameter information and documentation popups as you type. These go under the general heading IntelliSense. The latest hotness is GitHub Copilot, which uses AI to make code suggestions. In my view, these are all part of a continuum of productivity enhancements that enable people to write better code faster. Use them (wisely).\nI use Visual Studio Code."
  },
  {
    "objectID": "setup.html#notebooks",
    "href": "setup.html#notebooks",
    "title": "2  Getting Going",
    "section": "2.8 Notebooks",
    "text": "2.8 Notebooks\nWhile software developers write .py files, modules and packages, scientists and others doing more exploratory work tend to favour a Notebook format that mixes code, text, and plots. The dominant option is the Jupyter notebook, which comes with the Anaconda distribution and can be started from the command line with jupyter notebook (or from the Anaconda Navigator application). This will open the notebook as a web page in your browser, where it can be edited and saved. The default extension is .ipynb.\nJupyter notebooks can actually run code in different languages (the processes running a particular language is called a kernel), but the default process is IPython with all the benefits described above.\nThe text cells can be formatted using Markdown and also support \\(\\LaTeX\\) equations, which is pretty handy for us.\nGoogle has their own cloud version of the Jupyter notebook called Colab. You can try it out for free, though you have to pay for significant compute. The “next generation” of the Jupyter notebook is called JupyterLab and can be started with jupyter lab. Notebook files can be opened in either Jupyter Lab or Jupyter Notebook"
  },
  {
    "objectID": "setup.html#codespaces",
    "href": "setup.html#codespaces",
    "title": "2  Getting Going",
    "section": "2.9 Codespaces",
    "text": "2.9 Codespaces\nNew from Github…"
  },
  {
    "objectID": "complexity.html#first-example-multiplication",
    "href": "complexity.html#first-example-multiplication",
    "title": "8  Algorithms and computational complexity",
    "section": "8.1 First example: multiplication",
    "text": "8.1 First example: multiplication\nHow hard is it to multiply numbers? The bigger they are, the harder it is, as you well know. You also know that computers are very good at multiplying, so once you’ve switched from multiplying numbers yourself to multiplying them on a computer, you may well be tempted to forget about how hard it is. Nevertheless, computers find big numbers harder than small numbers. How much harder?\nIf you remember how you learnt to multiply numbers at school, it probably went something like this: 1\n\nMultiplying two 3 digit numbers\n\n\n\n\n\n\n\n\n\n\n×\n1\n3\n2\n2\n3\n1\n\n\n\n\n_\n_\n3\n_\n2\n6\n1\n4\n9\n2\n6\n3\n\n\n3\n9\n4\n8\n3\n\n\n\nFor \\(n\\) digits we have to perform \\(n^2\\) single digit multiplications. We then have to add together the \\(n\\) resulting \\(n\\)-digit numbers. This is another \\(n^2\\) operations. Thus the overall number of operations is proportional to \\(n^2\\): doubling the number of digits will make the problem four times harder.\nExactly how long this takes to perform in your head or on a computer will depend on many things, such as how long it takes you to multiply two digits, or get the previous values out of memory (or read them of the page), but you can’t get away from the basic quadratic scaling law of this algorithm."
  },
  {
    "objectID": "complexity.html#defining-complexity",
    "href": "complexity.html#defining-complexity",
    "title": "8  Algorithms and computational complexity",
    "section": "8.2 Defining complexity",
    "text": "8.2 Defining complexity\nWhen computer scientists talk about the complexity of a problem, this question of scaling of the number of steps involved is what they have in mind. The complexity of any particular task (or calculation) may vary considerably — evaluating \\(100\\times 100\\) is considerably easier than the general case, for example — so instead we ask about how a particular general algorithm performs on a class of tasks. Such a class is what computer scientists mean when they talk about a problem: multiplication of \\(n\\) digit numbers is a problem, and any particular pair of \\(n\\) digit numbers represents an instance of that problem. What we discussed above is a particular algorithm for multiplication that has quadratic complexity, or “\\(O(n^2)\\) complexity” (say “order \\(n\\) squared”).\nThis description only keeps track of how the difficulty scales with the size of the problem. There are various reasons why this level of detail is important:\n\nIt allows us to gloss over what exactly we mean by a step. Are we working in base ten or binary? Looking the digit multiplications up in a table or doing them from scratch?\nLikewise we don’t have to worry about how the algorithm is implemented exactly in software or hardware, what language we use, and so on.\nInevitably, we always want to look at harder and harder problems with bigger and bigger \\(n\\) (whatever \\(n\\) means for the problem at hand). If a simulation finishes for a system of length 10 we immediately want to run it again for length 20, and so on. It then becomes important to know whether our code is going to run for twice as long, four times as long, or \\(2^{10}\\) times as long (exponential scaling).\n\n\n8.2.1 Best / worst / average\nEven when we focus on a problem in the above sense we still have to be careful in defining the complexity of an algorithm. In general we can characterize three complexities: best case, worse case, and average case. To see the difference between these three consider search, the very simple problem of finding an item in an (unordered) list of length \\(n\\). How hard is this? You have to check every item until you find the one you are looking for, so this suggests the complexity is \\(O(n)\\). You could be lucky and get it first try, however, or within the first ten tries. This means the best case complexity of search is \\(O(1)\\): it doesn’t increase with the size of the problem. The worst thing that could happen is that the sought item is last: the worst case complexity is \\(O(n)\\). On average, you’ll find your item in the middle of the list on attempt \\(\\sim n/2\\), so the average case complexity is \\(O(n/2)\\). But this is the same as \\(O(n)\\) (constants don’t matter)\nThus for linear search we have:\n\n\n\n\nComplexity\n\n\n\n\nBest case\n\\(O(1)\\)\n\n\nWorst case\n\\(O(n)\\)\n\n\nAverage case\n\\(O(n)\\)\n\n\n\nWe can check the average case performance experimentally by using randomly chosen lists: 2\n\ndef linear_search(x, val):\n    \"Return True if val is in x, otherwise return False\"\n    for item in x:\n        if item == val:\n            return True\n    return False\n\n\nimport numpy as np\n# Create array of problem sizes n we want to test (powers of 2)\nN = 2**np.arange(2, 20)\n\n# Generate the array of integers for the largest problem to use in plotting times\nx = np.arange(N[-1])\n\n# Initialise an empty array to stores times for plotting\ntimes = []\n\n# Time the search for each problem size\nfor n in N:\n\n    # Time search function (repeating 3 times) to find a random integer in x[:n]\n    t = %timeit -q -n4 -r1 -o linear_search(x[:n], np.random.randint(0, n))\n\n    # Store best case time (best on a randomly chosen problem)\n    times.append(t.best)\n\n\n\nCode for plot\nimport matplotlib.pyplot as plt\n# Plot and label the time taken for linear search\nplt.loglog(N, times, marker='o')\nplt.xlabel('$n$')\nplt.ylabel('$t$ (s)')\n\n# Show a reference line of O(n)\nplt.loglog(N, 1e-6*N, label='$O(n)$')\n\n# Add legend\nplt.legend(loc=0)\nplt.title(\"Experimental complexity of linear search\")\n\nplt.show()\n\n\n\n\n\nThe “experimental noise” in these plots arises because we don’t have full control over exactly what our computer is doing at any moment: there are lots of other processes running. Also, it takes a while to reach the linear regime: there is an overhead associated with starting the program that represents a smaller fraction of the overall run time as \\(n\\) increases.\n\n\n8.2.2 Polynomial complexity\nSince you’ve already learnt a lot of algorithms in mathematics (even if you don’t think of them this way) it’s very instructive to revisit them through the lens of computational complexity.\nMultiplying a vector by a matrix?\n$$\n$$\nmatrix vector\n\n\n8.2.3 Better than linear?\nIt seems obvious that for search you can’t do better than linear: you have to look at roughly half the items before you should expect to find the one you’re looking for 3. What if the list is ordered? Any order will do: numerical for numbers, or lexicographic for strings. This extra structure allows us to use an algorithm called binary search that you may have seen before. The idea is pretty intuitive: look in the middle of the list and see if the item you seek should be in the top half or bottom half. Take the relevant half and divide it in half again to determine which quarter of the list your item is in, and so on. Here’s how it looks in code:\n\ndef binary_search(x, val):\n    \"\"\"Peform binary search on x to find val. If found returns position, otherwise returns None.\"\"\"\n\n    # Intialise end point indices\n    lower, upper = 0, len(x) - 1\n\n    # If values is outside of interval, return None \n    if val < x[lower] or val > x[upper]:\n        return None\n\n    # Perform binary search\n    while True:\n                \n        # Compute midpoint index (integer division)\n        midpoint = (upper + lower)//2\n\n        # Check which side of x[midpoint] val lies, and update midpoint accordingly\n        if val < x[midpoint]:\n            upper = midpoint - 1\n        elif val > x[midpoint]:\n            lower = midpoint + 1\n        elif val == x[midpoint]:  # found, so return\n            return midpoint\n       \n        # In this case val is not in list (return None)\n        if upper < lower:\n            return None\n\nAnd here’s the performance\n\n\nCode for plot\n# Create array of problem sizes we want to test (powers of 2)\nN = 2**np.arange(2, 24)\n\n# Creat array and sort\nx = np.arange(N[-1])\nx = np.sort(x)\n\n# Initlise an empty array to capture time taken\ntimes = []\n\n# Time search for different problem sizes\nfor n in N:\n    # Time search function for finding '2'\n    t = %timeit -q -n5 -r2 -o binary_search(x[:n], 2)\n\n    # Store average\n    times.append(t.best)\n\n# Plot and label the time taken for binary search\nplt.semilogx(N, times, marker='o')\nplt.xlabel('$n$')\nplt.ylabel('$t$ (s)')\n\n# Change format on y-axis to scientific notation\nplt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\nplt.title(\"Experimental complexity of binary search\")\nplt.show()\n\n\n\n\n\nNote the axes: the plot is linear-log, so the straight line indicates logarithmic growth of complexity. This makes sense: if the length is a power of 2 i.e. \\(n=2^p\\), we are going to need \\(p\\) bisections to locate our value. The complexity is \\(O(\\log n)\\) (we don’t need to specify the base as overall constants don’t matter).\nHere’s another example of logarithm scaling. Exponentiation is the problem of raising a number \\(b\\) (the base) to the \\(n\\)th power. The obvious way is to multiply the number by itself \\(n\\) times. Linear scaling, right? But there’s a quicker way, since\n\\[\n\\begin{align}\nb^2 &= b\\cdot b\\\\\nb^4 &= b^2\\cdot b^2\\\\\nb^4 &= b^4\\cdot b^4\n\\end{align}\n\\]\nWe only have to do three multiplications! Exponentiation by this method is \\(O(\\log n)\\). To handle powers that aren’t a power of \\(2\\), we do the following\n\\[\nb^n = \\begin{cases}\n    b^{n/2} \\cdot b^{n/2} & \\text{if $n$ even} \\\\\n    b \\cdot b^{n-1} & \\text{if $n$ odd}\n\\end{cases}\n\\]\nHere’s a way to implement this in code.\n\ndef exp(b, n):\n    if n == 0:\n        return 1\n    elif n % 2 == 0:\n        return exp(b, n // 2)**2\n    else:\n        return b * exp(b, n - 1) \n\nexp(2, 6)\n\n64\n\n\nThis implementation is recursive: the function exp(b, n) calls itself. If this seems a bit self-referential, notice that it only calls itself with lower values of the exponent \\(n\\). This process continues until we hit \\(n=0\\), and 1 is returned by the first part of the if ... else. Any recursive function has to have such a base case to avoid an infinite regress. You’ll know if you haven’t provided one correctly:\n\ndef exp_no_base_case(b, n):\n    if n % 2 == 0:\n        return exp_no_base_case(b, n // 2)**2\n    else:\n        return b * exp_no_base_case(b, n - 1) \n\nexp_no_base_case(2, 6)\n\nRecursionError: maximum recursion depth exceeded in comparison\n\n\nOne interesting thing about exponentiation is that while it can be done efficiently, the inverse — finding the logarithm — cannot. To make this more precise one has to work with modular arithmetic i.e. do all operations modulo some number \\(m\\). Then for \\(b, y=0,\\ldots m-1\\) we are guaranteed that there is some number \\(x\\) such that \\(b^x=y\\) (this is called the discrete logarithm). Finding this number is hard: there is no known method for computing it efficiently. Certain public-key cryptosystems are based on the difficulty of the discrete log (for carefully chosen \\(b\\), \\(m\\) and \\(y\\)).\n\n\n8.2.4 Exponential complexity\nWhile we’re on the subject of recursion, here’s an example that’s often used to introduce the topic: calculating the Fibonacci numbers. Remember that the Fibonacci numbers are this sequence\n\\[\n0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233 ...\n\\]\nwhere each new term is obtained by adding together the previous two\n\\[\n\\text{Fib}(n) = \\text{Fib}(n-1) + \\text{Fib}(n-2)\n\\]\nThe fact that the value \\(\\text{Fib}(n)\\) is defined in terms of lower values of \\(n\\) makes a recursive definition possible\n\ndef fib(n):\n    if n == 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fib(n - 1) + fib(n - 2)\n\nfib(13)\n\n233\n\n\nThe first two terms are the base cases (we need two because the recursion refers to two earlier values). While this looks quite cool it’s actually a terrible way o f calculating \\(\\text{Fib}(n)\\). Look at the picture below which illustrates the function calls that are made during the evaluation of \\(\\text{Fib}(5)\\). There are huge amounts of duplication!\n\n\n\nThe recursive tree for calculating Fibonacci numbers. Source: SICP\n\n\nThe complexity of this algorithm actually grows exponentially with \\(n\\). Because of the branching structure the algorithm is \\(O(2^n)\\). Calculating the Fibonacci number the sensible way (i.e. the way you do it in your head) gives an \\(O(n)\\) algorithm.\nIt would be nice if exponential complexity were only ever the result of poor choices of algorithm. Unfortunately, this is not the case. It’s possible to come up with problems that definitely can’t be solved faster than exponentially: the Towers of Hanoi is one famous example. Closer to the hearts of physicists, the simulation of a quantum system with \\(n\\) qubits (a qubit — or _quantum bit — is just a computer sciencey word for a spin-1/2) is believed to have complexity \\(O(2^n)\\), which is a big part of the hype surrounding quantum computers.\nThere are problems whose solution, once found, is easy to check it is correct. The discrete logarithm we mentioned above is one example. Checking involves exponentiation, and exponentiation is \\(O(\\log n)\\) in the size of the numbers, or \\(O(n)\\) in the number of digits. The question of whether efficient (i.e. polynomial) algorithms always exist for problems which are easy to check is in fact the outstanding problem in computer science: it’s called P vs NP, where P is the class of problems with polynomial time algorithms and NP is the class whose solution can be checked in polynomial time. The question is: are these two classes the same or do they differ? I think it’s fair to say that most people with an opinion on the matter think they differ, but the proof is lacking…"
  },
  {
    "objectID": "complexity.html#from-quadratic-to-linear",
    "href": "complexity.html#from-quadratic-to-linear",
    "title": "8  Algorithms and computational complexity",
    "section": "8.3 From quadratic to linear",
    "text": "8.3 From quadratic to linear\nOf course, it is still worth\nExample of checking pairs\nA major goal in algorithm design"
  },
  {
    "objectID": "complexity.html#examples-of-different-complexities",
    "href": "complexity.html#examples-of-different-complexities",
    "title": "8  Algorithms and computational complexity",
    "section": "8.3 Examples of different complexities",
    "text": "8.3 Examples of different complexities\nA major goal in algorithm design\nSummary of asymptotic notation\nXKCD plots"
  },
  {
    "objectID": "complexity.html#space-vs.-time-complexity",
    "href": "complexity.html#space-vs.-time-complexity",
    "title": "8  Algorithms and computational complexity",
    "section": "8.5 Space vs. time complexity",
    "text": "8.5 Space vs. time complexity\n\n\n\n\nKaratsuba, Anatolii Alexeevich. 1995. “The Complexity of Computations.” Proceedings of the Steklov Institute of Mathematics-Interperiodica Translation 211: 169–83."
  },
  {
    "objectID": "complexity.html#back-to-multiplication-divide-and-conquer",
    "href": "complexity.html#back-to-multiplication-divide-and-conquer",
    "title": "8  Algorithms and computational complexity",
    "section": "8.4 Back to multiplication: divide and conquer",
    "text": "8.4 Back to multiplication: divide and conquer\nKaratsuba algorithm nice story\nKaratsuba’s account\nKaratsuba (1995) contains a nice account of the discovery of his algorithm (online version)\nKaratsuba algo\nBig O\nVisualization of sorting algorithms\nSimple example from Leetcode\nAnalaysis of algorithms\nExample of finding a unique item in list\nNice examples from Garth Wells\nhttps://github.com/CambridgeEngineering/PartIA-Computing-Michaelmas/blob/main/11%20Complexity.ipynb\nExamples of multiplication\nBreadth first and depth first\nImportance of choosing a data structure to match algorithm\nExamples: queue in Wolff. Was there a Numpy-ish way to do this faster? Priority queue in waiting time algo\nFFT uses\nhttps://en.wikipedia.org/wiki/Orthogonal_frequency-division_multiplexing\nNeedleman-Wunsch\nExamples\n\nMultiplication Karatsuba\nBinary search\nLinear algebra\nSorting\nFFT\nTaking powers (SICP). What is \\(n\\) in this case?\nEuclidean algorithm (GCD) (SICP)\n\nReferences. Nature of computation, grokking algos\nInsertion in a list etc."
  }
]