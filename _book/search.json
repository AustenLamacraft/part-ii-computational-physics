[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Part II Computational Physics",
    "section": "",
    "text": "Preface\nThese are the materials for the Part II Physics course Computational Physics, taught in Lent Term 2023 at the University of Cambridge."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Not a course on numerical analysis but computational physics\nComputation is important for experimental physicists for analysing data. For theoretical physics, computation is used to deal with the awkward fact that physical theories are generally not tractable. You can’t solve Maxwell’s equations, the Navier–Stokes equation, or Schrödinger’s equation in any but the simplest situations.\nTo be blunt, this means that your knowledge of physics, while very nice, is of no use whatsoever unless you can write a program to solve more complicated problems. Sorry.\nIt’s important to understand that this need to apply our mathematical descriptions of nature in more general settings was the principal driving force behind the invention of the computer.\nMore than this,\nChurch Turing hypothesis\nTuring’s cathedral\nBooks\nPart IB notes are very good\nGarth Wells\nhttps://github.com/CambridgeEngineering/PartIA-Computing-Michaelmas/\nRougier\nhttps://www.labri.fr/perso/nrougier/from-python-to-numpy/ https://github.com/rougier/scientific-visualization-book"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "12  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Jacobs, Laurence, and Claudio Rebbi. 1981. “Multi-Spin Coding: A\nVery Efficient Technique for Monte Carlo Simulations of Spin\nSystems.” Journal of Computational Physics 41 (1):\n203–10.\n\n\nKrauth, Werner. 1998. “Introduction to Monte Carlo\nAlgorithms.” In Advances in Computer Simulation, 1–35.\nSpringer.\n\n\n———. 2006. Statistical Mechanics: Algorithms and Computations.\nVol. 13. OUP Oxford.\n\n\nMacKay, David JC. 2003. Information Theory, Inference and Learning\nAlgorithms. Cambridge university press.\n\n\nMetropolis, Nicholas, Arianna W Rosenbluth, Marshall N Rosenbluth,\nAugusta H Teller, and Edward Teller. 1953. “Equation of State\nCalculations by Fast Computing Machines.” The Journal of\nChemical Physics 21 (6): 1087–92.\n\n\nWidom, B. 1966. “Random Sequential Addition of Hard Spheres to a\nVolume.” The Journal of Chemical Physics 44 (10):\n3888–94."
  },
  {
    "objectID": "setup.html#setting-up-an-environment",
    "href": "setup.html#setting-up-an-environment",
    "title": "2  Getting Going",
    "section": "2.1 Setting up an environment",
    "text": "2.1 Setting up an environment\nYou will need\n\nprint(\"Hello world!\")\n1 + 2\n\nHello world!\n\n\n3\n\n\nPython version. Make sure it’s Python3\nIPython code highlighting, help and so on.\nRunning a python program"
  },
  {
    "objectID": "setup.html#notebooks",
    "href": "setup.html#notebooks",
    "title": "2  Getting Going",
    "section": "2.8 Notebooks",
    "text": "2.8 Notebooks\nWhile software developers write .py files, modules and packages, scientists and others doing more exploratory work tend to favour a Notebook format that mixes code, text, and plots. The dominant option is the Jupyter notebook, which comes with the Anaconda distribution and can be started from the command line with jupyter notebook (or from the Anaconda Navigator application). This will open the notebook as a web page in your browser, where it can be edited and saved. The default extension is .ipynb.\nJupyter notebooks can actually run code in different languages (the processes running a particular language is called a kernel), but the default process is IPython with all the benefits described above.\nThe text cells can be formatted using Markdown and also support \\(\\LaTeX\\) equations, which is pretty handy for us.\nGoogle has their own cloud version of the Jupyter notebook called Colab. You can try it out for free, though you have to pay for significant compute. The “next generation” of the Jupyter notebook is called JupyterLab and can be started with jupyter lab. Notebook files can be opened in either Jupyter Lab or Jupyter Notebook"
  },
  {
    "objectID": "setup.html#plotting",
    "href": "setup.html#plotting",
    "title": "2  Getting Going",
    "section": "2.9 Plotting",
    "text": "2.9 Plotting\nWithout introducing numpy explicitly at this stage…\n\\(\\alpha = \\beta\\)"
  },
  {
    "objectID": "numpy.html",
    "href": "numpy.html",
    "title": "4  Introduction to NumPy",
    "section": "",
    "text": "NumPy is the key building block of the Python scientific ecosystem\nUfuncs"
  },
  {
    "objectID": "setup.html#the-python-language",
    "href": "setup.html#the-python-language",
    "title": "2  Getting Going",
    "section": "2.10 The Python Language",
    "text": "2.10 The Python Language\nObjects\ntwo language problem\nGotchas"
  },
  {
    "objectID": "setup.html#your-coding-environment",
    "href": "setup.html#your-coding-environment",
    "title": "2  Getting Going",
    "section": "2.2 Your coding environment",
    "text": "2.2 Your coding environment\nTo run Python code on your computer you will need to have installed the Python language. I recommend the Anaconda distribution as it comes with all the parts of the toolkit we’ll need such as Jupyter notebooks and the major libraries NumPy and SciPy.\nTry running python at the command line. You should get something like\nPython 3.9.12 (main, Apr  5 2022, 01:53:17) \n[Clang 12.0.0 ] :: Anaconda, Inc. on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \nYou should confirm that you are using Python 3 (the command python3 will also work and guarantee this if you happen to have Python 2 as the default). The prompt >>> indicates that you have started the Python interactive shell or REPL and are good to go:\n\nprint(\"Hello world!\")\n1 + 2\n\nHello world!\n\n\n3\n\n\nTo leave and return to the command line, you can run quit() or exit()."
  },
  {
    "objectID": "setup.html#editors",
    "href": "setup.html#editors",
    "title": "2  Getting Going",
    "section": "2.7 Editors",
    "text": "2.7 Editors\nModern editors come with a huge number of tools that make writing code much easier, and you would be crazy not to take advantage of them. These range from the visual cues provided by syntax highlighting – which we’ve already met – to code completion, parameter information and documentation popups as you type. These go under the general heading IntelliSense. The latest hotness is GitHub Copilot, which uses AI to make code suggestions. In my view, these are all part of a continuum of productivity enhancements that enable people to write better code faster. Use them (wisely).\nI use Visual Studio Code."
  },
  {
    "objectID": "setup.html#installing-libraries",
    "href": "setup.html#installing-libraries",
    "title": "2  Getting Going",
    "section": "2.6 Installing libraries",
    "text": "2.6 Installing libraries\n99% of the code 1 you run will have been written by somebody else in the form of a library (a collection of modules or packages). Package installation is handled by the command line utilities pip or conda, the latter being the package manager for the Anaconda distribution. If you have NumPy and SciPy installed you won’t need to worry about this too much in this course."
  },
  {
    "objectID": "setup.html#running-a-python-program",
    "href": "setup.html#running-a-python-program",
    "title": "2  Getting Going",
    "section": "2.4 Running a Python program",
    "text": "2.4 Running a Python program\nPython code in a file with a .py extension can be run from the command line with python hello_world.py or python -m hello_world. In the latter case the -m option tells the interpreter to look for a module called hello_world. More on modules below.\nFrom the IPython shell you can instead use run hello_world.py or just run hello_world.\nTODO: These magics are normally documented with a %. When is it necessary?"
  },
  {
    "objectID": "setup.html#importing-code",
    "href": "setup.html#importing-code",
    "title": "2  Getting Going",
    "section": "2.5 Importing code",
    "text": "2.5 Importing code\nA Python module is just a file containing definition and statements. Breaking long code into modules is good practice for writing clear and reusable software. Users may not want to delve into the details of some function you have written in order to be able to us it, and separating the corresponding code into a separate file is a hygienic way to handle this.\nThus if I make the file hello_world.py containing the function:\n\ndef hello():\n    print(\"Hello world!\")\n\nI can run this function by first importing the module:\n\nimport hello_world\nhello_world.hello()\n\nHello world!\n\n\nNotice that the function hello is accessed from the hello_world namespace. This is to avoid any confusion that may arise if more that one imported module has a function of the same name. If you are confident that’s not an issue and want more concise code you can do this:\n\nfrom hello_world import hello\nhello()\n\nHello world!\n\n\nor even:\n\nfrom hello_world import *\nhello()\n\nHello world!\n\n\nThe issue with the latter is that it may introduce a whole bunch of names that may interfere with things you already defined.\nA collection of modules in a folder is called a package. You can import a package in the same way and access all the modules using the same . notation i.e. package.module1, package.module2, etc..\nSince explicit namespaces are preferred to avoid ambiguity it’s common to introduce shorthand names for the package or module you are importing, hence the ubiquitous:\n\nimport numpy as np\nnp.arange(10)\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n(You can call it what you like, of course!)\nFor details about where the interpreter looks to find modules you try to import are in the documentation."
  },
  {
    "objectID": "setup.html#finding-your-way",
    "href": "setup.html#finding-your-way",
    "title": "2  Getting Going",
    "section": "2.1 Finding your way",
    "text": "2.1 Finding your way\nEveryone finds their own workflow for coding, depending on their preferred language, editor, how they run their code, and so on. The aim of the sections below is to give a roundup of some popular tools in the Python ecosystem."
  },
  {
    "objectID": "setup.html#codespaces",
    "href": "setup.html#codespaces",
    "title": "2  Getting Going",
    "section": "2.9 Codespaces",
    "text": "2.9 Codespaces\nNew from Github…"
  },
  {
    "objectID": "setup.html#ipython",
    "href": "setup.html#ipython",
    "title": "2  Getting Going",
    "section": "2.3 IPython",
    "text": "2.3 IPython\nIf you ran the above command from within python you may have noticed that the nice colour scheme that you see above was absent. This is called syntax highlighting and provides a visual guide to the syntax of the language.\nIPython is an interactive shell that provides syntax highlighting and much more. If you have installed IPython (it comes with Anaconda) you can start it from the command line with ipython.\nAmong the most helpful features of IPython are:\n\nTab completion: hit tab to autocomplete. This is particularly useful for viewing all properties or methods of an object: \nTyping ?word or word? prints detailed information about an object (?? provides additional detail).\nCertain magic commands prefixed by % that provide certain additional functionality. For example, %timeit finds the executation time of a single line statement, which is useful when profiling the performance of code:\n\n\n%timeit L = [n ** 2 for n in range(1000)]\n\n227 µs ± 3.97 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n%timeit automatically runs several times to give some statistics on the execution time. For multiple lines you can use the %%timeit magic.\nYou can find much more exploring the documentation."
  },
  {
    "objectID": "numbers.html",
    "href": "numbers.html",
    "title": "5  Floating point and all that",
    "section": "",
    "text": "Include RNGs here?\nSince physics is all about numbers we had better develop some understanding of how computers represent numbers, and what limitations this representations has.\nAs a motivating example\n\n0.1  + 0.2 == 0.3\n\nFalse"
  },
  {
    "objectID": "numpy.html#plotting",
    "href": "numpy.html#plotting",
    "title": "5  Introduction to NumPy and friends",
    "section": "5.4 Plotting",
    "text": "5.4 Plotting\nAnimation"
  },
  {
    "objectID": "numpy.html#dealing-with-data",
    "href": "numpy.html#dealing-with-data",
    "title": "5  Introduction to NumPy and friends",
    "section": "5.5 Dealing with data",
    "text": "5.5 Dealing with data\nSaving etc…."
  },
  {
    "objectID": "numpy.html#arrays",
    "href": "numpy.html#arrays",
    "title": "4  NumPy and friends",
    "section": "4.2 Arrays",
    "text": "4.2 Arrays\nThe fundamental object in NumPy is the Array, which you can think of as a multidimensional version of a list. Let’s start with two dimensions to demonstrate:\n\nimport numpy as np\nmy_array = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n\n\ntype(my_array)\n\nnumpy.ndarray\n\n\nArrays can be indexed, similar to lists\n\nprint(my_array[0], my_array[1], my_array[3][1])\n\n[1 2 3] [4 5 6] 11\n\n\nbut – different from a ordinary list of lists – the last one can be much more pleasantly achieved with the syntax\n\nmy_array[3,1]\n\n11\n\n\nWe also have a generalization of the slice syntax\n\nmy_array[1:, 1:]\n\narray([[ 5,  6],\n       [ 8,  9],\n       [11, 12]])\n\n\nSlicing can be mixed with integer indexing\n\nmy_array[1:, 1]\n\narray([ 5,  8, 11])\n\n\nNumPy offers all sorts of fancy indexing options for slicing and dicing your data: see the documentation for details.\nA fundamental property of an array is its shape:\n\n# [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]\nmy_array.shape\n\n(4, 3)\n\n\nThe way to read off the shape of an array is as follows. To begin with you encounter a number of [ corresponding to the rank of the array (two in the above example). You then scan over a number of entries that give the rightmost (innermost) dimension in the shape tuple before closing ] (3 here). After a number of 1D arrays [...] equal to the next innermost dimension (4 here), we have another closing ], and so on.\nIt’s definitely something that will take a bit of time getting used to!\nNotice that slicing does not change the rank of the array\n\nmy_array[1:, 1:].shape\n\n(3, 2)\n\n\nbut integer indexing does\n\nmy_array[1:, 1].shape\n\n(3,)\n\n\nNumPy has lots of methods to create arrays with a given shape and populated in different ways:\n\na = np.zeros((2,2))\nprint(a)\n\nb = np.ones((2,2))\nprint(b)\n\nc = np.full((2,2), 5)\nprint(c)\n\nd = np.random.random((2,2)) # random numbers uniformly in [0.0, 1.0)\nprint(d)\n\n[[0. 0.]\n [0. 0.]]\n[[1. 1.]\n [1. 1.]]\n[[5 5]\n [5 5]]\n[[0.39132314 0.35116356]\n [0.74468628 0.91228837]]\n\n\nThere are also lots of methods to change the shape of arrays, for example\n\nnumpy.reshape to change the shape of an array.\nnumpy.expand_dims to insert new axes of length one.\nnumpy.squeeze (the opposite) to remove new axes of length one.\n\nA NumPy array has a dtype property that gives the datatype. If the array was created from data, this will be inferred\n\nmy_array.dtype\n\ndtype('int64')\n\n\nFunctions that construct arrays also have an optional argument to specify the datatype\n\nmy_float_array = np.array([1,2,3], dtype=np.float64)\nmy_float_array.dtype\n\ndtype('float64')"
  },
  {
    "objectID": "numpy.html#objects-in-python",
    "href": "numpy.html#objects-in-python",
    "title": "5  Introduction to NumPy and friends",
    "section": "5.1 Objects in Python",
    "text": "5.1 Objects in Python\nEverything in Python is an object. For example [1,2,3] is a list:\n\ntype([1,2,3])\n\nlist\n\n\n\ndir([1,2,3])\n\n['__add__',\n '__class__',\n '__class_getitem__',\n '__contains__',\n '__delattr__',\n '__delitem__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__gt__',\n '__hash__',\n '__iadd__',\n '__imul__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__len__',\n '__lt__',\n '__mul__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__reversed__',\n '__rmul__',\n '__setattr__',\n '__setitem__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'append',\n 'clear',\n 'copy',\n 'count',\n 'extend',\n 'index',\n 'insert',\n 'pop',\n 'remove',\n 'reverse',\n 'sort']\n\n\ndunder"
  },
  {
    "objectID": "numpy.html#preamble-objects-in-python",
    "href": "numpy.html#preamble-objects-in-python",
    "title": "4  NumPy and friends",
    "section": "4.1 Preamble: objects in Python",
    "text": "4.1 Preamble: objects in Python\nEverything in Python is an object. For example [1,2,3] is a list:\n\nmy_list = [1, 2, 3]\ntype(my_list)\n\nlist\n\n\nYou can think of an object as a container for properties and methods, the latter being functions associated with the object. Properties and methods are accessed with the . syntax. For example, lists have the append method, which adds an element to the end of the list:\n\nmy_list.append(\"boop\")\nmy_list\n\n[1, 2, 3, 'boop']\n\n\nWith IPython you can see all the available methods by hitting tab:\n\n\n\n\n\n\n\nDunder methods\n\n\n\n\n\nYou can list all of an objects properties and methods using dir:\n\ndir(my_list)\n\n['__add__',\n '__class__',\n '__class_getitem__',\n '__contains__',\n '__delattr__',\n '__delitem__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__gt__',\n '__hash__',\n '__iadd__',\n '__imul__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__len__',\n '__lt__',\n '__mul__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__reversed__',\n '__rmul__',\n '__setattr__',\n '__setitem__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'append',\n 'clear',\n 'copy',\n 'count',\n 'extend',\n 'index',\n 'insert',\n 'pop',\n 'remove',\n 'reverse',\n 'sort']\n\n\nNotice that lots of these are methods have a name sandwiched between double underscores and for this reason are called dunder methods (or magic methods, or just special methods). This is to indicate that they are not to be used by you, but by the Python interpreter to implement certain standard functions that apply to many different classes of objects. For instance, when you write len(my_list) to find the length of my_list Python is actually calling the dunder method my_list.__len__ which does the job of actually finding the length.\n\nmy_list.__len__()\n\n4\n\n\nIn this way the same function (len in this case) can operate on many different objects, an example of what is called polymorphism in object oriented programming."
  },
  {
    "objectID": "numpy.html#mathematical-operations-with-arrays",
    "href": "numpy.html#mathematical-operations-with-arrays",
    "title": "4  NumPy and friends",
    "section": "4.3 Mathematical operations with arrays",
    "text": "4.3 Mathematical operations with arrays\nNow here comes the payoff. On lists, multiplication by an integer concatentates multiple copies\n\n2 * [1, 2, 3]\n\n[1, 2, 3, 1, 2, 3]\n\n\nwhich is sometimes useful. But in numerical applications what we really want is this\n\n2 * np.array([1, 2, 3])\n\narray([2, 4, 6])\n\n\nThis illustrates a general feature of NumPy that all mathematical operations are performed elementwise on arrays!\n\nprint(np.array([1, 2, 3]) + np.array([4, 5, 6]))\nprint(np.array([1, 2, 3])**2)\nprint(np.sqrt(np.array([1, 2, 3])))\n\n[5 7 9]\n[1 4 9]\n[1.         1.41421356 1.73205081]\n\n\nThis avoids the need to write nested loops to perform some operation on each element of some multidimensional data. Of course, the loops are still there, it’s just that NumPy handles them in highly optimized C rather than Python. Code which operates in this way – rather than with explicit loops – is often described as vectorized, and in NumPy-speak vectorized functions are called ufuncs, short for universal functions (you can write your own if you need to). As a basic principle you should never use a Python loop to access your data in NumPy code. Loops may appear at a high level in stepping through time steps in a simulation, for example.\n\n4.3.1 Broadcasting\nVectorization is even more versatile than the above examples might suggest. Broadcasting is a powerful protocol that allows us to combine arrays of different shapes. Thus we can add a number to an array\n\nnp.array([1, 2, 3]) + 2.3\n\narray([3.3, 4.3, 5.3])\n\n\nMore generally, elementwise operations can be performed on two arrays of the same rank if in each dimension the sizes either match or one array has size 1.\n\n# These have shape (2, 3) and (1, 3)\nnp.array([[1, 2, 3], [4, 5, 6]]) + np.array([[4, 3, 2]])\n\narray([[5, 5, 5],\n       [8, 8, 8]])\n\n\nIn fact, we can simplify this last example\n\n# These have shape (2, 3) and (3,)\nnp.array([[1, 2, 3], [4, 5, 6]]) + np.array([4, 3, 2])\n\narray([[5, 5, 5],\n       [8, 8, 8]])\n\n\nBroadcasting two arrays follows these rules:\n\nIf the arrays do not have the same rank, prepend the shape of the lower rank array with 1s until both shapes have the same length.\nThe two arrays are said to be compatible in a dimension if they have the same size in the dimension, or if one of the arrays has size 1 in that dimension.\nThe arrays can be broadcast together if they are compatible in all dimensions. After broadcasting, each array behaves as if it had shape equal to the elementwise maximum of shapes of the two input arrays.\nIn any dimension where one array had size 1 and the other array had size greater than 1, the first array behaves as if it were copied along that dimension.\n\nThe documentation has more detail.\n\n\n4.3.2 Example: playing with images\nNice example of a 2D array?\nTODO"
  },
  {
    "objectID": "numpy.html#plotting-with-matplotlib",
    "href": "numpy.html#plotting-with-matplotlib",
    "title": "4  NumPy and friends",
    "section": "4.4 Plotting with Matplotlib",
    "text": "4.4 Plotting with Matplotlib\nThere are various specialized Python plotting libraries but the entry-level option is the catchily named Matplotlib. The pyplot module provides a plotting system that is similar to MATLAB (I’m told)\n\nimport matplotlib.pyplot as plt\n\nHere’s a simple example of the plot function, used to plot 2D data\n\n# Compute the x and y coordinates for points on a sine curve\nx = np.arange(0, 3 * np.pi, 0.1)\ny = np.sin(x)\n\n# Plot the points using matplotlib\nplt.plot(x, y)\nplt.show()\n\n\n\n\nNote: you must call plt.show() to make graphics appear. Here’s a fancier example with some labelling\n\n# Compute the x and y coordinates for points on sine and cosine curves\nx = np.arange(0, 3 * np.pi, 0.1)\ny_sin = np.sin(x)\ny_cos = np.cos(x)\n\n# Plot the points using matplotlib\nplt.plot(x, y_sin)\nplt.plot(x, y_cos)\nplt.xlabel('x axis label')\nplt.ylabel('y axis label')\nplt.title('Sine and Cosine')\nplt.legend(['Sine', 'Cosine'])\nplt.show()\n\n\n\n\nOften you’ll want to make several related plots and present them together, which can be achieved using the subplot function\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Compute the x and y coordinates for points on sine and cosine curves\nx = np.arange(0, 3 * np.pi, 0.1)\ny_sin = np.sin(x)\ny_cos = np.cos(x)\n\n# Set up a subplot grid that has height 2 and width 1,\n# and set the first such subplot as active.\nplt.subplot(2, 1, 1)\n\n# Make the first plot\nplt.plot(x, y_sin)\nplt.title('Sine')\n\n# Set the second subplot as active, and make the second plot.\nplt.subplot(2, 1, 2)\nplt.plot(x, y_cos)\nplt.title('Cosine')\n\n# Show the figure.\nplt.show()"
  },
  {
    "objectID": "numpy.html#saving-and-loading-data",
    "href": "numpy.html#saving-and-loading-data",
    "title": "4  NumPy and friends",
    "section": "4.5 Saving and loading data",
    "text": "4.5 Saving and loading data\nIn the course of your work you are likely to produce, as well as consume lots of data. While it’s good practice to keep notebooks capable of reproducing any of your analyses, this could be time consuming and resource heavy for larger computations. Thus at some point you’ll probably want to save and load data. For example, after saving the data of a large scale simulation you’d like to load it and perform some analysis.\nNumPy comes with its own save and load functions and associated binary format .npy. The benefit of using these is that after loading you get back a NumPy array ready to be used.\nA related function savez allows several arrays to be saved and then loaded as a dictionary-like object."
  },
  {
    "objectID": "ode.html",
    "href": "ode.html",
    "title": "6  Solving differential equations with SciPy",
    "section": "",
    "text": "Newton’s fundamental discovery, the one which he considered necessary to keep secret and published only in the form of an anagram, consists of the following: Data aequatione quotcunque jluentes quantitae involvente jluxiones invenire et vice versa. In contemporary mathematical language, this means: “It is useful to solve differential equations”.\nVladimir Arnold, Geometrical Methods in the Theory of Ordinary Differential Equations\n\nWhile Arnold (and Newton) are of course right the problem is that solving differential equations is not possible in general. Even the simplest example of a first order ordinary differential equation (ODE) in a single variable\n\\[\n\\frac{dx}{dt} = f(x, t)\n\\tag{6.1}\\]\ncannot be solved for general \\(f(x,t)\\). Of course, formulating a physical (or whatever) system in terms of differential equations represents a nontrivial step on the road to understanding it, but a lot remains to be done.\nNumerical analysis of differential equations is a colossal topic in applied maths and we are barely going to scratch the surface. The important thing is to be able to access existing solvers (and implement your own if necessary) and crucially to understand their limitations.\n\n7 Euler’s method\nThe basic idea behind all ODE solves is to introduce a discretization of the equation and its solution \\(x_j\\equiv x(t_j)\\) at time points \\(t_j = hj\\) for some step size \\(h\\) and \\(j=0, 1, \\ldots\\). The very simplest is called the Euler method and approximates the derivative on the right hand side of Equation 6.1 as\n\\[\n\\frac{dx}{dt} \\approx \\frac{x_{j+1} - x_j}{h}\n\\]\nLorenz attractor following Trebst notebooks\nChaos and discretization\nhttps://stackoverflow.com/questions/60338471/lyapunov-spectrum-for-known-odes-python-3\nThere is already some discussion in Part IB. There is an exercise based on planets\nOf practical value: Experimentally demonstrate order of integration\nFundamental Anagram of Calculus"
  },
  {
    "objectID": "numbers.html#integers",
    "href": "numbers.html#integers",
    "title": "5  Floating point and all that",
    "section": "5.1 Integers",
    "text": "5.1 Integers\nLet’s begin with something simpler\n\n1 + 1 == 2\n\nTrue\n\n\nwhich is a bit more reassuring. Integers can be represented in binary\n\n3 == 0b11\n\nTrue\n\n\nor octal or hexadecimal (with a prefix 0o or 0h). You can get the binary string representing an integer using the bin function\n\nbin(-2)\n\n'-0b10'\n\n\nPython allows for arbitrarily large integers, so there is no possibility of overflow or rounding error\n\n2**100\n\n1267650600228229401496703205376\n\n\nThe only limitation is the memory required to store it.\nNumpy integers are a different story\n\nimport numpy as np\nnp.int64(2**100)\n\nOverflowError: Python int too large to convert to C long\n\n\nSince NumPy is using C the types have to play nicely. The range of integers that can be represented with 32 bit numpy.int32s is \\(\\approx\\pm 2^{31} \\approx \\pm 2.1 × 10^9\\) (one bit is for the sign) and 64 bit numpy.int64s is \\(\\approx\\pm 2^{63} \\approx \\pm 9.2 × 10^{18}\\). Apart from the risk of overflow when working NumPy’s integers there are not other gotchas to worry about."
  },
  {
    "objectID": "numbers.html#floating-point-numbers",
    "href": "numbers.html#floating-point-numbers",
    "title": "5  Floating point and all that",
    "section": "5.2 Floating point numbers",
    "text": "5.2 Floating point numbers\nThe reason why \\(0.1 + 0.2 \\neq 0.3\\) in Python is that specifying a real number exactly would involve an infinite number of bits, so that any finite representation is necessarily approximate.\nThe representation computers use for the reals is called floating point arithmetic. It is essentially a form of scientific notation, in which a significand (it contains the significant figures) is multiplied by an exponent. The name floating point reflects the fact that the number of digits after the decimal point is not fixed (I’m using the base ten terms for convenience)\nThis representation requires the choice of a base, and Python’s floating point numbers use binary. Numbers with finite binary representations therefore behave nicely\n\n0.125 + 0.25 == 0.375\n\nTrue\n\n\nFor decimal numbers to be represented exactly we’d have to use base ten. This can be achieved with the decimal module. Our \\(0.1+0.2\\) example then works as expected\n\nfrom decimal import *\nDecimal('0.1') + Decimal('0.2')\n\nDecimal('0.3')\n\n\nSince there is nothing to single out the decimal representation in physics (as opposed to, say, finance) we won’t have any need for it.\nA specification for floating point numbers must give\n\nA base (or radix) \\(b\\)\nA precision \\(p\\), the number of digits in the significand \\(c\\). Thus \\(0\\leq c \\leq b^{p}-1\\).\nA range of exponents \\(q\\) specifed by \\(\\text{emin}\\) and \\(\\text{emax}\\) with \\(\\text{emin}\\leq q+p-1 \\leq \\text{emax}\\).\n\nIncluding one bit \\(s\\) for the overall sign, a number then has the form \\((-1)^s\\times c \\times b^q\\). The smallest positive nonzero number that can be represented is therefore \\(b^{1 + \\text{emin} - p}\\) (corresponding to the smallest value of the exponent) and the largest is \\(b^{1 + \\text{emax}} - 1\\).\nThe above representation isn’t unique: for some numbers you could make the significand smaller and the exponent bigger. A unique representation is fixed by choosing the exponent to be as small as possible.\nRepresenting numbers smaller than \\(b^{\\text{emin}}\\) involves a loss of precision, as the number of digits in the significand falls below \\(p\\) and the exponent has taken its minimum value . These are called subnormal numbers. For binary floats, if we stick with the normal numbers and a \\(p\\)-bit significand the leading bit will be 1 and so can be dropped from the representation, which then only requires \\(p-1\\) bits.\nThe specification for the floating point numbers used by Python (and many other languages) is contained in the IEEE Standard for Floating Point Arithmetic IEEE 754. The default Python float uses the 64 bit binary64 representation (often called double precision). Here’s how those 64 bits are used\n\n\\(p=53\\) for the significand, encoded in 52 bits\n11 bits for the exponent\n1 bit for the sign\n\nAnother common representation is the 32 bit binary32 (single precision) with\n\n\\(p=24\\) for the significand, encoded in 23 bits\n8 bits for the exponent\n1 bit for the sign\n\n\n5.2.1 Floating point numbers in NumPy\nIf this all a bit theoretical you can just get NumPy’s finfo function to tell all about the machine precision\n\nnp.finfo(np.float64)\n\nfinfo(resolution=1e-15, min=-1.7976931348623157e+308, max=1.7976931348623157e+308, dtype=float64)\n\n\nNote that \\(2^{-52}=2.22\\times 10^{-16}\\) which accounts for the value \\(10^{-15}\\) of the resolution. This can be checked by finding when a number is close enough to treated as 1.0.\n\nx=1.0\nwhile 1.0 + x != 1.0:\n    x /= 1.01 \nprint(x)\n\n1.099427563084686e-16\n\n\nFor binary32 we have a resolution of \\(10^{-6}\\).\n\nnp.finfo(np.float32)\n\nfinfo(resolution=1e-06, min=-3.4028235e+38, max=3.4028235e+38, dtype=float32)\n\n\nOne lesson from this is that taking small differences between numbers is a potential source of rounding error, as in this somewhat mean exam question\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSolution: \\(x-x'=x(1-\\gamma^{-1})\\sim x\\beta^2/2\\sim 4.2\\text{mm}\\).\n\nimport numpy as np\nfrom scipy.constants import c\nbeta = 384400e3 / (76 * 3600) / c\ngamma = 1/np.sqrt(1 - beta**2)\nprint(1 - np.float32(1/gamma), 1 - np.float64(1/gamma))\n\n0.0 1.0981660025777273e-11\n\n\n\n\n\n\n\n5.2.2 The dreaded NaN\nAs well as a floating point system, IEEE 754 defines Infinity and NaN (Not a Number)\n\nnp.array([1, -1, 0]) / 0\n\n/var/folders/jd/_f7hc0516qb6dybrb414xjhw0000gn/T/ipykernel_5404/2604490398.py:1: RuntimeWarning:\n\ndivide by zero encountered in true_divide\n\n/var/folders/jd/_f7hc0516qb6dybrb414xjhw0000gn/T/ipykernel_5404/2604490398.py:1: RuntimeWarning:\n\ninvalid value encountered in true_divide\n\n\n\narray([ inf, -inf,  nan])\n\n\nThey behave as you might guess\n\n2 * np.inf, 0 * np.inf, np.inf > np.nan\n\n(inf, nan, False)\n\n\nNaNs propagate through subsequent operations\n\n2 * np.nan\n\nnan\n\n\nwhich means that if you get a NaN somewhere in your calculation, you’ll probably end up seeing it somewhere in the output (which is the idea)."
  },
  {
    "objectID": "random.html",
    "href": "random.html",
    "title": "7  Random algorithms",
    "section": "",
    "text": "Simplest examples…"
  },
  {
    "objectID": "linear.html",
    "href": "linear.html",
    "title": "9  Linear algebra",
    "section": "",
    "text": "Krylov subspaces\nSVD and quantum mechanics. Quantum entanglement.\nImage compression using SVD\nhttp://timbaumann.info/svd-image-compression-demo/\nTrebst has nice Einstein example\nPCA for big data"
  },
  {
    "objectID": "complexity.html",
    "href": "complexity.html",
    "title": "8  Algorithms and computational complexity",
    "section": "",
    "text": "Big O\nSimple example from Leetcode\nAnalaysis of algorithms\nExample of finding a unique item in list\nNice examples from Garth Wells\nhttps://github.com/CambridgeEngineering/PartIA-Computing-Michaelmas/blob/main/11%20Complexity.ipynb\nExamples of multiplication\nBreadth first and depth first\nImportance of choosing a data structure to match algorithm\nExamples: queue in Wolff. Was there a Numpy-ish way to do this faster? Priority queue in waiting time algo\nFFT uses\nhttps://en.wikipedia.org/wiki/Orthogonal_frequency-division_multiplexing\nNeedleman-Wunsch\nExamples\n\nMultiplication Karatsuba\nBinary search\nLinear algebra\nSorting\nFFT\nTaking powers (SICP)\nEuclidean algorithm (GCD) (SICP)"
  },
  {
    "objectID": "numbers.html#practical-matters",
    "href": "numbers.html#practical-matters",
    "title": "5  Floating point and all that",
    "section": "5.3 Practical matters",
    "text": "5.3 Practical matters\nChecking for equality in NumPy"
  },
  {
    "objectID": "monte-carlo.html",
    "href": "monte-carlo.html",
    "title": "7  Monte Carlo methods",
    "section": "",
    "text": "Simplest examples…\nHow to estimate errors\nSome curve fitting here to extract something?\nGoogle pagerank\nMCMC in Bayesian inference\nRelation to Ising models. Community detection. Why not?"
  },
  {
    "objectID": "numbers.html#main-takeaways",
    "href": "numbers.html#main-takeaways",
    "title": "5  Floating point and all that",
    "section": "5.3 Main takeaways",
    "text": "5.3 Main takeaways\n\nFloating point numbers are a necessarily imperfect model of the real numbers. You should not expect facts about real numbers to be true of their floating point representations in general."
  },
  {
    "objectID": "ode.html#eulers-method",
    "href": "ode.html#eulers-method",
    "title": "6  Solving differential equations with SciPy",
    "section": "6.1 Euler’s method",
    "text": "6.1 Euler’s method\nThe basic idea behind all ODE solvers is to introduce a discretization of the equation and its solution \\(x_j\\equiv x(t_j)\\) at time points \\(t_j = hj\\) for some step size \\(h\\) and \\(j=0, 1, \\ldots\\). The very simplest approach is called Euler’s method 2 and approximates the derivative on the right hand side of Equation 6.1 as\n\\[\n\\frac{dx}{dt}\\Bigg|_{t=t_j} \\approx \\frac{x_{j+1} - x_j}{h}.\n\\tag{6.2}\\]\nRearranging the ODE then gives the update rule\n\\[\nx_{j+1} = x_j + hf(x_j, t_j).\n\\tag{6.3}\\]\nOnce an initial condition \\(x_0\\) is specified, subsequent values can be obtained by iteration.\nNotice that Equation 6.2 involved a forward finite difference: the derivative at time \\(t_j\\) was approximated in terms of \\(x_j\\) and \\(x_{j+1}\\) (i.e. one step forward in time). Why do this? So that the update rule Equation 6.3 is an explicit formula for \\(x_{j+1}\\) in terms of \\(x_j\\). This is called an explicit method. If we had used the backward derivative we would end up with backward Euler method \\[\nx_{j+1} = x_j + hf(x_{j+1}, t_{j+1})\n\\tag{6.4}\\]\nwhich is implicit. This means that the update requires an additional step to numerically solve for \\(x_{j+1}\\). Although this is more costly, there are benefits to the backward method associated with stability.\n\n6.1.1 Truncation error\nIn making the approximation Equation 6.2 we make an \\(O(h^2)\\) local truncation error. To integrate for a fixed time the number of steps required is proportional to \\(h^{-1}\\), which means that the worst case error at fixed time (the global truncation error) is \\(O(h)\\). For this reason Euler’s method is called first order. More sophisticated methods are typically higher order: the SciPy function scipy.integrate.solve_ivp uses a fifth order method by default.\nTODO discuss midpoint method?\n\n\n6.1.2 Rounding error\nIf you had unlimited computer time you might think you could make the step size \\(h\\) ever smaller in order to make the updates more accurate. This ignores the machine precision \\(\\epsilon\\), discussed in Section 5.2.1. The rounding error is roughly \\(\\epsilon x_j\\), and if the \\(N\\propto h^{-1}\\) errors in successive steps can be treated as independent random variables, the relative total rounding error will be \\(\\propto \\sqrt{N}\\epsilon=\\frac{\\epsilon}{\\sqrt{h}}\\) and will dominate for \\(h\\) small.\n\n\n6.1.3 Stability\nApart from the relatively low accuracy that comes from using a first order method, the Euler method may additionally be unstable, depending on the equation. This can be demonstrated for the linear equation\n\\[\n\\frac{dx}{dt} = kx\n\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef euler(h, t_max, k=1):\n    \"\"\"\n    Solve the equation x' = k x, with x(0) = 1 using\n    the Euler method. \n\n    Integrate from t=0 to t=t_max using stepsize h for\n    num_steps = t_max / h.\n    \n    Returns two arrays of length num_steps: t, the time coordinate, and x_0, the position.\n    \"\"\"\n    num_steps = int(t_max / h)\n    # Allocate return arrays\n    x = np.zeros(num_steps, dtype=np.float32)\n    t = np.zeros(num_steps, dtype=np.float32)\n    x[0] = 1.0  # Initial condition\n    for i in range(num_steps - 1):\n        x[i+1] = x[i] + k * x[i] * h\n        t[i+1] = t[i] + h  # Time step\n    return t, x\n\nk = -2.3\nt_max = 5\nt, x = euler(1, t_max, k)\nplt.plot(t, x, label=\"h=1 Euler\")\nt, x = euler(0.7, t_max, k)\nplt.plot(t, x, label=\"h=0.7 Euler\")\nt = np.linspace(0, t_max, 100)\nplt.plot(t, np.exp(k * t), label=\"exact solution\")\nplt.title(\"k=-2.3\")\nplt.legend()\nplt.show()\n\n\n\n\nFor a linear equation the Euler update Equation 6.3 is a simple rescaling\n\\[\nx_{j+1} = x_j(1 + hk)\n\\]\nso the region of stability is \\(|1 + hk|\\leq 1\\). You can check that the backward Euler method Equation 6.4 eliminates the instability for \\(k<0\\)."
  },
  {
    "objectID": "ode.html#using-scipy",
    "href": "ode.html#using-scipy",
    "title": "6  Solving differential equations with SciPy",
    "section": "6.2 Using SciPy",
    "text": "6.2 Using SciPy\nComing up with integration schemes is best left to the professionals. Your first port of call for solving ODEs in Python should probably be the integrate module of the SciPy scientific computing library. The function scipy.integrate.solve_ivp provides a versatile API.\nOne important thing to understand is that all these integration schemes apply to systems of first order differential equations. Higher order equations can always be presented as a first order system, at the expense of introducing more equations. For example, in physics we are often concerned with Newton’s equation\n\\[\nm\\frac{d^2 \\mathbf{x}}{dt^2} = \\mathbf{f}(\\mathbf{x},t),\n\\]\nwhich is three second order equations. We turn this into a first order system by introducing the velocity \\(\\mathbf{v}=\\dot{\\mathbf{x}}\\), giving the six equations\n\\[\n\\begin{align}\n\\frac{d\\mathbf{x}}{dt} &= \\mathbf{v}\\\\\nm\\frac{d \\mathbf{v}}{dt} &= \\mathbf{f}(\\mathbf{x},t).\n\\end{align}\n\\]\nAs a simple example, let’s consider the pendulum equation\n\\[\n\\ddot \\theta = -\\sin\\theta\n\\]\nwhich can be cast as\n\\[\n\\begin{align}\n\\dot\\theta &= l\\\\\n\\dot l &= -\\sin\\theta\n\\end{align}\n\\]\nSolving the equation using SciPy just requires us to define a function giving the right hand side of these equations\n\ndef pendulum(t, y): return [y[1], -np.sin(y[0])]\n# The pendulum equation: y[0] is theta and y[1] is l\n\nand then calling solve_ivp\n\nfrom scipy.integrate import solve_ivp\nimport matplotlib.pyplot as plt\n\nt_max = 1000\npendulum_motion = solve_ivp(pendulum, [0, t_max], [2, 0], dense_output=True)\n\nThe option dense_output=True is used to specify that a continuous solution should be found. What this means in practice is that the returned object pendulum_motion has a sol property that is an instance of OdeSolution. sol(t) returns the computed solution at \\(t\\) (this involves interpolation). We can use this to plot the pendulum’s trajectory in the \\(\\theta- l\\) phase plane, along with the contours of the conserved energy function\n\\[\nE(\\theta, l) = \\frac{1}{2}l^2 - \\cos\\theta\n\\]\n\n\nCode for plot\nfig, ax = plt.subplots()\n\ntheta = np.linspace(-1.1 * np.pi, 1.1 * np.pi, 60)\nl = np.linspace(-2, 2, 60)\nE = -np.cos(theta[np.newaxis,:]) + (l[:,np.newaxis])**2 / 2\n# Note the use of broadcasting to obtain the energy as a function of the phase space coordinates\n\nxx, yy = np.meshgrid(theta, l)\n\nax.contourf(xx, yy, E, cmap='Reds')\nt = np.linspace(0, t_max, 10000)\nax.plot(*pendulum_motion.sol(t))\nplt.xlabel(r'$\\theta$')\nplt.ylabel(r'$l$')\nplt.show()\n\n\n\n\n\nThe thickness of the blue line is due to the variation of the energy over the \\(t=1000\\) trajectory (measured in units where the frequency of linear oscillation is \\(2\\pi\\)). Notice that we did not have to specify a time step: this is determined adaptively by the solver to keep the estimate of the local error below atol + rtol * abs(y), where atol and rtol are optional arguments that correspond to the absolute and relative tolerances, with default values of \\(10^{-6}\\) and \\(10^{-3}\\) respectively. The global error is of course much larger. In general, monitoring conserved quantities is a good experimental method for assessing the accuracy of integration.\nThe alternative to dense_output=True is to track “events”, which are user-defined points of interest on the trajectory. We supply solve_ivp with functions event(t, x) whose zeros define the events. We can use events to take a “cross section” of higher dimensional motion. As an example let’s consider the Hénon–Heiles system, a model chaotic system with origins in stellar dynamics\n\\[\n\\begin{align}\n\\dot x &= p_x \\\\\n\\dot p_x &= -x -2\\lambda xy \\\\\n\\dot y &= p_y \\\\\n\\dot p_y &=  - y -\\lambda(x^2-y^2).\n\\end{align}\n\\]\nThese coupled first order systems for the \\(N\\) coordinates and \\(N\\) momenta of a mechanical system with \\(N\\) degrees of freedom are an example of Hamilton’s equations. The phase space is now four dimensional and impossible to visualize.\nThe conserved energy is\n\\[\nE = \\frac{1}{2}\\left(p_x^2+p_y^2 + x^2 + y^2\\right) + \\lambda\\left(x^2y-\\frac{1}{3}y^3\\right)\n\\]\nIf we take a Poincaré section with \\(x=0\\) a system with energy \\(E\\) must lie within the curve defined by\n\\[\nE = \\frac{1}{2}\\left(p_y^2 + y^2\\right) -\\frac{\\lambda}{3}y^3.\n\\]\nStarting from \\(x=0\\) we can generate a section of given \\(E\\) by solving for \\(p_x\\)\n\\[\np_x = \\sqrt{2E-y^2-p_y^2 + \\frac{2\\lambda}{3}y^3}\n\\]\n\ndef henon_heiles(t, z, 𝜆): \n    x, px, y, py = z\n    return [px, -x - 2 * 𝜆 * x * y, py, -y - 𝜆 * (x**2 - y**2)]\n\ndef px(E, y, py, 𝜆):\n    return np.sqrt(2 * E - y**2 - py**2 + 2 * 𝜆 * y**3 / 3)\n\ndef section(t, y, 𝜆): return y[0] # The section with x=0\n\nt_max = 10000\n𝜆 = 1\nhh_motion = []\nfor E in [1/12, 1/8, 1/6]:\n    hh_motion.append(solve_ivp(henon_heiles, [0, t_max], [0, px(E, 0.1, 0.1, 𝜆), 0.1, 0.1], events=section, args=[𝜆], atol=1e-7, rtol=1e-7))\n\nWe can then plot a section of the phase space with increasing energy, showing the transition from regular to chaotic dynamics.\n\n\nCode for plot\nfig, ax = plt.subplots(1, 3)\nenergies = [\"1/12\", \"1/8\", \"1/6\"]\nfor idx, data in enumerate(hh_motion): \n        ax[idx].scatter(*data.y_events[0][:, 2:].T, s=0.1)\n        ax[idx].title.set_text(f\"E={energies[idx]}\")        \n        ax[idx].set_xlabel(r'$y$')\n\nax[0].set_ylabel(r'$p_y$')\nplt.show()\n\n\n\n\n\nTODO Leapfrog?\nNice demo on Poincaré sections\nSymplectic integrator see e.g. \nLook at leapfrog?\nhttps://github.com/scipy/scipy/issues/12690\nProblem is that it’s hard to do in scipy\nhttps://stackoverflow.com/questions/60338471/lyapunov-spectrum-for-known-odes-python-3"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Part II Computational Physics",
    "section": "Schedule",
    "text": "Schedule\nThe course of eight Lectures will take place at 10.00 on Mondays and Fridays in the Pippard Lecture Theatre. After the lectures there will be four computing exercises to be completed in the last four weeks of full Lent term; one per week. Remember that the exercises count for 0.2 units or further work, or roughly 2% of your final mark for the year. Thus each exercise should only take you a few hours.\nThe schedule is as follows\n\nFirst lecture: Monday 23th January\nLast lecture: Friday 17th February\nFirst exercise: Friday 17th February – Friday 24th February\nSecond exercise: Friday 24th February – Friday 3rd March\nThird exercise: Friday 3rd March – Friday 10th March\nFourth exercise: Friday 10th March – Friday 17th March (last day of full Lent term)"
  },
  {
    "objectID": "divide.html",
    "href": "divide.html",
    "title": "10  Divide and Conquer",
    "section": "",
    "text": "FFT. Use split step as illustration Matrix multiplication"
  },
  {
    "objectID": "monte-carlo.html#random-number-generators",
    "href": "monte-carlo.html#random-number-generators",
    "title": "7  Monte Carlo methods",
    "section": "7.4 Random number generators",
    "text": "7.4 Random number generators\nComputers are deterministic\nThis is a subject dealt with already\nRNGs in Trebst?"
  },
  {
    "objectID": "monte-carlo.html#sampling-from-a-distribution",
    "href": "monte-carlo.html#sampling-from-a-distribution",
    "title": "7  Monte Carlo methods",
    "section": "7.1 Sampling from a distribution",
    "text": "7.1 Sampling from a distribution\nLet’s suppose that we have a source of samples of a real valued random variable \\(X\\) that follows a particular probability density function \\(p_X\\) 1 (more about where they might come from in Section 7.5). This means that the probability of drawing a sample in the region \\([x, x+dx]\\) is \\(p_X(x)dx\\). If we now map the samples using a function \\(f\\), what is the probability density \\(p_Y\\) of \\(y=f(x)\\)? The new probability density is defined in just the same way: the probability of \\(y\\) lying in the region \\([y, y+dy]\\) is \\(p_Y(y)dy\\). Since \\(x\\) is being mapped deterministically to \\(y\\) these two probabilities are therefore the same\n\\[\np_X(x)dx = p_Y(y)dy\n\\]\nor\n\\[\np_Y(y)=p_X(x)\\Bigg\\lvert \\frac{dx}{dy}\\Bigg\\rvert= \\frac{p_X(x)}{|f'(x)|},\\qquad x=f^{-1}(y)\n\\]\nThis formula shows that we can create samples from an arbitrary probability distribution by choosing an invertible map \\(f\\) appropriately. If \\(p_X\\) is a standard uniform distribution on \\([0,1]\\) then \\(f(x)\\) is the inverse of the cummulative probability distribution of \\(Y\\) i.e.\n\\[\nf^{-1}(y) = \\int^y_{-\\infty} p_Y(y')dy'\n\\]\nThe same approach works in higher dimensions: \\(\\big\\lvert \\frac{dx}{dy}\\big\\rvert\\) is replaced by the inverse of the Jacobian determinant.\nThe Box–Muller transform is one example of this idea. Take two independent samples from a standard uniform distribution \\(u_{1,2}\\) and form\n\\[\n\\begin{align}\nx &= \\sqrt{-2\\log u_1}\\cos(2\\pi u_2)\\\\\ny &= \\sqrt{-2\\log u_1}\\sin(2\\pi u_2).\n\\end{align}\n\\]\n\\(x\\) and \\(y\\) are independent samples from a standard normal distribution.\nVarious functions are available in the numpy.random module to generate random arrays drawn from a variety of distributions. Box–Muller has now been retired in favour of the Ziggurat algorithm.\n\nimport numpy.random as random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nmu, sigma = 0, 0.1 # mean and standard deviation\ns = random.normal(mu, sigma, size=10000)\ncount, bins, ignored = plt.hist(s, 30, density=True)\nplt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *\n               np.exp( - (bins - mu)**2 / (2 * sigma**2) ),\n         linewidth=2, color='r')\nplt.xlabel(\"Value\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\nFor complex multivariate (i.e. high dimensional) distributions there is no general recipe to construct an appropriate \\(f\\). One very recent application of these ideas is in machine learning models called normalizing flows that use a mapping \\(f\\) parameterized by a neural network. The workhorse for sampling from complicated distributions is Markov chain Monte Carlo, as we discuss in Section 7.2.2."
  },
  {
    "objectID": "monte-carlo.html#example-of-the-ising-model",
    "href": "monte-carlo.html#example-of-the-ising-model",
    "title": "7  Monte Carlo methods",
    "section": "7.2 Example of the Ising model",
    "text": "7.2 Example of the Ising model\nStatistical mechanics is a natural source of such complex distributions in physics. Remember the fundamental principle that the probability of finding a statistical mechanical system in a microstate \\(x\\) 2 with energy \\(E(x)\\) is\n\\[\np_X(x) = \\frac{\\exp(-\\beta E(x))}{Z}\n\\]\nwhere \\(Z\\) is a normalizing constant called the partition function.\nSimplest examples…\nHow to estimate errors\nSome curve fitting here to extract something?\nGoogle pagerank\nMCMC in Bayesian inference\nRelation to Ising models. Community detection. Why not?"
  },
  {
    "objectID": "monte-carlo.html#statistical-mechanics",
    "href": "monte-carlo.html#statistical-mechanics",
    "title": "7  Monte Carlo methods",
    "section": "7.3 Statistical mechanics",
    "text": "7.3 Statistical mechanics\nOriginal paper treated hard spheres:\nhttps://aip.scitation.org/doi/10.1063/1.1699114\nHow to you get thermodynamics out of hard sphere samples?\nStatistical mechanics is a natural source of such complex distributions in physics. Remember the fundamental principle that the probability of finding a statistical mechanical system in a microstate \\(x\\) 4 with energy \\(E(x)\\) is\n\\[\np_X(x) = \\frac{\\exp(-\\beta E(x))}{Z}\n\\]\nwhere \\(Z\\) is a normalizing constant called the partition function and \\(\\beta=1/k_\\text{B}T\\), where \\(T\\) is the temperature and \\(k_\\text{B}\\) is Boltzmann’s constant.\nMetropolis m\n\n7.3.1 The Ising model\nBackground on Markov processes. Transition kernel. Stochastic matrices\nExample of SEP\nSimplest examples…\nHow to estimate errors\nSome curve fitting here to extract something?\nGoogle pagerank\nMCMC in Bayesian inference\nRelation to Ising models. Community detection. Why not?\nhttps://arxiv.org/pdf/cond-mat/0005264.pdf"
  },
  {
    "objectID": "monte-carlo.html#the-monte-carlo-method",
    "href": "monte-carlo.html#the-monte-carlo-method",
    "title": "7  Monte Carlo methods",
    "section": "7.2 The Monte Carlo method",
    "text": "7.2 The Monte Carlo method\nMonte Carlo is the general prefix applied to variety of numerical methods that use randomness in some way. Two of the main classes of problem encountered in physics that come under this heading are:\n\nInterpret a numerical evaluation as an expectation value of some random variable and use sampling to estimate it. Monte Carlo integration is an example of this idea.\nSampling from a complex probability distribution (which may include taking expectation values). Example: Markov chain Monte Carlo.\n\n\n7.2.1 Monte Carlo integration\nThe technique is exemplified by the following fairly dumb way of estimating \\(\\pi\\)\n\nmax_samples = 10000\ninside = 0\nareas = []\nfor sample in range(1, max_samples + 1):\n    x = random.uniform(-1, 1)\n    y = random.uniform(-1, 1)\n    \n    if x ** 2 + y ** 2 <= 1:\n        inside += 1\n    areas.append(4 * inside / sample)\n\nplt.plot(np.arange(1, max_samples + 1), areas)\nplt.plot(np.arange(1, max_samples + 1), np.pi * np.ones(max_samples), linestyle='dashed')\nplt.show()\n\n\n\n\nIn terms of integration, you can think of this as a way to compute the integral of a function which is one inside the unit disc, and zero outside it.\nAlthough it’s a silly method, this does illustrate one important feature of Monte Carlo methods in general: that the relative error with \\(N\\) samples is typically \\(\\propto N^{-1/2}\\) (thus at the 1% level for \\(10^4\\) samples) because the variance of a sum of \\(N\\) iid variables is \\(\\propto N^{1/2}\\).\nTODO General setting. Importance sampling\nMonte Carlo integration comes into its own for high dimensional problems. For low dimensional integrals the quadrature methods in scipy.integrate are preferable.\n\n\n7.2.2 Markov chain Monte Carlo\nSuppose you want to generate configurations at random (i.e. with a uniform distribution) from a “gas” of hard disks 2.\n\n\n\nCoins in a shoe box (gas of hard disks). From Krauth (1998)\n\n\nIt’s harder than it looks! The first guess you might have is to start adding coins at random, and if you get an overlap, try again until you don’t. Obviously this will become inefficient as the box fills up, and most attempts fail. Worse, it doesn’t in fact yield a uniform distribution!\nTODO Why not? See Widom (1966) for an explanation\nHere’s an approach that works:\n\nExample 7.1 (Metropolis algorithm for hard disks)  \n\nFix the number of disks and an initial configuration (some regular lattice configuration, say).\nPick a disk at random and attempt (or propose) to move it by a small random amount (i.e. random direction; random small magnitude).\nIf this results in the moved disk intersecting another, reject the move, leaving the disk where it is. Otherwise, accept the move.\nRepeat 2. and 3. many times.\n\n\n.\nTODO js demo\nThis is the simplest example of the Metropolis–Hastings algorithm, the first Markov chain Monte Carlo (MCMC) algorithm.\nMore generally, the goal of MCMC is to come up with a sequential random process (a Markov chain) that generates (usually after many steps) a sample from a particular distribution.\nYou’ve all heard of a random walk, perhaps as a model for diffusion. At each step you make a move in a random direction, independently of your earlier moves. After many steps these random moves gives rise to a distribution of possible locations. A random walk is the simplest example of a Markov chain.\nMore generally, a Markov chain is a sequence of random variables \\(X_n\\) with each having a distribution that is is conditional on the value of the previous one, and so is defined in terms of transition probabilities \\(p(X_{n}=x_n|X_{n-1}=x_{n-1})\\) (hence they form a “chain”). I’m going to immediately drop this cumbersome notation in favour of \\(p(x_n|x_{n-1})\\), a function of \\(x_n\\) and \\(x_{n-1}\\), but in general the function giving the transition probabilities can be different at each step (the random variables could all be different).\nThe probability of a particular sequence \\(X_1=x_1\\ldots X_n=x_n\\) is therefore\n\\[\np(x_n|x_{n-1})p(x_{n-1}|x_{n-2})\\cdots p(x_2|x_{1})p^{(1)}(x_1)\n\\]\n\\(X_1\\) has no “parent” so is not conditional on any other value.\nSuppose we don’t care about the earlier values and just want to know the marginal distribution \\(p^{(n)}(x_n)\\) of the final variable. For a random walk this is easy, as \\(x_n\\) typically represents a displacement that is a sum of iid increments. In general this is not the case, however, as the marginal distribution is\n\\[\np^{(n)}(x_n)=\\sum_{x_{n-1},\\ldots x_1}p(x_n|x_{n-1})p(x_{n-1}|x_{n-2})\\cdots p(x_2|x_{1})p^{(1)}(x_1)\n\\]\n(I’m writing all these expressions for discrete random variables, but the continuous version involving probability density functions is straightforward)\nThe sums are over all possible values that the random variables might take in the state space of the problem. These could be finite or infinite in number.\nThings are not as bad as they appear, however, as the marginal distribution can be interpreted as the result of acting \\(n-1\\) times on the vector of values of \\(p^{(1)}_j\\equiv p^{(1)}(j)\\) with the transition matrix with elements \\(\\mathsf{P}_{jk}=p(j|k)\\)\n\\[\n\\mathbf{p}^{(n)} = \\mathsf{P}^{n-1}\\mathbf{p}^{(1)}.\n\\]\nIn a single step the marginal probabilities are updated as\n\\[\n\\mathbf{p}^{(n)} = \\mathsf{P}^{n}\\mathbf{p}^{(n-1)}.\n\\]\n\\(\\mathsf{P}\\) has some structure. The matrix elements are positive, as they represent probabilities, and each row sums to one\n\\[\n\\sum_j \\mathsf{P}_{jk} = 1.\n\\]\nSuch matrices are called stochastic.\nAlthough \\(p^{(n)}\\) — the probability distribution at the \\(n\\)th step — changes from step to step, you might expect that after many steps it tends to converge to a stationary distribution \\(p^{(n)}\\to\\boldsymbol{\\pi}\\). If it exists, this distribution must satisfy\n\\[\n\\boldsymbol{\\pi} = \\mathsf{P}\\boldsymbol{\\pi}.\n\\tag{7.1}\\]\nIn other words, it is an eigenvector of \\(\\mathsf{P}\\) with eigenvalue one. This property is guaranteed by the Perron–Frobenius theorem 3.\nThus \\(\\mathsf{P}\\) determines \\(\\boldsymbol{\\pi}\\). MCMC turns this idea on its head and asks: if there is some \\(\\boldsymbol{\\pi}\\) that I would like to generate samples from, can I find a \\(\\mathsf{P}\\) that has it as a stationary distribution?\nThere is a trivial answer to this question. Sure, take \\(\\mathsf{P}_{jk}=\\boldsymbol{\\pi}_j\\). That is, jump straight to the stationary distribution no matter what the starting state. But we are interested in highly complicated distributions over large state spaces (think the Boltzmann distribution for a statistical mechanical system comprised of billions of particles). Thus what we really want is to be able to approach such a complicated distribution by making many transitions with simple distributions.\nOne more idea is useful before returning to concrete algorithms. The quantity\n\\[\n\\mathsf{P}_{jk}\\pi_k = p(j|k)\\pi_k = p(j,k)\n\\]\nis the joint distribution of seeing state \\(k\\) followed by state \\(j\\) in the stationary distribution. A reversible Markov chain is one where \\(p(j,k)=p(k,j)\\). Roughly, you can’t tell the direction of time because any transition is equally likely to happen forward in time as backward. Random physical processes that respect time reversal symmetry are often modeled as reversible Markov processes.\nCombining reversibility with the definition of the stationary state yields the condition of detailed balance\n\\[\n\\mathsf{P}_{jk}\\pi_k = \\pi_j\\mathsf{P}_{kj}.\n\\tag{7.2}\\]\nThis condition is stronger than the condition Equation 7.1 for a stationary state. This makes it easier to check: you don’t have to do a sum over a state space index. The Metropolis algorithm Example 7.1 for the hard disk problem satisfies detailed balance for a stationary distribution that is constant when disks don’t intersect and zero when they do.\nWhen the stationary distribution \\(\\boldsymbol{\\pi}\\) has more structure, designing an appropriate transition matrix is harder. The idea is to generalize the hard disk approach by separating the transition into a proposal distribution \\(p_\\text{prop}(j|k)\\) and an acceptance distribution \\(p_\\text{acc}(a=0,1|j\\leftarrow k)\\) that gives the probability of a move from \\(k\\) to \\(j\\) being accepted (\\(a=1\\)) or rejected (\\(a=0\\)). The probability of moving from \\(k\\) to \\(j\\) is then\n\\[\np(j|k) = p_\\text{acc}(a=1|j\\leftarrow k) p_\\text{prop}(j|k).\n\\]\nSubstituting this into the detailed balance condition Equation 7.2 gives \\[\n\\frac{p_\\text{acc}(a=1|j\\leftarrow k)}{p_\\text{acc}(a=1|k\\leftarrow j)} = \\frac{\\pi_j}{\\pi_k}\\frac{p_\\text{prop}(k|j)}{p_\\text{prop}(j|k)}.\n\\]\nAny \\(p_\\text{acc}\\) that satisfies this relation for all \\(j\\) and \\(k\\) will do the job. The Metropolis choice is\n\\[\np_\\text{acc}(a=1|j \\leftarrow k) = \\min\\left(1,  \\frac{\\pi_j}{\\pi_k}\\frac{p_\\text{prop}(k|j)}{p_\\text{prop}(j|k)}\\right).\n\\tag{7.3}\\]\nThis gives an extremely general algorithm, one of the top ten in applied mathematics, according to one list:\n\nExample 7.2 (Metropolis algorithm)  \n\nStarting from state \\(k\\) sample a next state \\(j\\) from the proposal distribution \\(p_\\text{prop}(j|k)\\).\nAccept the proposal with probability \\(p_\\text{acc}(a=1|j \\leftarrow k)\\) and move to state \\(j\\). Otherwise reject the proposal and stay in state \\(k\\).\nRepeat 1. and 2. many times.\n\n\nMCMC has the benefit of being embarrassingly parallel. If you want to average something over \\(\\boldsymbol{\\pi}\\), just run the algorithm many times independently and average the results. This is perfect for parallel computing.\nThe Metropolis algorithm has an Achilles’ heel, however. To perform a move one has to sample from \\(p_\\text{prop}(j|k)\\) and from \\(p_\\text{acc}(a|j \\leftarrow k)\\). The proposal therefore has to be tractable, like the small shift in position for the hard disk case. This may however, mean that that many of the \\(j\\)s suggested correspond to very small \\(\\pi_j\\), and therefore a very low acceptance probability (c.f. Equation 7.3). For example, in the hard disk case at high density many proposed moves will give rise to overlap of disks and be rejected. This means that many steps are required to have one successful update of the simulation. This kind of slowdown is a common feature of MCMC methods applied to complex distributions.\nWe’ll see some more examples of MCMC algorithms for statistical mechanical problems in Section 7.3, and ways in which this problem can be avoided.\n\n\n7.2.3 Relaxation to equilibrium\nEigenvalues\nMaster equation\nTransition matrix"
  },
  {
    "objectID": "monte-carlo.html#sec-rng",
    "href": "monte-carlo.html#sec-rng",
    "title": "7  Monte Carlo methods",
    "section": "7.5 Random number generators",
    "text": "7.5 Random number generators\nComputers are deterministic\nThis is covered in some detail in the Nature of Computation\nThis is a subject dealt with already\nRNGs in Trebst?\nFurther reading: refer to Krauth notes or book\nOther suggestions from Twitter\nhttps://roomno308.github.io/blog/MCMC.html https://maximilianrohde.com/posts/code-breaking-with-metropolis/\n\n\n\n\nJacobs, Laurence, and Claudio Rebbi. 1981. “Multi-Spin Coding: A Very Efficient Technique for Monte Carlo Simulations of Spin Systems.” Journal of Computational Physics 41 (1): 203–10.\n\n\nKapfer, Sebastian C, and Werner Krauth. 2013. “Sampling from a Polytope and Hard-Disk Monte Carlo.” In Journal of Physics: Conference Series, 454:012031. 1. IOP Publishing.\n\n\nKrauth, Werner. 1998. “Introduction to Monte Carlo Algorithms.” In Advances in Computer Simulation, 1–35. Springer.\n\n\n———. 2006. Statistical Mechanics: Algorithms and Computations. Vol. 13. OUP Oxford.\n\n\nMacKay, David JC. 2003. Information Theory, Inference and Learning Algorithms. Cambridge university press.\n\n\nMetropolis, Nicholas, Arianna W Rosenbluth, Marshall N Rosenbluth, Augusta H Teller, and Edward Teller. 1953. “Equation of State Calculations by Fast Computing Machines.” The Journal of Chemical Physics 21 (6): 1087–92.\n\n\nWidom, B. 1966. “Random Sequential Addition of Hard Spheres to a Volume.” The Journal of Chemical Physics 44 (10): 3888–94."
  },
  {
    "objectID": "monte-carlo.html#sec-statmech",
    "href": "monte-carlo.html#sec-statmech",
    "title": "7  Monte Carlo methods",
    "section": "7.3 Statistical mechanics",
    "text": "7.3 Statistical mechanics\nStatistical mechanics is a natural source of such complex distributions in physics. Remember the fundamental principle that the probability of finding a statistical mechanical system in a microstate \\(\\mathbf{x}\\) 4 with energy \\(\\mathcal{E}(\\mathbf{x})\\) is\n\\[\np(\\mathbf{x})=\\frac{\\exp\\left[-\\beta \\mathcal{E}(\\mathbf{x})\\right]}{Z},\n\\tag{7.4}\\]\nwhere \\(Z\\) is a normalizing constant called the partition function and \\(\\beta=1/k_\\text{B}T\\), where \\(T\\) is the temperature and \\(k_\\text{B}\\) is Boltzmann’s constant.\nThe central problem of statistical mechanics is computing ensemble averages of physical quantities, and the principle difficulty is the intractability of those averages for large systems. For example, if we are dealing with a classical gas, the configuration space point \\(\\mathbf{x}\\) corresponds to the positions of each of the gas molecules \\(\\mathbf{x}=(\\mathbf{x}_1,\\ldots \\mathbf{x}_N)\\) and an average is a \\(3N\\)-dimensional integral. The only situation in which this integral is tractable is when the gas is noninteracting (ideal), in which case the energy function takes the form\n\\[\n\\mathcal{E}(\\mathbf{x}) = \\sum_{n=1}^N \\mathcal{E}_1(\\mathbf{x}_n)\n\\]\nwhere \\(\\mathcal{E}_1(\\mathbf{x})\\) is the single particle energy. In this case the integral factorizes. As soon as we introduce interactions between particles of the form\n\\[\n\\mathcal{E}(\\mathbf{x}) = \\sum_{n<m}^N \\mathcal{E}_2(\\mathbf{x}_n,\\mathbf{x}_m)\n\\]\nthings get a lot harder. The same issue arises in models involving discrete random variables. The canonical example is the Ising model, in which a configuration corresponds to fixing the values of \\(N\\) “spins” \\(\\sigma_n=\\pm 1\\) with an energy function of the form\n\\[\n\\mathcal{E}(\\sigma)=\\sum_n h_n\\sigma_n + \\sum_{m<n} J_{mn}\\sigma_m\\sigma_n.\n\\]\nThe two terms correspond to a (magnetic) field that acts on each spin and a coupling between spins. As in the gas, it’s the latter that causes problems / interest.\nThe Ising model comes in a great many flavours according to how the fields and couplings are chosen. They may reflect a lattice structure: \\(J_{mn}\\neq 0\\) for nearest neighbours, say, or longer range. They may be fixed or random, defining an ensemble of models.\nThe most pessimistic assessment is that to calculate an average we are going to have sum over \\(2^N\\) configurations. Computing the partition function \\(Z\\) that normalizes the average (or which gives the free energy via \\(F=-k_\\text{B}T\\log Z\\)) is another such sum.\nMonte Carlo simulation is a much more attractive alternative. MCMC can be used to generate samples from \\(p(\\sigma)\\) which are then used to estimate the averages of interest (e.g. average energy \\(\\langle\\mathcal{E}(\\sigma)\\rangle\\), average magnetization \\(\\langle\\sum_n \\sigma_n\\rangle\\), etc.).\n\n7.3.1 MCMC updates for the Ising model\nHow does MCMC work in practice for the Ising model? To apply the Metropolis alogorithm Example 7.2 we can use a simple proposal: pick each spin in turn in some order and try to flip it.\nThe form of \\(p(\\sigma)\\) means that, although we cannot compute the probabilities explicitly, we can calculate ratios, which is all we need for Metropolis. For two configurations that differ only by \\(\\sigma_n=\\pm 1\\) we have\n\\[\n\\begin{align}\n\\frac{p(\\sigma_n=1|\\sigma_{m\\neq n})}{p(\\sigma_n=-1|\\sigma_{m\\neq n})} &= \\exp\\left[-2\\beta \\left(h_n+\\sum_{m\\neq n} J_{mn}\\sigma_m\\right)\\right]\\\\\n&\\equiv \\exp\\left[-\\beta\\Delta \\mathcal{E}\\right],\n\\end{align}\n\\]\nwhere \\(\\Delta \\mathcal{E}\\) is the energy difference between two configurations.\nOne alternative to Metropolis is the Heat bath algorithm (or Glauber dynamics or Gibbs sampling) 5. The idea behind the name is that, since we can calculate the influence of the spin’s environment (the “bath”), we can just choose the spin’s orientation with the corresponding probabilities. Since there are only two probabilities the ratio is all we need and we get\n\\[\np(\\sigma_n=\\pm 1|\\sigma_{m\\neq n}) = \\frac{1}{1+ e^{\\pm\\beta \\Delta \\mathcal{E}}}.\n\\tag{7.5}\\]\nThe algorithm is then:\n\nExample 7.3 (Heat bath algorithm)  \n\nPick a spin \\(n\\). 6\nCompute \\(\\Delta E\\), the energy difference between \\(\\sigma_n=\\pm 1\\).\nSet \\(\\sigma_n=\\pm 1\\) with probabilities given by Equation 7.5.\nRepeat 1-3 many times\n\n\nWhat happens if we try and come up with more complicated proposals, flipping many spins at once? For Metropolis, the problem is that without a cleverly designed proposal we will be suggesting moves that are likely to be rejected. For the heat bath algorithm, the more spins we flip, the more complicated the evaluation of the corresponding probabilities (\\(2^n\\) outcomes if we flip \\(n\\) spins).\nThe good news is that we can do better — much better — than the above algorithms. The Wolff algorithm is one example. This proposes a cluster of spins of the same orientation to be flipped by adding adjacent spins to an initially random chosen spin with probability \\(p_\\text{add}\\). It turns out that for the nearest neighbour Ising model with Ferromagnetic coupling \\(J<0\\) the “magic” value \\(p_\\text{add}=1-e^{2\\beta J}\\) is rejection free: the probability to flip the whole cluster is always one. This makes for an extremely fast algorithm that is not subject to the usual critical slowing down at phase transitions.\n\n\nIsing model code\nclass IsingModel:\n    def __init__(self, L):\n        self.L = L\n        self.spins = np.random.choice(a=[1, -1], size=(L, L))\n        stagger = np.empty(self.L, dtype = bool)\n        stagger[::2] = True\n        stagger[1::2] = False\n        self.mask = np.logical_xor(stagger[:, np.newaxis], stagger[np.newaxis, :])\n\n    def gibbs_update(self, beta, sublattice):\n        fields = np.roll(self.spins, 1, 0) + np.roll(self.spins, -1, 0) + np.roll(self.spins, 1, 1) + np.roll(self.spins, -1, 1)\n        delta_E = 2 * fields\n        spin_up_probabilities = 1 / (1 + np.exp(- beta * delta_E))\n        new_spins = 2 * (np.random.rand(self.L, self.L) < spin_up_probabilities) - 1\n        self.spins = np.choose(np.logical_xor(sublattice, self.mask), [self.spins, new_spins])\n\n    def glauber_update(self, beta):\n        x, y = np.random.randint(self.L, size=2)\n        fields = 0\n        for neighbour in [((x + 1) % self.L, y), ((x - 1) % self.L, y), (x, (y + 1) % self.L), (x, (y - 1) % self.L)]:\n            fields += self.spins[neighbour]\n        delta_E = 2 * fields\n        spin_up_probability = 1 / (1 + np.exp(- beta * delta_E))        \n        if np.random.rand() < spin_up_probability:\n            self.spins[x, y] = 1\n        else:\n            self.spins[x, y] = -1\n\n    def wolff_update(self, beta):\n        initial_x, initial_y = np.random.randint(self.L, size=2)\n        initial_spin = self.spins[initial_x, initial_y]\n        cluster = deque([(initial_x, initial_y)])\n        add_prob = 1 - np.exp(-2 * beta)\n\n        while len(cluster) != 0:\n            x, y = cluster.popleft()\n            if self.spins[x, y] == initial_spin:\n                self.spins[x, y] *= -1\n                for neighbour in (((x + 1) % self.L, y), ((x - 1) % self.L, y), (x, (y + 1) % self.L), (x, (y - 1) % self.L)):\n                    if self.spins[neighbour] == initial_spin:\n                        if np.random.rand() < add_prob:\n                            cluster.append(neighbour)\n\n\n\n\n\n\n\n\n\n\nFigure 7.1: Glauber dynamics, Block Gibbs sampling and Wolff updates compared. Change the temperature using the slider. The centre of the slider corresponds to the critical temperature \\(k_\\text{B}T = 2|J|/\\log(1+\\sqrt{2})\\sim 2.269|J|\\)."
  },
  {
    "objectID": "monte-carlo.html#heat-bath-algorithm-or-glauber-dynamics",
    "href": "monte-carlo.html#heat-bath-algorithm-or-glauber-dynamics",
    "title": "7  Monte Carlo methods",
    "section": "7.4 Heat bath algorithm or Glauber dynamics",
    "text": "7.4 Heat bath algorithm or Glauber dynamics\n\nPick a spin \\(n\\) at random\nCompute \\(\\Delta E\\), the energy difference between \\(\\sigma_n=\\pm 1\\),\nSet \\(\\sigma_n\\) with probabilities Equation 7.5.\nRepeat 1-3 many times :::\n\nThis is called the heat bath algorithm or Glauber dynamics\nWhy can’t we always do this?\nAlthough Equation 7.4 is\nNo rejection for heat bath / Gibbs\n\n7.4.1 Summary of issues\n\nFinite size effects\nApproach to equilibrium\nCritical slowing down / loss of ergodicity\nBias of estimators\n\nOther updates\nA huge topic, see Krauth for more\nComment at the end about typicality\n\n\n7.4.2 The Ising model\nBackground on Markov processes. Transition kernel. Stochastic matrices\nExample of SEP\nSimplest examples…\nHow to estimate errors\nSome curve fitting here to extract something?\nGoogle pagerank\nMCMC in Bayesian inference\nRelation to Ising models. Community detection. Why not?\nhttps://arxiv.org/pdf/cond-mat/0005264.pdf\nBayesian inference"
  },
  {
    "objectID": "monte-carlo.html#heat-bath-algorithm-or",
    "href": "monte-carlo.html#heat-bath-algorithm-or",
    "title": "7  Monte Carlo methods",
    "section": "7.4 Heat bath algorithm or",
    "text": "7.4 Heat bath algorithm or\n\nPick a spin \\(n\\) at random\nCompute \\(\\Delta E\\), the energy difference between \\(\\sigma_n=\\pm 1\\),\nSet \\(\\sigma_n\\) with probabilities Equation 7.5.\nRepeat 1-3 many times :::\n\nThis is called the heat bath algorithm or Glauber dynamics\nWhy can’t we always do this?\nAlthough Equation 7.4 is\nNo rejection for heat bath / Gibbs\n\n7.4.1 Summary of issues\n\nFinite size effects\nApproach to equilibrium\nCritical slowing down / loss of ergodicity\nBias of estimators\n\nOther updates\nA huge topic, see Krauth for more\nComment at the end about typicality\n\n\n7.4.2 The Ising model\nBackground on Markov processes. Transition kernel. Stochastic matrices\nExample of SEP\nSimplest examples…\nHow to estimate errors\nSome curve fitting here to extract something?\nGoogle pagerank\nMCMC in Bayesian inference\nRelation to Ising models. Community detection. Why not?\nhttps://arxiv.org/pdf/cond-mat/0005264.pdf\nBayesian inference"
  },
  {
    "objectID": "monte-carlo.html#heat-bath-algorithm",
    "href": "monte-carlo.html#heat-bath-algorithm",
    "title": "7  Monte Carlo methods",
    "section": "7.4 Heat bath algorithm",
    "text": "7.4 Heat bath algorithm\n\nPick a spin \\(n\\) at random 5\nCompute \\(\\Delta E\\), the energy difference between \\(\\sigma_n=\\pm 1\\).\nSet \\(\\sigma_n=\\pm 1\\) with probabilities given by Equation 7.5.\nRepeat 1-3 many times\n\nWhat happens if we try and come up with more complicated proposals, flipping many spins at once? For Metropolis, the problem is that without a cleverly designed proposal we will be suggesting moves that are likely to be rejected. For the heat bath algorithm, the more spins we flip, the more complicated the evaluation of the corresponding probabilities (\\(2^n\\) outcomes if we flip \\(n\\) spins).\n\n7.4.1 Summary of issues\nhttps://en.wikipedia.org/wiki/Swendsen%E2%80%93Wang_algorithm\nhttps://en.wikipedia.org/wiki/Wolff_algorithm\n\nFinite size effects\nApproach to equilibrium\nCritical slowing down / loss of ergodicity\nBias of estimators\n\nMultispin encoding: 32 or 64 simulations Jacobs and Rebbi (1981)\nOther updates\nA huge topic, see Krauth for more\nComment at the end about typicality\nMCMC in Bayesian inference\nRelation to Ising models. Community detection. Why not?\nhttps://arxiv.org/pdf/cond-mat/0005264.pdf\nBayesian inference"
  },
  {
    "objectID": "monte-carlo.html#the-universe-of-monte-carlo-methods",
    "href": "monte-carlo.html#the-universe-of-monte-carlo-methods",
    "title": "7  Monte Carlo methods",
    "section": "7.4 The universe of Monte Carlo methods",
    "text": "7.4 The universe of Monte Carlo methods\nMonte Carlo simulation is a vast field with practitioners and specialists across the natural sciences, engineering, machine learning, and statistics. In this section I’ll mention a few important topics to give a taste of what’s out there. For much more detail take a look at Krauth (2006) and / or MacKay (2003).\nProbably the biggest single issue is: how do you kow when your MCMC simulation has reached the stationary distribution \\(\\boldsymbol{\\pi}\\)? The pragmatic approach is to monitor the averages of interest (magnetization, say, in the case of the Ising model) over different simulations or over a time interval and see when they stop changing.\nWe’ve touched on the issue of the mixing time in a Markov chain.\n\nFinite size effects\nApproach to equilibrium\nCritical slowing down / loss of ergodicity\nBias of estimators. Importance sampling\n\nExact sampling\nHamiltonian Monte Carlo.\nMultispin encoding: 32 or 64 simulations Jacobs and Rebbi (1981)\nhttps://en.wikipedia.org/wiki/Gibbs_sampling\nOther updates\nA huge topic, see Krauth (2006) for much more\nAlso Chapter 29 of MacKay (2003)\nComment at the end about typicality\nMCMC in Bayesian inference\nRelation to Ising models. Community detection. Why not?\nhttps://arxiv.org/pdf/cond-mat/0005264.pdf\nBayesian inference"
  },
  {
    "objectID": "index.html#outline",
    "href": "index.html#outline",
    "title": "Part II Computational Physics",
    "section": "Outline",
    "text": "Outline\nHere’s a list of topics that I’d like to cover. We make not have time for all of them.\n\nSetup. Running Python. Notebooks. Language overview\nNumPy and friends\nFloating point and all that\nSoving differential equations with SciPy\nMonte Carlo methods\nLinear algebra with NumPy\nIntroduction to algorithms and complexity\nThe fast Fourier transform\nAutomatic differentiation"
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Part II Computational Physics",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis course assumes a basic knowledge of the Python language, including variables, control flow, and writing and using functions, at the level of last year’s IB course."
  },
  {
    "objectID": "index.html#these-notes",
    "href": "index.html#these-notes",
    "title": "Part II Computational Physics",
    "section": "These notes…",
    "text": "These notes…\n…were prepared using Quarto. Each chapter should be thought of as a Jupyter notebook (actually, they are Jupyter notebooks), so you’ll only see import numpy as np once in each chapter, for example.\nIn several places I’ve used examples from an earlier version of the course by David Buscher."
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "Part II Computational Physics",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nIn this course you will learn\n\nAbout the Python scientific stack (based on the NumPy library)\nIts use in implementing some common algorithms in computational physics.\nBasic ideas of computational complexity used in the analysis of algorithms"
  },
  {
    "objectID": "autograd.html",
    "href": "autograd.html",
    "title": "12  Automatic Differentiation",
    "section": "",
    "text": "Karpathy’s micrograd Michael Nielsen’s book"
  },
  {
    "objectID": "complexity.html#first-example-multiplication",
    "href": "complexity.html#first-example-multiplication",
    "title": "8  Algorithms and computational complexity",
    "section": "8.1 First example: multiplication",
    "text": "8.1 First example: multiplication\nHow hard is it to multiply numbers? The bigger they are, the harder it is, as you well know. You also know that computers are very good at multiplying, so once you’ve switched from multiplying numbers yourself to multiplying them on a computer, you may well be tempted to forget"
  },
  {
    "objectID": "complexity.html#defining-complexity",
    "href": "complexity.html#defining-complexity",
    "title": "8  Algorithms and computational complexity",
    "section": "8.2 Defining complexity",
    "text": "8.2 Defining complexity\nXKCD plots\nBig O\nSimple example from Leetcode\nAnalaysis of algorithms\nExample of finding a unique item in list\nNice examples from Garth Wells\nhttps://github.com/CambridgeEngineering/PartIA-Computing-Michaelmas/blob/main/11%20Complexity.ipynb\nExamples of multiplication\nBreadth first and depth first\nImportance of choosing a data structure to match algorithm\nExamples: queue in Wolff. Was there a Numpy-ish way to do this faster? Priority queue in waiting time algo\nFFT uses\nhttps://en.wikipedia.org/wiki/Orthogonal_frequency-division_multiplexing\nNeedleman-Wunsch\nExamples\n\nMultiplication Karatsuba\nBinary search\nLinear algebra\nSorting\nFFT\nTaking powers (SICP)\nEuclidean algorithm (GCD) (SICP)"
  }
]