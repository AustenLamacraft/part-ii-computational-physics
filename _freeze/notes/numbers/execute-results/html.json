{
  "hash": "37239f6dc0371d346e5dbd1c12493a7c",
  "result": {
    "markdown": "---\ntitle: Floating point and all that\n---\n\n<!-- https://pythonspeed.com/articles/float64-float32-precision/ -->\n\nSince physics is all about numbers we had better develop some understanding of how computers represent them, and the limitations of this representation. Hopefully this example is sufficiently motivating:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n0.1  + 0.2 == 0.3\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\nFalse\n```\n:::\n:::\n\n\nAh...\n\n# Integers\n\nLet's begin with something simpler\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n1 + 1 == 2\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\nTrue\n```\n:::\n:::\n\n\nwhich is a bit more reassuring. Integers can be represented in binary\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n3 == 0b11\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\nTrue\n```\n:::\n:::\n\n\nor octal or hexadecimal (with a prefix `0o` or `0h`). You can get the binary string representing an integer using the `bin` function\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nbin(-2)\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\n'-0b10'\n```\n:::\n:::\n\n\nPython allows for arbitrarily large integers, so there is no possibility of overflow or rounding error\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n2**100\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\n1267650600228229401496703205376\n```\n:::\n:::\n\n\nThe only limitation is the memory required to store it. \n\nNumpy integers are a different story \n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nimport numpy as np\nnp.int64(2**100)\n```\n\n::: {.cell-output .cell-output-error}\n```\nOverflowError: Python int too large to convert to C long\n```\n:::\n:::\n\n\nSince NumPy is using C the types have to play nicely. The range of integers that can be represented with 32 bit `numpy.int32`s is $\\approx\\pm 2^{31} \\approx \\pm 2.1 × 10^9$ (one bit is for the sign) and 64 bit `numpy.int64`s is $\\approx\\pm 2^{63} \\approx \\pm 9.2 × 10^{18}$. Apart from the risk of overflow when working NumPy's integers there are no other gotchas to worry about.\n\n\n# Floating point numbers\n\nThe reason why $0.1 + 0.2 \\neq 0.3$ in Python is that specifying a real number exactly would involve an infinite number of bits, so that any finite representation is necessarily approximate.\n\nThe representation computers use for the reals is called [floating point arithmetic](https://en.wikipedia.org/wiki/Floating-point_arithmetic). It is essentially a form of scientific notation, in which a [significand](https://en.wikipedia.org/wiki/Significand) (it contains the significant figures) is multiplied by an _exponent_. The name _floating point_ reflects the fact that the number of digits after the decimal point is not fixed (I'm using the base ten terms for convenience)\n\nThis representation requires the choice of a base, and Python's floating point numbers use binary. Numbers with finite binary representations therefore behave nicely\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n0.125 + 0.25 == 0.375\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\nTrue\n```\n:::\n:::\n\n\nFor decimal numbers to be represented exactly we'd have to use base ten. This can be achieved with the `decimal` module. Our $0.1+0.2$ example then works as expected\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom decimal import *\nDecimal('0.1') + Decimal('0.2')\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```\nDecimal('0.3')\n```\n:::\n:::\n\n\nSince there is nothing to single out the decimal representation in physics (as opposed to, say, finance) we won't have any need for it.\n\nA specification for floating point numbers must give\n\n1. A base (or _radix_) $b$\n2. A precision $p$, the number of digits in the significand $c$. Thus $0\\leq c \\leq b^{p}-1$.\n3. A range of exponents $q$ specifed by $\\text{emin}$ and $\\text{emax}$ with $\\text{emin}\\leq q+p-1 \\leq \\text{emax}$.\n\nIncluding one bit $s$ for the overall sign, a number then has the form $(-1)^s\\times c \\times b^q$. The smallest positive nonzero number that can be represented is therefore $b^{1 + \\text{emin} - p}$ (corresponding to the smallest value of the exponent) and the largest is $b^{1 + \\text{emax}} - 1$. \n\nThe above representation isn't unique: for some numbers you could make the significand smaller and the exponent bigger. A unique representation is fixed by choosing the exponent to be as small as possible.\n\nRepresenting numbers smaller than $b^{\\text{emin}}$ involves a loss of precision, as the number of digits in the significand falls below $p$ and the exponent has taken its minimum value . These are called [subnormal numbers](https://en.wikipedia.org/wiki/Subnormal_number). For binary floats, if we stick with the normal numbers and a $p$-bit significand the leading bit will be 1 and so can be dropped from the representation, which then only requires $p-1$ bits.    \n \nThe specification for the floating point numbers used by Python (and many other languages) is contained in the IEEE Standard for Floating Point Arithmetic [IEEE 754](https://en.wikipedia.org/wiki/IEEE_754). The default Python `float` uses the 64 bit _binary64_ representation (often called _double precision_). Here's how those 64 bits are used\n\n- $p=53$ for the significand, encoded in 52 bits\n- 11 bits for the exponent\n- 1 bit for the sign\n\nAnother common representation is the 32 bit _binary32_ (_single precision_) with \n\n- $p=24$ for the significand, encoded in 23 bits\n- 8 bits for the exponent\n- 1 bit for the sign \n\n## Floating point numbers in NumPy {#sec-fp-numpy}\n\nIf this all a bit theoretical you can just get NumPy's [finfo](https://numpy.org/doc/stable/reference/generated/numpy.finfo.html) function to tell all about the [machine precision](https://en.wikipedia.org/wiki/Machine_epsilon)\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nnp.finfo(np.float64)\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```\nfinfo(resolution=1e-15, min=-1.7976931348623157e+308, max=1.7976931348623157e+308, dtype=float64)\n```\n:::\n:::\n\n\nNote that $2^{-52}=2.22\\times 10^{-16}$ which accounts for the value $10^{-15}$ of the resolution. This can be checked by finding when a number is close enough to treated as 1.0.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nx=1.0\nwhile 1.0 + x != 1.0:\n    x /= 1.01 \nprint(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.099427563084686e-16\n```\n:::\n:::\n\n\nFor binary32 we have a resolution of $10^{-6}$.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nnp.finfo(np.float32)\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```\nfinfo(resolution=1e-06, min=-3.4028235e+38, max=3.4028235e+38, dtype=float32)\n```\n:::\n:::\n\n\nOne lesson from this is that taking small differences between numbers is a potential source of rounding error, as in this somewhat mean exam question\n\n![](assets/ia-question.png)\n\n::: {.callout-tip collapse=\"true\"}\n# Solution\nSolution: $x-x'=x(1-\\gamma^{-1})\\sim x\\beta^2/2\\sim 4.2\\text{mm}$. \n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nimport numpy as np\nfrom scipy.constants import c\nbeta = 384400e3 / (76 * 3600) / c\ngamma = 1/np.sqrt(1 - beta**2)\nprint(1 - np.float32(1/gamma), 1 - np.float64(1/gamma))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.0 1.0981660025777273e-11\n```\n:::\n:::\n\n\n:::\n\n## The dreaded NaN\n\nAs well as a floating point system, IEEE 754 defines Infinity and NaN (Not a Number)\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nnp.array([1, -1, 0]) / 0\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/xs/y8sn45v943s2_62flnxw0p940000gn/T/ipykernel_6572/2604490398.py:1: RuntimeWarning:\n\ndivide by zero encountered in true_divide\n\n/var/folders/xs/y8sn45v943s2_62flnxw0p940000gn/T/ipykernel_6572/2604490398.py:1: RuntimeWarning:\n\ninvalid value encountered in true_divide\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=28}\n```\narray([ inf, -inf,  nan])\n```\n:::\n:::\n\n\nThey behave as you might guess\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\n2 * np.inf, 0 * np.inf, np.inf > np.nan\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```\n(inf, nan, False)\n```\n:::\n:::\n\n\nNaNs propagate through subsequent operations\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\n2 * np.nan\n```\n\n::: {.cell-output .cell-output-display execution_count=30}\n```\nnan\n```\n:::\n:::\n\n\nwhich means that if you get a NaN somewhere in your calculation, you'll probably end up seeing it somewhere in the output (which is the idea). \n\n",
    "supporting": [
      "numbers_files"
    ],
    "filters": [],
    "includes": {}
  }
}