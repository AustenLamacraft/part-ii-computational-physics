{
  "hash": "d476a3239dc3e28467892134eab29aa1",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Fast Fourier transform\n---\n\n\nYou've met Fourier in the context of Fourier _series_, where a periodic function is represented as an infinite series, and Fourier _transforms_, where a non-periodic function is represented as an integral. Both representations are infinite, involving all the points on a real interval or an infinite number of terms in a series, so not really suitable for representation on a computer. For that we need a _third_ thing: the discrete Fourier transform (DFT). After defining it, we'll see there is a divide and conquer type algorithm for calculating it efficiently called the [fast Fourier transform](https://en.wikipedia.org/wiki/Fast_Fourier_transform) (FFT), which opened up an enormous number of applications across science and engineering.\n\n# The discrete Fourier transform\n\nThe discrete Fourier transform is a change of basis in a finite dimensional space. You can think of it as taking a signal sampled at a finite number of regularly spaced time points and outputting a representation of the signal at a finite number of frequency points (this is a common application).\n\nFor a vector $x_j = 0,\\ldots N-1$, we define the DFT by\n$$\nF_n = \\sum_{j=0}^{N-1} f_j e^{-i\\eta_n j},\n$$ {#eq-dft}\nwhere $\\eta_n\\equiv \\frac{2\\pi n}{N}$, and $n = 0,\\ldots N-1$. The coefficients $F_n$ contain the same information as $f_j$, and $f_j$ can be recovered completely by inverting the operation. The key to inverting @eq-dft is the observation \n$$\n\\sum_{n=0}^{N-1} e^{i\\eta_n j} = \\begin{cases}\n0 & j\\neq 0 \\mod N\\\\\nN & j = 0 \\mod N.\n\\end{cases} \n$$ {#eq-DFTIdent}\n\n>Perhaps the slickest way to to see this is to observe that\n>$$\n>z^N-1 = (z-1)(1 + z + z^2 +\\cdots z^{N-1})\n>$$\n>Can you fill in the rest of the argument?\n\nThis gives\n$$\nf_j = \\frac{1}{N}\\sum_{n=0}^{N-1}  F_n e^{i\\eta_n j}.\n$$ {#eq-IDFT}\nA more democratic definition would have $1/\\sqrt{N}$ in both definitions @eq-dft and @eq-IDFT. This would allow us to regard the DFT as a basis change to an orthonormal basis of vectors $e^{(n)}_j = \\frac{e^{i\\eta_n j}}{\\sqrt{N}}$ [^1]. Then both the DFT and its inverse would be unitary transformations.\n\n[^1]: Orthonormality follows from @eq-DFTIdent.\n\nWe often think of Fourier transforms involving complex exponentials in terms of positive and negative frequency contributions. When $N$ is finite the $\\eta_n$ values for $n$ close to $N-1$ can be regarded as the negative frequencies since $e^{-i\\eta_n j}=e^{2\\pi ij -i\\eta_n j}=e^{i\\eta_{N - n}j}$.\n\nThere are a several limits we can consider that connect the DFT with the things you have already met.\n\n## $N\\to\\infty$ limit\n\nIn this limit the $\\eta_n$ values become dense in the range $(-\\pi,\\pi]$, with separation $\\Delta \\eta = 2\\pi/N$, and we replace the sum in the inverse DFT @eq-IDFT by an integral according to the prescription\n$$\n\\sum_{n=0}^{N-1} \\left(\\cdots\\right) \\xrightarrow{N\\to\\infty} N \\int_{0}^{2\\pi} \\frac{d\\eta}{2\\pi}\\left(\\cdots\\right),\n$$\ngiving\n$$\nf_j = \\int_{0}^{2\\pi} \\frac{d\\eta}{2\\pi}\\,F(\\eta) e^{i\\eta j}.\n$$\n\n## $N\\to\\infty$ with $f_j = f(jL/N)$\n\nAlternatively, regard the $N\\to\\infty$ limit as sampling a function $f(x)$ ever more finely in the range (0,L]. Now it's the DFT, rather than the inverse, that becomes an integral\n$$\n\\hat f(k) \\equiv \\int_0^L f(x) e^{-ik_n x}\\,dx,\n$$\nwhere $k_n =2\\pi n/L$. Note that $k_n x = \\eta_n j$. The pair of transformations is now\n$$\n\\begin{align}\n\\hat f_k &= \\int_0^L f(x) e^{-ik_n x}\\,dx\\nonumber\\\\\nf(x) &= \\frac{1}{L}\\sum_k \\hat f_k e^{ik_n x}\n\\end{align} \n$$ {#eq-FTSeries}\nThis is the conventional form of the Fourier series for a function with period $L$.\n\nWith this definition $\\hat f_k$ has an extra dimension of distance (on account of the integral), which gets removed by the $1/L$ in the inverse transform.\n\nThe analog of the identity @eq-DFTIdent is\n$$\n \\frac{1}{L}\\sum_k e^{ik x} = \\delta_L(x),\n$$\nwhere $\\delta_L(x)$ is an $L$-periodic version of the $\\delta$-function.\n\n## $L\\to\\infty$\n\nFinally we arrive at the Fourier transform, where we take $L\\to\\infty$, so that the inverse transform in @eq-FTSeries becomes an integral too\n$$\n\\begin{align}\n\\hat{f}(k) & = \\int_{-\\infty}^\\infty f(x) e^{-ik_n x}\\,dx\\nonumber\\\\\nf(x) &= \\int_{-\\infty}^\\infty \\hat f(k) e^{ik_n x}\\,\\frac{dk}{2\\pi}.\n\\label{coll_FTTrans}\n\\end{align}\n$$\n\n## Some important properties {#sec-properties}\n\nHere are some properties that hold for all of the above.\n\n1. If $f_j$ is real then $F_n = \\left[F_{-n}\\right]^*$.\n\n2. If $f_j$ is even (odd) [^2] , $F_n$ is even (odd).\n\n3. (Ergo) if $f_j$ is real and even, so is $F_n$.\n\n[^2]: with respect to the middle of the data.\n\n## Higher dimensions\n\nThe DFT generalizes to higher dimensions straightforwardly. Suppose we have data living in $d$ dimensions with $N_i$ datapoints along dimension $i=1,\\dots d$, then the DFT is\n$$\nF_{\\mathbf{n}} = \\sum_{\\mathbf{n}} f_\\mathbf{j}e^{-i \\boldsymbol{\\eta}_\\mathbf{n}\\cdot \\mathbf{j}},\n$$\nwhere $\\mathbf{j}=(j_1,\\ldots j_{d})$ with $j_i = 0,\\ldots N_i - 1$ and likewise $\\boldsymbol{\\eta}_\\mathbf{n} = 2\\pi (n_1 / N_1, \\ldots n_d/ N_d)$ $n_i = 0,\\ldots N_i - 1$ \n\n# The fast Fourier transform\n\nWhat is the complexity of computing the Fourier transform? @eq-dft may be regarded as matrix-vector multiplication, where $f_j$ are the $N$ components of a vector and the matrix has elements $Q_{n j}\\equiv e^{-i\\eta_n j}$, so that the DFT is written\n$$\n\\mathbf{F} = Q \\cdot \\mathbf{f}.\n$$\nNaively, then, the complexity is $O(N^2)$.\n\nSurprisingly, we can do a lot better than this, because $Q$ is not any old matrix but has a great deal of structure. To see this, we follow the divide and conquer pattern of trying to break the problem up into two sub-problems, in this case by separating the $N/2$ even and $N/2$ odd coefficients (assuming $N$ even)\n\n$$\nf^{\\text{e}}_j = f_{2j}\\qquad f^{\\text{o}}_j = f_{2j+1},\\qquad j=0,\\ldots N/2 - 1.\n$$\n\nThe key idea is that we are going to express the DFT $F_n$ in terms of the DFT of $f^\\text{e}_j$ and $f^\\text{o}_j$.\n\nBefore going on, it's convenient to introduce the $N$th root of 1 $\\omega_N \\equiv e^{2\\pi i/N}$, so that $e^{i\\eta_n j}=\\omega_N^{nj}$ and we can write the Fourier transform as\n\n$$\nF_n = \\sum_{j=0}^{N-1} f_j \\omega_N^{-nj}\n$$\n\nThen we notice the following\n\n$$\n\\begin{align}\nF_n &= \\sum_{j=0}^{N-1} \\omega_N^{-nj} f_j \\\\\n&=\\sum_{j=0}^{N/2-1} \\left[\\omega_N^{-2nj} f^{\\text{e}}_j + \\omega_N^{-n(2j+1)}f^{\\text{o}}_j\\right]\\\\\n&=\\sum_{j=0}^{N/2-1} \\left[\\omega_{N/2}^{-nj} f^{\\text{e}}_j + \\omega_N^{-n}\\omega_{N/2}^{-nj}f^{\\text{o}}_j\\right].\\\\\n\\end{align}\n$$\n\nThis is already looking like the sum of two DFTs of size $N/2$ but we have to be a bit careful with the indexing, as the original index $n$ takes values $0,\\ldots N-1$. We can write $n$ as\n\n$$\nn = (N/2)n_0 + n'\n$$\n\nwhere $n_0=0,1$ and $n'=0,\\ldots N/2 - 1$. If $n$ is a power of 2 then $n_0$ is the most significant bit of $n$ when written in binary and $n'$ are the remaining bits. Since $\\omega_{N/2}^{-jn}=\\omega_{N/2}^{-jn'}$ we have\n\n$$\n\\begin{align}\nF_n &= \\sum_{j=0}^{N/2-1} \\left[\\omega_{N/2}^{-n'j} f^{\\text{e}}_j + (-1)^{n_0}\\omega_N^{-n'} \\omega_{N/2}^{-n'j}f^{\\text{o}}_j\\right]\\\\\n&= F^\\text{e}_{n'} + (-1)^{n_0}\\omega_N^{-n'} F^\\text{o}_{n'}.\n\\end{align}\n$$ {#eq-divide-and-conquer}\n\nThis is the big payoff: using the properties of the Fourier matrix we have expressed the DFT $F_n$ in terms of the DFTs of two arrays of half the length. If $N$ is a power of 2 we can repeat that process until we get to arrays of length 1, in which case the array and its DFT are the same single number.\n\nThe structure of the FFT is often illustrated in terms of the \"Butterfly\" diagram, which probably makes more sense after you've seen @eq-divide-and-conquer.\n\n![Butterfly diagram for the FFT Source:[Wikipedia](https://en.wikipedia.org/wiki/Fast_Fourier_transform)](../assets/FFT-butterfly.png){width=60%}\n\n## Complexity\n\nIt should be clear that the FFT is going to beat the naive approach, but let's analyze the complexity more carefully. If $T(N)$ is time (or number of steps) required to compute the DFT for size $N$ inputs, then calculating $F^\\text{e}_{n'}$ and $F^\\text{0}_{n'}$ takes time $2T(N/2)$. Using @eq-divide-and-conquer to evaluate $F_n$ is a further $N$ steps, so we have [^3]\n\n[^3]: Strictly we should write $T(N) = 2T(N/2) +\\Theta(N)$, where $f(N)=\\Theta(g(N))$ means $\\lim_{N\\to \\infty} \\frac{f(N)}{g(N)}$ is a finite nonzero number. Thus $\\Theta(N)$ means \"proportional to $N$ in the large $N$ limit\".\n\n$$\nT(N) = 2T(N/2) +\\alpha N\n$$\n\nfor some $\\alpha$. This implies $T(N)=\\Theta(N\\log N)$.\n\nWhat happens when $N$ isn't a power of 2? You can still use the divide and conquer strategy for any other factor $p$ of $N$. If the largest prime factor of $N$ is bounded i.e. doesn't grow with $N$ [^4], then this still yields $T(N)=\\Theta(N\\log N)$. If $N$ is prime you have to use [something else](https://en.wikipedia.org/wiki/Rader%27s_FFT_algorithm).\n\n[^4]: Such numbers are called _smooth_.\n\nAs a practical matter it's probably best to try and ensure that $N$ _is_ a power of two e.g. by choosing the size of your simulation appropriately or padding your data with zeros until its length is a power of 2.\n\n## History\n\nThe modern invention of the FFT is credited to  @cooley1965algorithm. Certainly they were the first to discuss its complexity. The divide and conquer approach was however anticipated by @danielson1942some, who had applications in crystallography in mind, and by unpublished work of [Carl Friedrich Gauss](https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss) in 1805 (predating even Fourier) in his astronomical studies. See @cooley1967historical for more on the historical background.\n\n## FFT in Python\n\nAs usual, you don't have to go implementing this yourself. The FFT is available in both NumPy (in the [`numpy.fft`](https://numpy.org/doc/stable/reference/routines.fft.html#module-numpy.fft) module) and SciPy (in [`scipy.fft`](https://docs.scipy.org/doc/scipy/reference/fft.html#module-scipy.fft)), the latter with a more comprehensive set of functions.\n\n@eq-dft and @eq-IDFT are the default definitions used in these modules, though you should always check the conventions used in any library implementation. The NumPy documentation provides a careful discussion of the positive and negative frequency components and there are several helper functions available to make your life easier, such as:\n\n1. `np.fft.fftfreq(n, d)`, which returns the _frequencies_ (not the angular frequencies) for input size $n$ and sample spacing $d$.\n2. `np.fft.fftshift(A)` shifts data so that the zero frequency is in the centre.\n3. `np.fft.ifftshift(A)` inverts this.\n\nHere are some simple examples of their use, applied to a very simple signal consisting of two sinusoids at 12 Hz and 34 Hz:\n\n::: {#e37e91a6 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\ndt=0.01\nfftsize=256\nt=np.arange(fftsize)*dt\n#Generate some fake data at 12 Hz and 34 Hz\ny=np.cos(2*np.pi*12*t)+0.5*np.sin(2*np.pi*34*t)\nplt.plot(t,y)\nplt.xlabel(\"Time\")\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\nText(0.5, 0, 'Time')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](fourier_files/figure-html/cell-2-output-2.png){width=582 height=429}\n:::\n:::\n\n\nNow we can take the FFT and plot vs array index, or against the real frequency (with the given sample spacing of $dt=0.01$)\n\n::: {#9cdb78fd .cell execution_count=2}\n``` {.python .cell-code}\nY=np.fft.fft(y)\n# Plot FFT modulus versus array index\nplt.subplot(2,1,1); plt.plot(abs(Y))\n# Now use the correct frequency coordinates\nf=np.fft.fftfreq(fftsize, dt)\n# Reordering makes a tidier plot...\nY=np.fft.fftshift(Y)\nf=np.fft.fftshift(f)\nplt.subplot(2,1,2); plt.plot(f, abs(Y))\nplt.xlabel(\"Frequency / Hz\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](fourier_files/figure-html/cell-3-output-1.png){width=575 height=429}\n:::\n:::\n\n\nAs we discussed in @sec-properties, the Fourier transform of real valued data has the property $F_n = \\left[F_{-n}\\right]^*$:\n\n::: {#3adc10c1 .cell execution_count=3}\n``` {.python .cell-code}\nplt.subplot(2,1,1); plt.plot(f,Y.real)\nplt.subplot(2,1,2); plt.plot(f,Y.imag)\nplt.xlabel(\"Frequency / Hz\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](fourier_files/figure-html/cell-4-output-1.png){width=578 height=429}\n:::\n:::\n\n\n## Windowing\n\nNote that, even though our signal consists of just a pair of sinusoids, the FFT does _not_ just consist of $\\delta$-functions, which is more obvious on a log-scale: \n\n::: {#d70b8b90 .cell execution_count=4}\n``` {.python .cell-code}\nplt.magnitude_spectrum(y, Fs=100, scale='dB')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](fourier_files/figure-html/cell-5-output-1.png){width=604 height=429}\n:::\n:::\n\n\nThe reason, as you may guess from your knowledge of Fourier transforms, is that our data is of _finite length_. This _windowing_ causes the FFT to have non-zero values outside the frequencies present in the signal, a phenomenon called [spectral leakage](https://en.wikipedia.org/wiki/Spectral_leakage). In our case a sharp window means that the FFT is effectively convolved with the Fourier transform of a top hat function i.e. a sinc function. If the window happens to contain a whole number of wavelengths of the signals present, spectral leakage does not occur. \n\nThe effects of windowing can be mitigated by choosing different [window functions](https://en.wikipedia.org/wiki/Window_function) — with smooth edges for example — to multiply our data, depending on what you are looking for. The rectangular / top hat window has _low dynamic range_: it is not great at distinguishing contributions of different amplitude even when their frequencies differ, as leakage from the large peak may obscure the other, smaller ones. On the other hand it has _high resolution_, meaning that it is good at resolving peaks of similar amplitude that are close in frequency. Expect to be presented with several options (Hamming, Tukey (him again), etc.) when using library functions that perform spectral analysis. \n\n![Importance of windowing LIGO data, from @abbott2020guide. Without appropriate windowing, the entire power spectrum is dominated by spectral leakage following a $1/f^2$ scaling.](../assets/ligo-window.png){width=80%}\n\n# Applications of the FFT\n\nThe FFT has a mindboggling number of applications ranging from signal processing in experimental data, audio and video signals, to numerical simulation, where [spectral methods](https://en.wikipedia.org/wiki/Spectral_method) exploit the FFT as part of the numerical solution of partial differential equations. Here, we'll look at just a couple of examples.\n\n## Signal processing\n\nAs an example, let's look at some of the stages in the analysis of time series data from the LIGO and Virgo experiments on [gravitational wave detection](https://en.wikipedia.org/wiki/First_observation_of_gravitational_waves) that led to the 2017 Nobel prize in physics. I'm following @abbott2020guide closely here, and you should check it out for further details, as well as the [accompanying notebook](https://github.com/losc-tutorial/Data_Guide/blob/master/Guide_Notebook.ipynb) that describes how the analysis is performed in Python. \n\nAs illustrated in @fig-ligo-stages uncovering the signal in the raw data involves a number of processing steps designed to eliminate noise, mostly carried out in the Fourier domain. The guiding principle is that the noise is _stationary_ — meaning that it is described by a random process that does not change in time — while the signal is _transient_. This idea can be used to reduce noise in the data even though low frequency noise completely dominates the raw measurement (top panel). \n\n![Stages in the analysis of LIGO strain data,from @abbott2020guide. Note the scale on the $y$-axis!](../assets/ligo-stages.png){#fig-ligo-stages width=80%}\n\nThe first step is windowing, which we have already discussed, designed to reduce spectral leakage. Next, the data is _whitened_, meaning that the Fourier spectrum is normalized by the spectral density (the power spectrum is made to resemble the power spectrum of white noise)\n$$\n\\tilde d(f)\\longrightarrow \\frac{\\tilde d(f)}{S_n^{1/2}(f)}.\n$$\n\nThe idea behind this step is to prevent high amplitude noise in certain parts of the spectrum from swamping the signal. After this step (third panel, red trace), the low frequency noise has been greatly reduced. \n\nFinally, the data are bandpass filtered with a pass band [35 Hz, 350 Hz], which removes low frequency seismic noise and high frequency (quantum) noise from the detector. Filtering is the Fourier analog of windowing i.e. multiplying by a function to discard certain parts of the signal. At this point, a transient is revealed in the data (bottom panel).\n\nThe next step is to fit this transient with a model that describes the graviational wave physics. An important check on the correctness of this approach is to then analyze the residual — the difference between the data and the model — and to check whether it is described by a stationary noise process (see @fig-ligo-residuals). In such a process the phases of the Fourier components are random and uncorrelated, for example.\n\n![Residuals from the modelled signal, from @abbott2020guide](../assets/ligo-residuals.png){#fig-ligo-residuals width=80%}\n\nHopefully this short summary has emphasized the vital role played throughout by the processing of signals in the Fourier domain, and therefore the importance of the FFT in analyzing time series data.\n\n## Partial differential equations\n\nWe'll illustrate the role of the FFT in the numerical solution of partial differential equations (PDEs) using one that you all know well: the time-dependent Schrödinger equation\n\n$$\ni\\hbar \\frac{\\partial \\psi}{\\partial t} = -\\frac{\\hbar^2}{2m}\\nabla^2 \\psi + V(\\mathbf{r})\\psi.\n$$ {#eq-schrodinger}\n\nWhen the potential is absent, the solutions are superpositions of plane waves\n\n$$\n\\Psi_\\mathbf{k}(\\mathbf{r}, t) = \\exp\\left[-i\\frac{\\hbar \\mathbf{k}^2 t}{2m} +i\\mathbf{k}\\cdot\\mathbf{r}\\right].\n$$ {#eq-momentum}\n\nOn the other hand, if the first term on the right hand side of @eq-schrodinger — representing the kinetic energy — were absent, then the evolution of the wavefunction would be \n\n$$\n\\Psi(\\mathbf{r}, t) = \\Psi(\\mathbf{r}, 0)\\exp\\left[-iV(\\mathbf{r})t/\\hbar\\right].\n$$ {#eq-position}\n\nThe idea behind the [split-step method](https://en.wikipedia.org/wiki/Split-step_method) is that the time evolution can be approximated by alternating the two kinds of time evolution represented by @eq-momentum and @eq-position. In more formal terms, write @eq-schrodinger in operator form as\n\n$$\ni\\hbar \\frac{\\partial \\ket{\\psi}}{\\partial t} = H\\ket{\\psi} = (T+V)\\ket{\\psi}\n$$\n\nThe solution is $\\ket{\\psi(t)} = \\exp(-iHt/\\hbar)\\ket{\\psi}(0)$. The exponential of an operator sum $A+B$ obeys the [Lie product formula](https://en.wikipedia.org/wiki/Lie_product_formula)\n\n$$\ne^{A+B} = \\lim_{n\\to\\infty}\\left( e^{A/n}e^{B/n}\\right)^n.\n$$\n\nThe logic behind this formula is that when the exponent is small, the failure of $A$ and $B$ to commute can be neglected. More precisely,\n$$\ne^{xA}e^{xB} = e^{x(A+B) + O(x^2)}.\n$$\n\nIt turns out that a more accurate approximation is given by the Suzuki—Trotter formula\n\n$$\ne^{xA/2}e^{xB}e^{xA/2} = e^{x(A+B) + O(x^3)}.\n$$\n\nIn any case, the practical algorithm suggested by these formulas is implemented for the Schrödinger equation by switching between real space and Fourier space, where the two kinds of evolution are implemented. Here's a simple 1D example:\n\n::: {#55cdafa5 .cell execution_count=5}\n``` {.python .cell-code}\ndef split_step_schrodinger(psi_0, dx, dt, V, N, x_0 = 0., k_0 = None, m = 1.0, non_linear = False):\n\n\tlen_x = psi_0.shape[0]\n\n\tx = x_0 + dx*np.arange(len_x)\n\n\tdk_x = (2*np.pi)/(len_x*dx)\n\tif k_0 == None:\n\t\tk_0 = -np.pi/dx\n\tk_x = k_0+dk_x*np.arange(len_x)\n\n\tpsi_x = np.zeros((len_x,N), dtype = np.complex128)\n\tpsi_k = np.zeros((len_x,N), dtype = np.complex128)\n\tpsi_mod_x = np.zeros((len_x), dtype = np.complex128)\n\tpsi_mod_k = np.zeros((len_x), dtype = np.complex128)\n\tpsi_x[:,0] = psi_0\n\n\tif not non_linear:\n\t\tV_n = V(x)\n\telse:\n\t\tV_n = V(x,psi_0)\n\n\tdef _compute_psi_mod(j):\n\t\treturn (dx/np.sqrt(2*np.pi))*psi_x[:,j]*np.exp(-1.0j*k_x[0]*x)\n\n\tdef _compute_psi(j):\n\t\tpsi_x[:,j] = (np.sqrt(2*np.pi)/dx)*psi_mod_x*np.exp(1.0j*k_x[0]*x)\n\t\tpsi_k[:,j] = psi_mod_k*np.exp(-1.0j*x[0]*dk_x*np.arange(len_x))\n\n\tdef _x_half_step(j,ft = True):\n\t\tif ft == True:\n\t\t\tpsi_mod_x[:] = np.fft.ifft(psi_mod_k[:])\n\t\tif non_linear:\n\t\t\tV_n[:] = V(x,psi_x[:,j])\n\t\tpsi_mod_x[:] = psi_mod_x[:]*np.exp(-1.0j*(dt/2.)*V_n)\t\n\n\tdef _k_full_step():\n\t\tpsi_mod_k[:] = np.fft.fft(psi_mod_x[:])\n\t\tpsi_mod_k[:] = psi_mod_k[:]*np.exp(-1.0j*k_x**2*dt/(2.*m))\t\t\n\n\tdef _main_loop():\n\t\tpsi_mod_x[:] = _compute_psi_mod(0)\n\n\t\tfor i in range(N-1):\n\t\t\t_x_half_step(i,ft = False)\n\t\t\t_k_full_step()\n\t\t\t_x_half_step(i)\n\t\t\t_compute_psi(i+1)\n\n\t_main_loop()\n\n\treturn psi_x,psi_k,k_x\n```\n:::\n\n\n::: {#fcfb2536 .cell execution_count=6}\n``` {.python .cell-code}\ndef oneD_gaussian(x,mean,std,k0):\n    return np.exp(-((x-mean)**2)/(4*std**2)+ 1j*x*k0)/(2*np.pi*std**2)**0.25\n\ndef V(x):\n    V_x = np.zeros_like(x)\n    V_x[np.where(abs(x) < 0.5)] = 1.5\n    return V_x\n```\n:::\n\n\n::: {#d2e04ade .cell execution_count=7}\n``` {.python .cell-code}\nN_x = 2**11\ndx = 0.05\nx = dx * (np.arange(N_x) - 0.5 * N_x)\n\ndt = 0.01\nN_t = 2000\n\np0 = 2.0\nd = np.sqrt(N_t*dt/2.)\n\npsi_0 = oneD_gaussian(x,x.max()-10.*d, d, -p0)\n\npsi_x,psi_k,k = split_step_schrodinger(psi_0, dx, dt, V, N_t, x_0 = x[0])\n```\n:::\n\n\n::: {#d7f1501f .cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Animation code\"}\nfrom matplotlib.animation import FuncAnimation\n\nreal_psi = np.real(psi_x)\nimag_psi = np.imag(psi_x)\nabsl_psi = np.absolute(psi_x)\nabs_psik = np.absolute(psi_k)\n\nfig = plt.figure(figsize = (10,10))\nax1 = plt.subplot(211)\nline1_R = ax1.plot(x,real_psi[:,0],'b')[0]\nline1_I = ax1.plot(x,imag_psi[:,0],'r')[0]\nline1_A = ax1.plot(x,absl_psi[:,0],'k')[0]\nline_V = ax1.plot(x,0.5*V(x),'k',alpha=0.5)[0]\nax1.set_ylim((real_psi.min(),real_psi.max()))\nax1.set_xlim((x.min(),x.max()))\n\nax2 = plt.subplot(212)\nline2 = ax2.plot(k,abs_psik[:,1],'k')[0]\nax2.set_ylim((abs_psik.min(),abs_psik.max()))\nax2.set_xlim((-10,10))\n\ndef nextframe(arg):\n    line1_R.set_data(x,real_psi[:,10*arg])\n    line1_I.set_data(x,imag_psi[:,10*arg])\n    line1_A.set_data(x,absl_psi[:,10*arg])\n    line2.set_data(k,abs_psik[:,10*arg])\n    \nanimate = FuncAnimation(fig, nextframe, frames = int(N_t/10), interval = 50, repeat = False)\nplt.show()\n```\n:::\n\n\n![(top) Wavepacket colliding with a top hat barrier. Black line is the modulus, while red and blue are the real and imaginary parts. (bottom) Absolute value of the Fourier transform](../assets/gaussian-barrier.mp4)\n\n",
    "supporting": [
      "fourier_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}