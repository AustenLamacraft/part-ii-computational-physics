{
  "hash": "c374f5cc19e7da0db08f93e694dcef66",
  "result": {
    "markdown": "---\ntitle: Solving differential equations with SciPy\nformat:\n  html:\n    code-overflow: wrap\n---\n\n> Newton's fundamental discovery, the one which he considered necessary to keep secret and published only in the form of an anagram, consists of the following: _Data aequatione quotcunque fluentes quantitates involvente, fluxiones invenire; et vice versa_. In contemporary mathematical language, this means: \"It is useful to solve differential equations\".\n>\n>Vladimir Arnold, _Geometrical Methods in the Theory of Ordinary Differential Equations_\n\nWhile Arnold (and Newton) are of course right the problem is that solving differential equations is _not possible in general_. Even the simplest example of a first order ordinary differential equation (ODE) in a single variable\n\n$$\n\\frac{dx}{dt} = f(x, t)\n$$ {#eq-ode}\n\ncannot be solved for general $f(x,t)$ [^1]. Of course, formulating a physical (or whatever) system in terms of differential equations represents a nontrivial step on the road to understanding it, but a lot remains to be done. \n\n[^1]: We'll refer to $t$ as the time in the following, as this is the most common setting in physics.\n\nNumerical analysis of differential equations is a colossal topic in applied mathematics and we are barely going to scratch the surface. The important thing is to be able to access existing solvers (and implement your own if necessary) and crucially to _understand their limitations_. \n\n# Euler's method\n\nThe basic idea behind all ODE solvers is to introduce a discretization of the equation and its solution $x_j\\equiv x(t_j)$ at time points $t_j = hj$ for some step size $h$ and $j=0, 1, \\ldots$. The very simplest approach is called [Euler's method](https://en.wikipedia.org/wiki/Euler_method) [^2] and approximates the derivative on the left hand side of @eq-ode as\n\n[^2]: As featured in [Hidden Figures](https://www.youtube.com/watch?v=v-pbGAts_Fg).\n\n$$\n\\frac{dx}{dt}\\Bigg|_{t=t_j} \\approx \\frac{x_{j+1} - x_j}{h}.\n$$ {#eq-fwd}\n\nRearranging the ODE then gives the update rule\n\n$$\nx_{j+1} = x_j + hf(x_j, t_j).\n$$ {#eq-euler-update}\n\nOnce an _initial condition_ $x_0$ is specified, subsequent values can be obtained by iteration. \n\nNotice that @eq-fwd involved a [forward finite difference](https://en.wikipedia.org/wiki/Finite_difference): the derivative at time $t_j$ was approximated in terms of $x_j$ and $x_{j+1}$ (i.e. one step _forward_ in time). Why do this? So that the update rule @eq-euler-update is an _explicit_ formula for $x_{j+1}$ in terms of $x_j$. This is called an _explicit method_. If we had used the backward derivative we would end up with [backward Euler method](https://en.wikipedia.org/wiki/Backward_Euler_method)\n$$\nx_{j+1} = x_j + hf(x_{j+1}, t_{j+1})\n$$ {#eq-backward-euler-update}\n\nwhich is _implicit_. This means that the update requires an additional step to numerically solve for $x_{j+1}$. Although this is more costly, there are benefits to the backward method associated with stability.\n\n## Truncation error\n\nIn making the approximation @eq-fwd we make an $O(h^2)$ _local truncation error_. To integrate for a fixed time the number of steps required is proportional to $h^{-1}$, which means that the worst case error at fixed time (the _global truncation error_) is $O(h)$. For this reason Euler's method is called _first order_. More sophisticated methods are typically higher order: the SciPy function [scipy.integrate.solve_ivp](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html#r179348322575-1) uses a fifth order method by default.\n\n\nThe [midpoint method](https://en.wikipedia.org/wiki/Midpoint_method) is a simple example of a higher order integration scheme\n\n$$\n\\begin{align}\nk_1 &\\equiv h f(x_j,t_j) \\\\\nk_2 &\\equiv h f(x_i + k_1/2, t_j + h/2) \\\\\nx_{j+1} &= x_j + k_2 +O(h^3)\n\\end{align}\n$$\n\nThe $O(h^2)$ error cancels! The downside is that we have two function evaluations to perform per step, but this is often worthwhile.\n\n## Rounding error\n\nIf you had unlimited computer time you might think you could make the step size $h$ ever smaller in order to make the updates more accurate. This ignores the machine precision $\\epsilon$. The rounding error is roughly $\\epsilon x_j$, and if the $N\\propto h^{-1}$ errors in successive steps can be treated as independent random variables, the relative total rounding error will be $\\propto \\sqrt{N}\\epsilon=\\frac{\\epsilon}{\\sqrt{h}}$ and will dominate for $h$ small.\n\n## Stability\n\nApart from the relatively low accuracy that comes from using a first order method, the Euler method may additionally be unstable, depending on the equation. This can be demonstrated for the linear equation\n\n$$\n\\frac{dx}{dt} = kx\n$$\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef euler(h, t_max, k=1):\n    \"\"\"\n    Solve the equation x' = k x, with x(0) = 1 using\n    the Euler method. \n\n    Integrate from t=0 to t=t_max using stepsize h for\n    num_steps = t_max / h.\n    \n    Returns two arrays of length num_steps: t, the time coordinate, and x_0, the position.\n    \"\"\"\n    num_steps = int(t_max / h)\n    # Allocate return arrays\n    x = np.zeros(num_steps, dtype=np.float32)\n    t = np.zeros(num_steps, dtype=np.float32)\n    x[0] = 1.0  # Initial condition\n    for i in range(num_steps - 1):\n        x[i+1] = x[i] + k * x[i] * h\n        t[i+1] = t[i] + h  # Time step\n    return t, x\n\nk = -2.3\nt_max = 5\nt, x = euler(1, t_max, k)\nplt.plot(t, x, label=\"h=1 Euler\")\nt, x = euler(0.7, t_max, k)\nplt.plot(t, x, label=\"h=0.7 Euler\")\nt = np.linspace(0, t_max, 100)\nplt.plot(t, np.exp(k * t), label=\"exact solution\")\nplt.title(\"k=-2.3\")\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ode_files/figure-html/cell-2-output-1.png){width=569 height=424}\n:::\n:::\n\n\nFor a linear equation the Euler update @eq-euler-update is a simple rescaling\n\n$$\nx_{j+1} = x_j(1 + hk)\n$$\n\nso the region of stability is $|1 + hk|\\leq 1$. You can check that the backward Euler method @eq-backward-euler-update eliminates the instability for $k<0$.\n\n# Using SciPy\n\nComing up with integration schemes is best left to the professionals. Your first port of call for solving ODEs in Python should probably be the [integrate](https://docs.scipy.org/doc/scipy/tutorial/integrate.html) module of the [SciPy](https://scipy.org/) scientific computing library. The function [scipy.integrate.solve_ivp](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html#r179348322575-1) provides a versatile API. \n\nOne important thing to understand is that all these integration schemes apply to systems of _first order_ differential equations. Higher order equations can always be presented as a first order system, at the expense of introducing more equations. For example, in physics we are often concerned with Newton's equation \n\n$$\nm\\frac{d^2 \\mathbf{x}}{dt^2} = \\mathbf{f}(\\mathbf{x},t),\n$$\n\nwhich is three second order equations. We turn this into a first order system by introducing the velocity $\\mathbf{v}=\\dot{\\mathbf{x}}$, giving the six equations\n\n$$\n\\begin{align}\n\\frac{d\\mathbf{x}}{dt} &= \\mathbf{v}\\\\\nm\\frac{d \\mathbf{v}}{dt} &= \\mathbf{f}(\\mathbf{x},t).\n\\end{align}\n$$\n\nAs a simple example, let's consider the pendulum equation\n\n$$\n\\ddot \\theta = -\\sin\\theta\n$$\n\nwhich can be cast as\n\n$$\n\\begin{align}\n\\dot\\theta &= l\\\\\n\\dot l &= -\\sin\\theta\n\\end{align}\n$$\n\nSolving the equation using SciPy just requires us to define a function giving the right hand side of these equations\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndef pendulum(t, y): return [y[1], -np.sin(y[0])]\n# The pendulum equation: y[0] is theta and y[1] is l\n```\n:::\n\n\nand then calling `solve_ivp`\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom scipy.integrate import solve_ivp\nimport matplotlib.pyplot as plt\n\nt_max = 1000\npendulum_motion = solve_ivp(pendulum, [0, t_max], [2, 0], dense_output=True)\n```\n:::\n\n\nThe option `dense_output=True` is used to specify that a continuous solution should be found. What this means in practice is that the returned object `pendulum_motion` has a `sol` property that is an instance of [OdeSolution](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.OdeSolution.html#scipy.integrate.OdeSolution). `sol(t)` returns the computed solution at $t$ (this involves interpolation). We can use this to plot the pendulum's trajectory in the $\\theta- l$ [phase plane](https://en.wikipedia.org/wiki/Phase_plane), along with the contours of the conserved energy function\n\n$$\nE(\\theta, l) = \\frac{1}{2}l^2 - \\cos\\theta\n$$\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Code for plot\"}\nfig, ax = plt.subplots()\n\ntheta = np.linspace(-1.1 * np.pi, 1.1 * np.pi, 60)\nl = np.linspace(-2, 2, 60)\nE = -np.cos(theta[np.newaxis,:]) + (l[:,np.newaxis])**2 / 2\n# Note the use of broadcasting to obtain the energy as a function of the phase space coordinates\n\nxx, yy = np.meshgrid(theta, l)\n\nax.contourf(xx, yy, E, cmap='Reds')\nt = np.linspace(0, t_max, 10000)\nax.plot(*pendulum_motion.sol(t))\nplt.xlabel(r'$\\theta$')\nplt.ylabel(r'$l$')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ode_files/figure-html/cell-5-output-1.png){width=600 height=427}\n:::\n:::\n\n\nThe thickness of the blue line is due to the variation of the energy over the $t=1000$ trajectory (measured in units where the frequency of linear oscillation is $2\\pi$). Notice that we did not have to specify a time step: this is determined _adaptively_ by the solver to keep the estimate of the local error below `atol + rtol * abs(y)`, where `atol` and `rtol` are optional arguments that correspond to the absolute and relative tolerances, with default values of $10^{-6}$ and $10^{-3}$ respectively. The global error is of course much larger. In general, monitoring conserved quantities is a good experimental method for assessing the accuracy of integration.\n\nThe alternative to `dense_output=True` is to track \"events\", which are user-defined points of interest on the trajectory. We supply `solve_ivp` with functions `event(t, x)` whose zeros define the events. We can use events to take a \"cross section\" of higher dimensional motion. As an example let's consider the [HÃ©nonâ€“Heiles system](https://en.wikipedia.org/wiki/H%C3%A9non%E2%80%93Heiles_system), a model chaotic system with origins in stellar dynamics\n\n$$\n\\begin{align}\n\\dot x &= p_x \\\\\n\\dot p_x &= -x -2\\lambda xy \\\\\n\\dot y &= p_y \\\\\n\\dot p_y &=  - y -\\lambda(x^2-y^2).\n\\end{align}\n$$\n\nThese coupled first order systems for the $N$ coordinates and $N$ momenta of a mechanical system with $N$ degrees of freedom are an example of [Hamilton's equations](https://en.wikipedia.org/wiki/Hamiltonian_mechanics). The phase space is now four dimensional and impossible to visualize. \n\nThe conserved energy is\n\n$$\nE = \\frac{1}{2}\\left(p_x^2+p_y^2 + x^2 + y^2\\right) + \\lambda\\left(x^2y-\\frac{1}{3}y^3\\right)\n$$\n\nThe existence of a conserved quantity means that, once the energy is fixed by the initial conditions, the subsequent motion must be confined to a three dimensional submanifold.\n\nWhen $\\lambda=0$ the HH system corresponds to an isotropic 2D harmonic oscillator. In this case not only is the energy conserved, but also the angular momentum \n\n$$\nJ = x p_y - y p_x\n$$\n\nFixing $E$ and $J$ gives a two dimensional submanifold that is in fact a torus (it's not obvious).\n\nIf we take a [PoincarÃ© section](https://en.wikipedia.org/wiki/Poincar%C3%A9_map) with $x=0$ a system with energy $E$ must lie within the curve defined by\n\n$$\nE = \\frac{1}{2}\\left(p_y^2 + y^2\\right) -\\frac{\\lambda}{3}y^3.\n$$\n\nStarting from $x=0$ we can generate a section of given $E$ by solving for $p_x$\n\n$$\np_x = \\sqrt{2E-y^2-p_y^2 + \\frac{2\\lambda}{3}y^3}\n$$\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ndef henon_heiles(t, z, ðœ†): \n    x, px, y, py = z\n    return [px, -x - 2 * ðœ† * x * y, py, -y - ðœ† * (x**2 - y**2)]\n\ndef px(E, y, py, ðœ†):\n    return np.sqrt(2 * E - y**2 - py**2 + 2 * ðœ† * y**3 / 3)\n\ndef section(t, y, ðœ†): return y[0] # The section with x=0\n\nt_max = 10000\nðœ† = 1\nhh_motion = []\nfor E in [1/10, 1/8, 1/6]:\n    hh_motion.append(solve_ivp(henon_heiles, [0, t_max], [0, px(E, 0.1, -0.1, ðœ†), 0.1, -0.1], events=section, args=[ðœ†], atol=1e-7, rtol=1e-7))\n```\n:::\n\n\nWe can then plot a section of the phase space with increasing energy, showing the transition from regular to chaotic dynamics.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Code for plot\"}\nfig, ax = plt.subplots(1, 3)\nenergies = [\"1/10\", \"1/8\", \"1/6\"]\nfor idx, data in enumerate(hh_motion): \n        ax[idx].scatter(*data.y_events[0][:, 2:].T, s=0.1)\n        ax[idx].title.set_text(f\"E={energies[idx]}\")        \n        ax[idx].set_xlabel(r'$y$')\n\nax[0].set_ylabel(r'$p_y$')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ode_files/figure-html/cell-7-output-1.png){width=604 height=442}\n:::\n:::\n\n\n[Here's a nice demo on PoincarÃ© sections](https://duetosymmetry.com/tool/poincare-section-clicker-toy/) if you'd like to learn more.\n\n<!-- TODO Leapfrog?\n\nSymplectic integrator see e.g. \n\nLook at leapfrog?\n\nhttps://github.com/scipy/scipy/issues/12690\n\nProblem is that it's hard to do in scipy\n\nhttps://stackoverflow.com/questions/60338471/lyapunov-spectrum-for-known-odes-python-3 -->\n\n",
    "supporting": [
      "ode_files"
    ],
    "filters": [],
    "includes": {}
  }
}