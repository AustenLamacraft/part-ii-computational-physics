{
  "hash": "8cd06f12a6b56bdf40dca8914a2bb2ea",
  "result": {
    "markdown": "---\ntitle: Monte Carlo methods\nresources:\n  - \"assets/ising.py\"\n---\n\nMany physical phenomena, notably those falling within the domains of statistical mechanics and quantum theory, depend in an essential way on *randomness*. The simulation of these phenomena therefore requires algorithms that incorporate random (or pseudo-random) elements in the most efficient way.\n\n# Sampling from a distribution\n\nLet's suppose that we have a source of samples of a real valued random variable $X$ that follows a particular probability density function $p_X$ [^1]. This means that the probability of drawing a sample in the region $[x, x+dx]$ is $p_X(x)dx$. If we now map the samples using a function $f$, what is the probability density $p_Y$ of $y=f(x)$? The new probability density is defined in just the same way: the probability of $y$ lying in the region $[y, y+dy]$ is $p_Y(y)dy$. Since $x$ is being mapped deterministically to $y$ these two probabilities are therefore the same\n\n[^1]: A common shorthand notation is $x\\sim p_X$.\n\n$$\np_X(x)dx = p_Y(y)dy\n$$\n\nor\n\n$$\np_Y(y)=p_X(x)\\Bigg\\lvert \\frac{dx}{dy}\\Bigg\\rvert= \\frac{p_X(x)}{|f'(x)|},\\qquad x=f^{-1}(y)\n$$\n\nThis formula shows that we can create samples from an arbitrary probability distribution by choosing an invertible map $f$ appropriately. If $p_X$ is a [standard uniform distribution](https://en.wikipedia.org/wiki/Continuous_uniform_distribution) on $[0,1]$ then $f(x)$ is the inverse of the cummulative probability distribution of $Y$ i.e.\n\n$$\nf^{-1}(y) = \\int^y_{-\\infty} p_Y(y')dy'\n$$\n\nThe same approach works in higher dimensions: $\\big\\lvert \\frac{dx}{dy}\\big\\rvert$ is replaced by the inverse of the Jacobian determinant.\n\nThe [Box--Muller transform](https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform) is one example of this idea. Take two independent samples from a standard uniform distribution $u_{1,2}$ and form\n\n$$\n\\begin{align}\nx &= \\sqrt{-2\\log u_1}\\cos(2\\pi u_2)\\\\\ny &= \\sqrt{-2\\log u_1}\\sin(2\\pi u_2).\n\\end{align}\n$$\n\n$x$ and $y$ are independent samples from a [standard normal distribution](https://en.wikipedia.org/wiki/Standard_normal_distribution).\n\nVarious functions are available in the [`numpy.random`](https://numpy.org/doc/stable/reference/random/index.html#module-numpy.random) module to generate random arrays drawn from a variety of distributions. Box--Muller has now been retired in favour of the [Ziggurat algorithm](https://en.wikipedia.org/wiki/Ziggurat_algorithm).\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy.random as random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nmu, sigma = 0, 0.1 # mean and standard deviation\ns = random.normal(mu, sigma, size=10000)\ncount, bins, ignored = plt.hist(s, 30, density=True)\nplt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *\n               np.exp( - (bins - mu)**2 / (2 * sigma**2) ),\n         linewidth=2, color='r')\nplt.xlabel(\"Value\")\nplt.ylabel(\"Frequency\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](monte-carlo_files/figure-html/cell-2-output-1.png){width=592 height=422}\n:::\n:::\n\n\nFor complex multivariate (i.e. high dimensional) distributions there is no general recipe to construct an appropriate $f$. One very recent application of these ideas is in machine learning models called [normalizing flows](https://arxiv.org/abs/1908.09257) that use a mapping $f$ parameterized by a neural network. The workhorse for sampling from complicated distributions is Markov chain Monte Carlo, as we discuss in @sec-mcmc.\n\n# The Monte Carlo method\n\n*Monte Carlo* is the general prefix applied to variety of numerical methods that use randomness in some way. Two of the main classes of problem encountered in physics that come under this heading are:\n\n1.  Interpret a numerical evaluation as an expectation value of some random variable and use sampling to estimate it. [Monte Carlo integration](https://en.wikipedia.org/wiki/Monte_Carlo_integration) is an example of this idea.\n\n2.  Sampling from a complex probability distribution (which may include taking expectation values). Example: [Markov chain Monte Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo).\n\n## Monte Carlo integration\n\nThe technique is exemplified by the following fairly dumb way of estimating $\\pi$\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nmax_samples = 10000\ninside = 0\nareas = []\nfor sample in range(1, max_samples + 1):\n    x = random.uniform(-1, 1)\n    y = random.uniform(-1, 1)\n    \n    if x ** 2 + y ** 2 <= 1:\n        inside += 1\n    areas.append(4 * inside / sample)\n\nplt.plot(np.arange(1, max_samples + 1), areas)\nplt.plot(np.arange(1, max_samples + 1), np.pi * np.ones(max_samples), linestyle='dashed')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](monte-carlo_files/figure-html/cell-3-output-1.png){width=579 height=404}\n:::\n:::\n\n\nIn terms of integration, you can think of this as a way to compute the integral of a function which is one inside the unit disc, and zero outside it.\n\nAlthough it's a silly method, this does illustrate one important feature of Monte Carlo methods in general: that the relative error with $N$ samples is typically $\\propto N^{-1/2}$ (thus at the 1% level for $10^4$ samples) because the variance of a sum of $N$ [iid](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) variables is $\\propto N^{1/2}$.\n\n<!-- TODO General setting. Importance sampling -->\n\nMonte Carlo integration comes into its own for high dimensional problems. For low dimensional integrals the quadrature methods in [`scipy.integrate`](https://docs.scipy.org/doc/scipy/tutorial/integrate.html) are preferable.\n\n## Markov chain Monte Carlo {#sec-mcmc}\n\nSuppose you want to generate configurations at random (i.e. with a uniform distribution) from a \"gas\" of hard disks [^2].\n\n[^2]: This is in fact the original motivation for the development of the technique, see @metropolis1953equation.\n\n![Coins in a shoe box (gas of hard disks). From @krauth1998introduction](assets/hard-spheres.png)\n\nIt's harder than it looks! The first guess you might have is to start adding coins at random, and if you get an overlap, try again until you don't. Obviously this will become inefficient as the box fills up, and most attempts fail. *Worse, it doesn't in fact yield a uniform distribution!*\n\n<!-- TODO Why not? See @widom1966random for an explanation -->\n\nHere's an approach that works:\n\n::: {#exm-metropolis}\n# Metropolis algorithm for hard disks\n\n1.  Fix the number of disks and an initial configuration (some regular lattice configuration, say).\n2.  Pick a disk at random and attempt (or *propose*) to move it by a small random amount (i.e. random direction; random small magnitude).\n3.  If this results in the moved disk intersecting another, *reject* the move, leaving the disk where it is. Otherwise, *accept* the move.\n4.  Repeat 2. and 3. many times.\n:::\n\n![Accepted and rejected moves for hard disks. From ](assets/metropolis.png).\n\nThis is the simplest example of the [Metropolis--Hastings algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm), the first Markov chain Monte Carlo (MCMC) algorithm.\n\nMore generally, the goal of MCMC is to come up with a sequential random process (a **Markov chain**) that generates (usually after many steps) a sample from a particular distribution.\n\nYou've all heard of a [random walk](https://en.wikipedia.org/wiki/Random_walk), perhaps as a model for diffusion. At each step you make a move in a random direction, independently of your earlier moves. After many steps these random moves gives rise to a distribution of possible locations. A random walk is the simplest example of a Markov chain.\n\nMore generally, a [Markov chain](https://en.wikipedia.org/wiki/Markov_chain) is a sequence of random variables $X_n$ with each having a distribution that is is conditional on the value of the previous one, and so is defined in terms of **transition probabilities** $p(X_{n}=x_n|X_{n-1}=x_{n-1})$ (hence they form a \"chain\"). I'm going to immediately drop this cumbersome notation in favour of $p(x_n|x_{n-1})$, a function of $x_n$ and $x_{n-1}$, but in general the function giving the transition probabilities can be different at each step (the random variables could all be different).\n\nThe probability of a particular sequence $X_1=x_1\\ldots X_n=x_n$ is therefore\n\n$$\np(x_n|x_{n-1})p(x_{n-1}|x_{n-2})\\cdots p(x_2|x_{1})p^{(1)}(x_1)\n$$\n\n$X_1$ has no \"parent\" so is not conditional on any other value.\n\nSuppose we don't care about the earlier values and just want to know the **marginal distribution** $p^{(n)}(x_n)$ of the final variable. For a random walk this is easy, as $x_n$ typically represents a displacement that is a sum of iid increments. In general this is not the case, however, as the marginal distribution is\n\n$$\np^{(n)}(x_n)=\\sum_{x_{n-1},\\ldots x_1}p(x_n|x_{n-1})p(x_{n-1}|x_{n-2})\\cdots p(x_2|x_{1})p^{(1)}(x_1)\n$$\n\n(I'm writing all these expressions for discrete random variables, but the continuous version involving probability density functions is straightforward)\n\nThe sums are over all possible values that the random variables might take in the **state space** of the problem. These could be finite or infinite in number.\n\nThings are not as bad as they appear, however, as the marginal distribution can be interpreted as the result of acting $n-1$ times on the vector of values of $p^{(1)}_j\\equiv p^{(1)}(j)$ with the **transition matrix** with elements $\\mathsf{P}_{jk}=p(j|k)$\n\n$$\n\\mathbf{p}^{(n)} = \\mathsf{P}^{n-1}\\mathbf{p}^{(1)}.\n$$\n\nIn a single step the marginal probabilities are updated as\n\n$$\n\\mathbf{p}^{(n)} = \\mathsf{P}^{n}\\mathbf{p}^{(n-1)}.\n$$\n\n$\\mathsf{P}$ has some structure. The matrix elements are positive, as they represent probabilities, and each row sums to one\n\n$$\n\\sum_j \\mathsf{P}_{jk} = 1.\n$$\n\nSuch matrices are called [stochastic](https://en.wikipedia.org/wiki/Stochastic_matrix).\n\nAlthough $p^{(n)}$ --- the probability distribution at the $n$th step --- changes from step to step, you might expect that after many steps it tends to converge to a **stationary distribution** $p^{(n)}\\to\\boldsymbol{\\pi}$. If it exists, this distribution must satisfy\n\n$$\n\\boldsymbol{\\pi} = \\mathsf{P}\\boldsymbol{\\pi}.\n$$ {#eq-stat}\n\nIn other words, it is an eigenvector of $\\mathsf{P}$ with eigenvalue one. This property is guaranteed by the [Perron--Frobenius theorem](https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem) [^3].\n\n[^3]: There is an important caveat. If there are two or more subsets of the state space that are not connected by finite transition probabilities, the probability distribution in each subset evolves independently and there is not a unique stationary distribution. When there *is*, we say that the Markov chain is **ergodic** and the corresponding transition matrix is **irreducible**.\n\nThus $\\mathsf{P}$ determines $\\boldsymbol{\\pi}$. MCMC turns this idea on its head and asks: if there is some $\\boldsymbol{\\pi}$ that I would like to generate samples from, can I find a $\\mathsf{P}$ that has it as a stationary distribution?\n\nThere is a trivial answer to this question. Sure, take $\\mathsf{P}_{jk}=\\boldsymbol{\\pi}_j$. That is, jump straight to the stationary distribution no matter what the starting state. But we are interested in highly complicated distributions over large state spaces (think the Boltzmann distribution for a statistical mechanical system comprised of billions of particles). Thus what we really want is to be able to approach such a complicated distribution by making many transitions with *simple* distributions.\n\nOne more idea is useful before returning to concrete algorithms. The quantity\n\n$$\n\\mathsf{P}_{jk}\\pi_k = p(j|k)\\pi_k = p(j,k)\n$$\n\nis the joint distribution of seeing state $k$ followed by state $j$ in the stationary distribution. A *reversible* Markov chain is one where $p(j,k)=p(k,j)$. Roughly, you can't tell the direction of time because any transition is equally likely to happen forward in time as backward. Random physical processes that respect time reversal symmetry are often modeled as reversible Markov processes.\n\nCombining reversibility with the definition of the stationary state yields the condition of [detailed balance](https://en.wikipedia.org/wiki/Detailed_balance)\n\n$$\n \\mathsf{P}_{jk}\\pi_k = \\pi_j\\mathsf{P}_{kj}.\n$$ {#eq-detailed}\n\nThis condition is stronger than the condition @eq-stat for a stationary state. This makes it easier to check: you don't have to do a sum over a state space index. The Metropolis algorithm @exm-metropolis for the hard disk problem satisfies detailed balance for a stationary distribution that is constant when disks don't intersect and zero when they do.\n\nWhen the stationary distribution $\\boldsymbol{\\pi}$ has more structure, designing an appropriate transition matrix is harder. The idea is to generalize the hard disk approach by separating the transition into a *proposal* distribution $p_\\text{prop}(j|k)$ and an *acceptance* distribution $p_\\text{acc}(a=0,1|j\\leftarrow k)$ that gives the probability of a move from $k$ to $j$ being accepted ($a=1$) or rejected ($a=0$). The probability of moving from $k$ to $j$ is then\n\n$$\np(j|k) = p_\\text{acc}(a=1|j\\leftarrow k) p_\\text{prop}(j|k).\n$$\n\nSubstituting this into the detailed balance condition @eq-detailed gives $$\n\\frac{p_\\text{acc}(a=1|j\\leftarrow k)}{p_\\text{acc}(a=1|k\\leftarrow j)} = \\frac{\\pi_j}{\\pi_k}\\frac{p_\\text{prop}(k|j)}{p_\\text{prop}(j|k)}.\n$$\n\nAny $p_\\text{acc}$ that satisfies this relation for all $j$ and $k$ will do the job. The Metropolis choice is\n\n$$\np_\\text{acc}(a=1|j \\leftarrow k) = \\min\\left(1,  \\frac{\\pi_j}{\\pi_k}\\frac{p_\\text{prop}(k|j)}{p_\\text{prop}(j|k)}\\right).\n$$ {#eq-metropolis}\n\nThis gives an extremely general algorithm, one of the top ten in applied mathematics, according to [one list](https://nhigham.com/2016/03/29/the-top-10-algorithms-in-applied-mathematics/):\n\n::: {#exm-metropolis-gen}\n# Metropolis algorithm\n\n1.  Starting from state $k$ sample a next state $j$ from the proposal distribution $p_\\text{prop}(j|k)$.\n2.  Accept the proposal with probability $p_\\text{acc}(a=1|j \\leftarrow k)$ and move to state $j$. Otherwise reject the proposal and stay in state $k$.\n3.  Repeat 1. and 2. many times.\n:::\n\nMCMC has the benefit of being [embarrassingly parallel](https://en.wikipedia.org/wiki/Embarrassingly_parallel). If you want to average something over $\\boldsymbol{\\pi}$, just run the algorithm many times independently and average the results. This is perfect for parallel computing.\n\nThe Metropolis algorithm has an Achilles' heel, however. To perform a move one has to sample from $p_\\text{prop}(j|k)$ and from $p_\\text{acc}(a|j \\leftarrow k)$. The proposal therefore has to be tractable, like the small shift in position for the hard disk case. This may however, mean that that many of the $j$s suggested correspond to very small $\\pi_j$, and therefore a very low acceptance probability (c.f. @eq-metropolis). For example, in the hard disk case at high density many proposed moves will give rise to overlap of disks and be rejected. This means that many steps are required to have one successful update of the simulation. This kind of slowdown is a common feature of MCMC methods applied to complex distributions.\n\nWe'll see some more examples of MCMC algorithms for statistical mechanical problems in @sec-statmech, and ways in which this problem can be avoided.\n\n\n```{=html}\n<!-- ## Relaxation to equilibrium\n\nTODO\n\nEigenvalues\n\nMaster equation\n\nTransition matrix -->\n```\n\n# Statistical mechanics {#sec-statmech}\n\nStatistical mechanics is a natural source of such complex distributions in physics. Remember the fundamental principle that the probability of finding a statistical mechanical system in a microstate $\\mathbf{x}$ [^4] with energy $\\mathcal{E}(\\mathbf{x})$ is\n\n[^4]: For a classical gas of point particles this would correspond to specifying all the positions and velocities, for example.\n\n$$\np(\\mathbf{x})=\\frac{\\exp\\left[-\\beta \\mathcal{E}(\\mathbf{x})\\right]}{Z},\n$$ {#eq-boltzmann}\n\nwhere $Z$ is a normalizing constant called the partition function and $\\beta=1/k_\\text{B}T$, where $T$ is the temperature and $k_\\text{B}$ is Boltzmann's constant.\n\nThe *central problem* of statistical mechanics is computing ensemble averages of physical quantities, and the *principle difficulty* is the intractability of those averages for large systems. For example, if we are dealing with a classical gas, the configuration space point $\\mathbf{x}$ corresponds to the positions of each of the gas molecules $\\mathbf{x}=(\\mathbf{x}_1,\\ldots \\mathbf{x}_N)$ and an average is a $3N$-dimensional integral. The only situation in which this integral is tractable is when the gas is noninteracting (ideal), in which case the energy function takes the form\n\n$$\n\\mathcal{E}(\\mathbf{x}) = \\sum_{n=1}^N \\mathcal{E}_1(\\mathbf{x}_n)\n$$\n\nwhere $\\mathcal{E}_1(\\mathbf{x})$ is the single particle energy. In this case the integral factorizes. As soon as we introduce interactions between particles of the form\n\n$$\n\\mathcal{E}(\\mathbf{x}) = \\sum_{n<m}^N \\mathcal{E}_2(\\mathbf{x}_n,\\mathbf{x}_m)\n$$\n\nthings get a lot harder. The same issue arises in models involving discrete random variables. The canonical example is the [Ising model](https://en.wikipedia.org/wiki/Ising_model), in which a configuration corresponds to fixing the values of $N$ \"spins\" $\\sigma_n=\\pm 1$ with an energy function of the form\n\n$$\n\\mathcal{E}(\\sigma)=\\sum_n h_n\\sigma_n + \\sum_{m<n} J_{mn}\\sigma_m\\sigma_n.\n$$\n\nThe two terms correspond to a (magnetic) field that acts on each spin and a coupling between spins. As in the gas, it's the latter that causes problems / interest.\n\nThe Ising model comes in a great many flavours according to how the fields and couplings are chosen. They may reflect a lattice structure: $J_{mn}\\neq 0$ for nearest neighbours, say, or longer range. They may be fixed or random, defining an ensemble of models.\n\nThe most pessimistic assessment is that to calculate an average we are going to have sum over $2^N$ configurations. Computing the partition function $Z$ that normalizes the average (or which gives the free energy via $F=-k_\\text{B}T\\log Z$) is another such sum.\n\nMonte Carlo simulation is a much more attractive alternative. MCMC can be used to generate samples from $p(\\sigma)$ which are then used to estimate the averages of interest (e.g. average energy $\\langle\\mathcal{E}(\\sigma)\\rangle$, average magnetization $\\langle\\sum_n \\sigma_n\\rangle$, etc.).\n\n## MCMC updates for the Ising model\n\nHow does MCMC work in practice for the Ising model? To apply the Metropolis alogorithm @exm-metropolis-gen we can use a simple proposal: pick each spin in turn in some order and try to flip it.\n\nThe form of $p(\\sigma)$ means that, although we cannot compute the probabilities explicitly, we can calculate *ratios*, which is all we need for Metropolis. For two configurations that differ only by $\\sigma_n=\\pm 1$ we have\n\n$$\n\\begin{align}\n\\frac{p(\\sigma_n=1|\\sigma_{m\\neq n})}{p(\\sigma_n=-1|\\sigma_{m\\neq n})} &= \\exp\\left[-2\\beta \\left(h_n+\\sum_{m\\neq n} J_{mn}\\sigma_m\\right)\\right]\\\\\n&\\equiv \\exp\\left[-\\beta\\Delta \\mathcal{E}\\right],\n\\end{align}\n$$\n\nwhere $\\Delta \\mathcal{E}$ is the energy difference between two configurations.\n\nOne alternative to Metropolis is the **Heat bath algorithm** (or [Glauber dynamics](https://en.wikipedia.org/wiki/Glauber_dynamics) or [Gibbs sampling](https://en.wikipedia.org/wiki/Gibbs_sampling)) [^5]. The idea behind the name is that, since we can calculate the influence of the spin's environment (the \"bath\"), we can just choose the spin's orientation with the corresponding probabilities. Since there are only two probabilities the ratio is all we need and we get\n\n[^5]: Multiple names are sign that a technique was re-discovered by different communities who don't talk to each other.\n\n$$\np(\\sigma_n=\\pm 1|\\sigma_{m\\neq n}) = \\frac{1}{1+ e^{\\pm\\beta \\Delta \\mathcal{E}}}.\n$$ {#eq-heat-bath}\n\nThe algorithm is then:\n\n::: {#exm-heat-bath}\n# Heat bath algorithm\n\n1.  Pick a spin $n$. [^6]\n2.  Compute $\\Delta E$, the energy difference between $\\sigma_n=\\pm 1$.\n3.  Set $\\sigma_n=\\pm 1$ with probabilities given by @eq-heat-bath.\n4.  Repeat 1-3 many times\n:::\n\n[^6]: This can be done deterministically (e.g. sequentially or in alternating blocks when the model is defined on a [bipartite graph](https://en.wikipedia.org/wiki/Bipartite_graph)) --- which is what is normally called Gibbs sampling --- or at random, which corresponds to Glauber dynamics.\n\nWhat happens if we try and come up with more complicated proposals, flipping many spins at once? For Metropolis, the problem is that without a cleverly designed proposal we will be suggesting moves that are likely to be rejected. For the heat bath algorithm, the more spins we flip, the more complicated the evaluation of the corresponding probabilities ($2^n$ outcomes if we flip $n$ spins).\n\nThe good news is that we *can* do better --- much better --- than the above algorithms. The [Wolff algorithm](https://en.wikipedia.org/wiki/Wolff_algorithm) is one example. This proposes a cluster of spins of the same orientation to be flipped by adding adjacent spins to an initially random chosen spin with probability $p_\\text{add}$. It turns out that for the nearest neighbour Ising model with Ferromagnetic coupling $J<0$ the \"magic\" value $p_\\text{add}=1-e^{2\\beta J}$ is *rejection free*: the probability to flip the whole cluster is always one. This makes for an extremely fast algorithm that is not subject to the usual *critical slowing down* at phase transitions.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Ising model code\"}\nclass IsingModel:\n    def __init__(self, L):\n        self.L = L\n        self.spins = np.random.choice(a=[1, -1], size=(L, L))\n        stagger = np.empty(self.L, dtype = bool)\n        stagger[::2] = True\n        stagger[1::2] = False\n        self.mask = np.logical_xor(stagger[:, np.newaxis], stagger[np.newaxis, :])\n\n    def gibbs_update(self, beta, sublattice):\n        fields = np.roll(self.spins, 1, 0) + np.roll(self.spins, -1, 0) + np.roll(self.spins, 1, 1) + np.roll(self.spins, -1, 1)\n        delta_E = 2 * fields\n        spin_up_probabilities = 1 / (1 + np.exp(- beta * delta_E))\n        new_spins = 2 * (np.random.rand(self.L, self.L) < spin_up_probabilities) - 1\n        self.spins = np.choose(np.logical_xor(sublattice, self.mask), [self.spins, new_spins])\n\n    def glauber_update(self, beta):\n        x, y = np.random.randint(self.L, size=2)\n        fields = 0\n        for neighbour in [((x + 1) % self.L, y), ((x - 1) % self.L, y), (x, (y + 1) % self.L), (x, (y - 1) % self.L)]:\n            fields += self.spins[neighbour]\n        delta_E = 2 * fields\n        spin_up_probability = 1 / (1 + np.exp(- beta * delta_E))        \n        if np.random.rand() < spin_up_probability:\n            self.spins[x, y] = 1\n        else:\n            self.spins[x, y] = -1\n\n    def wolff_update(self, beta):\n        initial_x, initial_y = np.random.randint(self.L, size=2)\n        initial_spin = self.spins[initial_x, initial_y]\n        cluster = deque([(initial_x, initial_y)])\n        add_prob = 1 - np.exp(-2 * beta)\n\n        while len(cluster) != 0:\n            x, y = cluster.popleft()\n            if self.spins[x, y] == initial_spin:\n                self.spins[x, y] *= -1\n                for neighbour in (((x + 1) % self.L, y), ((x - 1) % self.L, y), (x, (y + 1) % self.L), (x, (y - 1) % self.L)):\n                    if self.spins[neighbour] == initial_spin:\n                        if np.random.rand() < add_prob:\n                            cluster.append(neighbour)\n```\n:::\n\n\n```{=html}\n<script src = \"https://cdn.jsdelivr.net/npm/p5@1.4.1/lib/p5.js\"></script>\n```\n\n```{=html}\n<script src=\"https://cdn.jsdelivr.net/pyodide/v0.22.0/full/pyodide.js\"></script>\n```\n\n```{=html}\n<script src = \"assets/ising.js\"></script>\n```\n\n::: {#fig-ising}\n::: {#ising-simulation align=\"center\"}\n:::\n\nGlauber dynamics, Block Gibbs sampling and Wolff updates compared. Change the temperature using the slider. The centre of the slider corresponds to the critical temperature $k_\\text{B}T = 2|J|/\\log(1+\\sqrt{2})\\sim 2.269|J|$.\n:::\n\n# The universe of Monte Carlo methods\n\nMonte Carlo simulation is a vast field with practitioners and specialists across the natural sciences, engineering, machine learning, and statistics. In this section I'll mention a few important topics to give a taste of what's out there. For much more detail take a look at @krauth2006statistical and / or @mackay2003information. The recent set of lectures [Monte Carlo Techniques](https://hef.ru.nl/~tbudd/mct/intro.html) by Timothy Budd also look fantastic.\n\n\n```{=html}\n<!-- Probably the biggest single issue is: how do you kow when your MCMC simulation has reached the stationary distribution $\\boldsymbol{\\pi}$? The pragmatic approach is to monitor the averages of interest (magnetization, say, in the case of the Ising model) over different simulations or over a time interval and see when they stop changing. \n\n\nWe've touched on the issue of the [mixing time](https://en.wikipedia.org/wiki/Markov_chain_mixing_time) in a Markov chain.\n\n1. Finite size effects\n2. Approach to equilibrium\n2. Critical slowing down / loss of ergodicity\n3. Bias of estimators. Importance sampling\n\nExact sampling\n\n[Hamiltonian Monte Carlo](https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo).\n\n\nMultispin encoding: 32 or 64 simulations @jacobs1981multi\n\nhttps://en.wikipedia.org/wiki/Gibbs_sampling\n\nOther updates\n\n\nA huge topic, see @krauth2006statistical for much more\n\nAlso Chapter 29 of @mackay2003information\n\n\nhttps://hef.ru.nl/~tbudd/mct/intro.html\nlooks nice and deals with the queue issue\n\nComment at the end about typicality\n\n\n\nMCMC in Bayesian inference\n\nRelation to Ising models. Community detection. Why not?\n\nhttps://arxiv.org/pdf/cond-mat/0005264.pdf\n\nBayesian inference\n\n## Random number generators {#sec-rng}\n\nComputers are deterministic \n\nThis is covered in some detail in the Nature of Computation\n\nThis is a subject dealt with already\n\nRNGs in Trebst?\n\nFurther reading: refer to [Krauth notes](https://arxiv.org/pdf/cond-mat/9612186.pdf) or book\n\nOther suggestions from Twitter\n\nhttps://roomno308.github.io/blog/MCMC.html\nhttps://maximilianrohde.com/posts/code-breaking-with-metropolis/ -->\n```\n\n",
    "supporting": [
      "monte-carlo_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}