{
  "hash": "4f4d4ef160285674813f94bdb5fd03ad",
  "result": {
    "engine": "jupyter",
    "markdown": "---\nnumber-sections: false\necho: true\nformat:\n  revealjs: \n    theme: [default, reveal_custom.scss]\n    slide-number: true\n    hash: true\n    center: true\n    auto-stretch: false\n    html-math-method: mathjax\n    preview-links: true\n---\n\n\n\n\n![Please fill out the [feedback form](https://cambridge.eu.qualtrics.com/jfe/form/SV_1Aditea2hGfPFHg)](../assets/II%20Computational%20Physics.png){width=50%}\n\n# Linear algebra\n\n- Numerical linear algebra is a huge topic\n\n- We'll look at how common operations are performed in NumPy and SciPy, and some applications in physics\n\n## Linear algebra with NumPy\n\n- Multiplying matrices is easy in NumPy using [`np.matmul`](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html)\n\n- `@` operator gives shortcut\n\n::: {#5a578ded .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nA = np.random.rand(3, 3)\nB = np.random.rand(3, 3)\nnp.matmul(A, B), A @ B\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\n(array([[0.53036987, 1.08642938, 1.11731659],\n        [0.7311074 , 1.3489778 , 1.28995269],\n        [0.82320476, 1.23275261, 1.02558923]]),\n array([[0.53036987, 1.08642938, 1.11731659],\n        [0.7311074 , 1.3489778 , 1.28995269],\n        [0.82320476, 1.23275261, 1.02558923]]))\n```\n:::\n:::\n\n\n---\n\n- You'll get an error if your matrices don't match...\n\n::: {#9985314d .cell execution_count=2}\n``` {.python .cell-code}\nC = np.random.rand(2, 3)\nD = np.random.rand(4, 2)\nC @ D\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ValueError</span>                                Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[2], line 3</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> C <span style=\"color:rgb(98,98,98)\">=</span> np<span style=\"color:rgb(98,98,98)\">.</span>random<span style=\"color:rgb(98,98,98)\">.</span>rand(<span style=\"color:rgb(98,98,98)\">2</span>, <span style=\"color:rgb(98,98,98)\">3</span>)\n<span class=\"ansi-green-fg ansi-bold\">      2</span> D <span style=\"color:rgb(98,98,98)\">=</span> np<span style=\"color:rgb(98,98,98)\">.</span>random<span style=\"color:rgb(98,98,98)\">.</span>rand(<span style=\"color:rgb(98,98,98)\">4</span>, <span style=\"color:rgb(98,98,98)\">2</span>)\n<span class=\"ansi-green-fg\">----&gt; 3</span> C <span style=\"color:rgb(98,98,98)\">@</span> D\n\n<span class=\"ansi-red-fg\">ValueError</span>: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 4 is different from 3)</pre>\n```\n:::\n\n:::\n:::\n\n\n- If either `A` or `B` has rank greater than two: treat as a stack of matrices, with each matrix in last two indices\n\n- Usual broadcasting rules then apply to remaining indices\n\n::: {#061afedd .cell execution_count=3}\n``` {.python .cell-code}\nC = np.random.rand(4, 3, 3)\nD = np.random.rand(3, 3)\n(C @ D).shape\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n(4, 3, 3)\n```\n:::\n:::\n\n\n---\n\n- Several library functions to perform matrix and vector algebra, including \n\n    - [`np.dot`](https://numpy.org/doc/stable/reference/generated/numpy.dot.html#numpy.dot) (dot product) \n    - [`np.vdot`](https://numpy.org/doc/stable/reference/generated/numpy.vdot.html#numpy.vdot) (dot product including complex conjugation)\n    - [`np.trace`](https://numpy.org/doc/stable/reference/generated/numpy.trace.html), etc.\n\n---\n\n- Most versatile is [`np.einsum`](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html): explicitly translate expressions using the Einstein summation convention \n\n$$\n\\left[A\\cdot B\\right]_{ik} = \\sum_{j} A_{ij}B_{jk} = A_{ij}B_{jk}\n$$\n\n::: {#bfbf7caa .cell execution_count=4}\n``` {.python .cell-code}\nnp.einsum('ij,jk->ik', A, B)\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\narray([[0.53036987, 1.08642938, 1.11731659],\n       [0.7311074 , 1.3489778 , 1.28995269],\n       [0.82320476, 1.23275261, 1.02558923]])\n```\n:::\n:::\n\n\n$$\n\\operatorname{tr}\\left[A\\cdot B\\right] = \\sum_{i,j} A_{ij}B_{ji} = A_{ij}B_{ji}\n$$\n\n::: {#fd8301c1 .cell execution_count=5}\n``` {.python .cell-code}\nnp.einsum('ij,ji->', A, B)\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n2.904936900205072\n```\n:::\n:::\n\n\n---\n\n- In what order should multiple contractions be evaluated?\n- How should loops be nested? \n- Recall: should evaluate $M_1 M_2\\cdots M_n \\mathbf{v}$ as $O(N^2)$ matrix-vector multiplications, rather than $O(N^3)$ matrix-matrix multiplications followed by matrix-vector multiplication\n\n- In general no efficient algorithm to find best way!\n\n - `einsum` can use a \"greedy\" algorithm (contracting pair of tensors with lowest cost at each step) \n \n - Information on contraction order provided by [`np.einsum_path`](https://numpy.org/doc/stable/reference/generated/numpy.einsum_path.html)\n\n---\n\n- Inversion ([`np.linalg.inv`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html))\n- Calculation of determinant ([`np.linalg.det`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.det.html))\n- Eigenvalues and eigenvectors ([`np.linalg.eig`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html) or [`np.linalg.eigh`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eigh.html#numpy.linalg.eigh) for hermitian problems)\n\n...inherit their complexity from $O(N^3)$ complexity of matrix multiplication\n\n## Power method\n\n- Better methods available if only want _largest_ (or smallest) eigenvalue and eigenvector (e.g. QM ground state)\n \n - Simplest is [Power method](https://en.wikipedia.org/wiki/Power_iteration)\n \n    - Start from arbitrary vector $\\mathbf{b}_0$\n    - Multiply repeatedly by matrix $A$\n    - Result tends to _dominant_ eigenvector (largest magnitude eigenvalue) \n\n---\n\n- Convenient to normalize each time\n\n$$\n\\mathbf{b}_{k+1} = \\frac{A \\mathbf{b}_k}{\\lVert A\\mathbf{b}_k\\rVert}\n$$\n\n$$\n\\lim_{k\\to\\infty}\\mathbf{b}_k = \\mathbf{v}_\\text{dom}\n$$\n\n$$\nA\\mathbf{v}_\\text{dom} = \\lambda_\\text{dom}\\mathbf{v}_\\text{dom}\n$$\n\n---\n\n- Already met this idea when we discussed [Markov chains](monte-carlo.qmd#sec-mcmc)\n\n- Relevant matrix was $\\mathsf{P}_{jk}=p(j|k)\\geq 0$ of transition probabilities, which is stochastic:\n\n$$\n\\sum_j \\mathsf{P}_{jk} = 1\n$$\n\n- Guarantees that dominant eigenvalue is one and dominant eigenvector has interpretation of the stationary distribution\n\n## PageRank\n\n- Google's [PageRank](https://en.wikipedia.org/wiki/PageRank) algorithm assesses relative importance of webpages based on structure of links between them\n\n![Larry Page](../assets/page.jpeg){width=40%}\n\n---\n\n- PageRank imagines a [web crawler](https://en.wikipedia.org/wiki/Web_crawler) navigating between pages according to a transition matrix $\\mathsf{P}$\n\n- Stationary distribution $\\boldsymbol{\\pi}$ satisfying$$\n\\mathsf{P}\\boldsymbol{\\pi} = \\boldsymbol{\\pi}\n$$\nthen interpreted as a ranking\n\n- Page $j$ more important than page $k$ if $\\boldsymbol{\\pi}_j>\\boldsymbol{\\pi}_k$\n\n---\n\n- Problem if Markov chain is _non-ergodic_: state space breaks up into several independent components, leading to nonunique $\\boldsymbol{\\pi}$\n\n$$\n\\begin{equation}\n\\mathsf{P}=\\begin{pmatrix}\n0 & 1 & 0 & 0 \\\\\n1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0\n\\end{pmatrix}\n\\end{equation}\n$$\n\n- First two pages and last two do not link to each other, so\n$\\boldsymbol{\\pi}^T = \\begin{pmatrix}\n\\pi_1 &\n\\pi_1 &\n\\pi_2 &\n\\pi_2 \n\\end{pmatrix}$ is a stationary state for any $\\pi_{1,2}$\n\n---\n\n- Way out is to modify Markov chain to restore ergodicity and give a unique $\\boldsymbol{\\pi}$\n\n- Crawler either moves as before with probability $\\alpha$ _or_ moves with probability $1-\\alpha$ to a random webpage\n$$\n\\alpha\\mathsf{P} + (1-\\alpha)\\mathbf{t} \\mathbf{e}^T\n$$\nwhere $\\mathbf{e}^T= (1, 1, \\ldots 1)$ and $\\mathbf{t}$ is a \"teleporting\" vector \n\n- Matrix has positive (i.e. $>0$) entries: there is a unique stationary state (and hence ranking)\n\n- Further modification is required to teleport away from \"dangling\" webpages without any outgoing links\n\n---\n\n- Power method is basis of more sophisticated algorithms such as [Lanczos iteration](https://en.wikipedia.org/wiki/Lanczos_algorithm)\n\n- All based on idea that matrix-vector products preferred over matrix-matrix products\n\n- Provide only incomplete information about eigenvalues and eigenvectors\n\n## Sparsity\n\n- Many matrices that we meet in physical applications are _sparse_, meaning that most of elements are zero\n\n$$\n\\left[-\\frac{\\hbar^2}{2m}\\frac{d^2}{dx^2} + V(x)\\right]\\psi(x) = E\\psi(x)\n$$\n\n$$\n\\frac{d^2}{dx^2} \\sim \\frac{1}{\\Delta x^2}\\begin{pmatrix}\n-2 &  1 & 0 & 0 & 0 & \\cdots & 1 \\\\\n1 &  -2 & 1 & 0 & 0 & \\cdots & 0 \\\\\n0 &  1 & -2 & 1 & 0 & \\cdots & 0 \\\\\n\\cdots &  \\cdots & \\cdots & \\cdots & \\cdots & \\cdots & \\cdots \\\\\n1 &  0 & 0 & \\cdots & 0 & 1 & -2 \n\\end{pmatrix}\n$$\n\n---\n\n- No point iterating over a _whole row_ to multiply this matrix into a vector representing wavefunction \n\n- Only need to store the non-zero values of a matrix (and their locations)\n\n- Variety of data structures implemented in [`scipy.sparse`](https://docs.scipy.org/doc/scipy/reference/sparse.html#) module\n\n- Matrix operations from [`scipy.sparse.linalg`](https://docs.scipy.org/doc/scipy/reference/sparse.linalg.html) use these structures effciently\n\n---\n\n- Alternative approach: pass matrix operations in `scipy.sparse.linalg` a _function_ which performs matrix-vector multiplication\n\n- Instantiate a [`LinearOperator`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.LinearOperator.html#scipy.sparse.linalg.LinearOperator) with the function\n\n- We'll see an example shortly\n\n## Singular value decomposition\n\n- Often faced with need to truncate large matrices in some way due to limits of finite storage space or processing time\n\n- What is \"right\" way to perform truncation?\n\n- [Singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition) (SVD) is natural in some settings: statistics, signal processing, quantum mechanics...\n\n---\n\n- SVD is an example of [matrix factorization](https://en.wikipedia.org/wiki/Matrix_decomposition)\n\n$$\nM = U\\Sigma V\n$$\n\n- $U$ and $V$ unitary; $\\Sigma$ diagonal with non-negative real entries\n\n- SVD _completely general_: applies to _rectangular matrices_ \n\n- If $M$ is $m\\times n$, $U$ is $m\\times m$, $V$ is $n\\times n$, and $\\Sigma$ is $m\\times n$\n\n- $\\min(m,n)$ diagonal elements $\\sigma_i$ of $\\Sigma$ are _singular values_\n\n---\n\n## Geometrical interpretation\n\n- Columns of $V$ define an orthonormal basis $\\mathbf{v}_i\\in \\mathbb{C}^n$ ($i=1,\\ldots n$)\n\n- $U$ defines a basis $\\mathbf{u}_i\\in \\mathbb{C}^m$ $i=1,\\ldots m$\n\n- If we act on $\\mathbf{v}_i$ with $M$ (to the left) we get $\\sigma_i \\mathbf{u}_i$\n\n---\n\n- Number of nonzero singular values is [rank](https://en.wikipedia.org/wiki/Rank_(linear_algebra)) of matrix\n\n- Equal to number of independent rows or columns\n\n- For general rectangular matrix rank is $\\min(m,n)$\n\n---\n\n- Often want to produce [low rank approximation](https://en.wikipedia.org/wiki/Low-rank_approximation)\n\n- Need to define how well the matrix is approximated by the lower rank matrix $M_r$ of rank $r<\\min(m,n)$\n\n- One possibility: [Frobenius norm](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm) of $M-M_r$ should be as small as possible.\n\n- Frobenius norm $\\|A\\|_{\\mathrm{F}}$ of a matrix $A$ is\n\n$$\n\\begin{equation}\n\\|A\\|_{\\mathrm{F}}^2=\\sum_i^m \\sum_j^n\\left|A_{i j}\\right|^2\n\\end{equation}\n$$\n\n---\n\n- $\\exists$ following simple result: best low rank approximation of rank $r$ obtained by taking SVD and discarding all but $r$ largest singular values from matrix $\\Sigma$\n\n- i.e. retain only $r$ \"most important\" directions $\\mathbf{v}_i\\in \\mathbb{C}^n$ and $\\mathbf{u}_i\\in \\mathbb{C}^m$\n\n---\n\n- SVD can be computed using [`np.linalg.svd`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html)\n\n- Try this fun demo of [image compression with SVD](http://timbaumann.info/svd-image-compression-demo/)\n\n## SVD in quantum mechanics\n\n- SVD arises naturally in QM of composite systems (with two subsystems)\n\n- Example: two spins $\\mathbf{S}_A$ and $\\mathbf{S}_B$\n\n- Hilbert space of each spin has dimension $n_{A,B}\\equiv 2S_{A,B}+1$, where $\\mathbf{S}_{A,B}^2=S_{A,B}(S_{A,B}+1)$ (e.g. 2 for spin-1/2). \n\n---\n\n- General state lives in $n_A\\times n_B$ dimensional Hilbert space \n\n- Write in terms of basis vectors $\\ket{a}_A$ and $\\ket{b}_B$ for A and B subsystems as\n$$\n\\ket{\\Psi_{AB}} = \\sum_{a=1}^{n_A}\\sum_{b=1}^{n_B} \\psi_{ab}\\ket{a}_A\\ket{b}_B \n$$\n\n- Regard components $\\psi_{ab}$ as a matrix and perform SVD\n\n- Equivalent to finding new orthonormal bases $\\ket{\\tilde n}_{A,B}$ for two spaces s.t. action of $\\psi_{ab}$ maps between basis vectors of two susbsystems (with rescaling)\n\n---\n\n- In new bases, state $\\ket{\\Psi_{AB}}$ is\n\n$$\n\\ket{\\Psi_{AB}} = \\sum_{n=1}^{\\min(n_A,n_B)} \\lambda_n\\ket{\\tilde n}_A\\ket{\\tilde n}_B.\n$$\n\n- Note _single sum_  c.f. double sum before\n\n- This is [Schmidt decomposition](https://en.wikipedia.org/wiki/Schmidt_decomposition): just a restatement of SVD\n\n---\n\n$$\n\\ket{\\Psi_{AB}} = \\sum_{n=1}^{\\min(n_A,n_B)} \\lambda_n\\ket{\\tilde n}_A\\ket{\\tilde n}_B\n$$\n\n- Singular values — or Schmidt coefficients — quantify [entanglement](https://en.wikipedia.org/wiki/Quantum_entanglement) of state ([2022 Nobel prize](https://www.nobelprize.org/prizes/physics/2022/summary/))\n\n- If only one nonzero singular value state is a _product state_: no correlations between subsystems\n\n- This might not have been evident in original form\n\n--- \n\n- [Bell states](https://en.wikipedia.org/wiki/Bell_state) of two spin-1/2 subsystems\n\n$$\n\\begin{equation}\n\\left|\\Psi^{+}\\right\\rangle=\\frac{1}{\\sqrt{2}}\\left(|0\\rangle_A \\otimes|1\\rangle_B+|1\\rangle_A \\otimes|0\\rangle_B\\right)\n\\end{equation}\n$$\n\n- Already written in Schmidt form and the two singular values are both $\\frac{1}{\\sqrt{2}}$, indicating maximal entanglement\n\n## Other applications of SVD\n\n- Applications of SVD in [recommender systems](https://en.wikipedia.org/wiki/Matrix_factorization_(recommender_systems)), described in [this blog post](https://sifter.org/~simon/journal/20061211.html)\n\n## Many body physics & tensor methods\n\n- We saw that state of a quantum system composed of two subsystems represented as a matrix $\\psi_{ab}$\n\n- Generalizes to $N$ subsystems: wavefunction may a tensor of rank $N$: $\\psi_{a_1,\\ldots a_N}$\n\n- Each index $a_i$ ranges over dimension of Hilbert space of corresponding subsystem\n\n## Penrose tensor notation\n\n- Graphical notation due to [Roger Penrose](https://en.wikipedia.org/wiki/Penrose_graphical_notation)\n\n- Rank $N$ tensor is represented as blob with $N$ legs:\n\n![The tensor notation. Source: [Glen Evenbly](https://www.tensors.net/)](../assets/tensor-pics.png){width=70%}\n\n---\n\n- Represent tensor contractions by connecting legs:\n\n![Tensor contractions. Left: matrix multiplication. Right: something more complicated](../assets/contractions.png)\n\n## Example: ground state of spin chain\n\n- Simplest example: [Heisenberg chain](https://en.wikipedia.org/wiki/Quantum_Heisenberg_model) for spin-1/2:\n\n$$\nH = \\sum_{j=1}^N \\left[\\sigma^x_j \\sigma^x_{j+1} + \\sigma^y_j \\sigma^y_{j+1} + \\sigma^z_j \\sigma^z_{j+1} \\right]\n$$\n\n- $\\sigma^{x,y,z}$ are usual Pauli matrices and subscript $j$ means that matrix acts only the $j$th index of the wavefunction\n\n- Usually impose periodic boundary conditions: $\\sigma^a_{j+N}=\\sigma^a_j$\n\n---\n\n- In tensor diagram notation\n\n![State and Hamiltonian of a spin chain](../assets/h-chain.png)\n\n---\n\n- Number of components of wavefunction $\\psi_{a_1,\\ldots a_N}$ is $2^N$\n\n$$\nH\\ket{\\Psi} = E\\ket{\\Psi}\n$$\n\n- Naive matrix-vector multiplication has complexity $O(2^{2N})$: very bad idea. \n\n- Take advantage of _structure_, using sparsity of Hamiltonian\n\n- $H=\\sum_j h_{j,j+1}$ consists of a sum of _local terms_, each acting on neighbouring pair\n\n---\n\n- Define function that acts on wavefunction with each $h_{j,j+1}$\n\n::: {#42a909f0 .cell execution_count=6}\n``` {.python .cell-code}\n# by Glen Evenbly (c) for www.tensors.net, (v1.2) - last modified 6/2019\n\ndef doApplyHam(psiIn: np.ndarray,\n               hloc: np.ndarray,\n               N: int,\n               usePBC: bool):\n  \"\"\"\n  Applies local Hamiltonian, given as sum of nearest neighbor terms, to\n  an input quantum state.\n  Args:\n    psiIn: vector of length d**N describing the quantum state.\n    hloc: array of ndim=4 describing the nearest neighbor coupling.\n    N: the number of lattice sites.\n    usePBC: sets whether to include periodic boundary term.\n  Returns:\n    np.ndarray: state psi after application of the Hamiltonian.\n  \"\"\"\n  d = hloc.shape[0]\n  psiOut = np.zeros(psiIn.size)\n  for k in range(N - 1):\n    # apply local Hamiltonian terms to sites [k,k+1]\n    psiOut += np.tensordot(hloc.reshape(d**2, d**2),\n                           psiIn.reshape(d**k, d**2, d**(N - 2 - k)),\n                           axes=[[1], [1]]).transpose(1, 0, 2).reshape(d**N)\n\n  if usePBC:\n    # apply periodic term\n    psiOut += np.tensordot(hloc.reshape(d, d, d, d),\n                           psiIn.reshape(d, d**(N - 2), d),\n                           axes=[[2, 3], [2, 0]]\n                           ).transpose(1, 2, 0).reshape(d**N)\n\n  return psiOut\n```\n:::\n\n\n---\n\n- Complexity is $O(N 2^N)$\n\n- $2^N$ arises from tensor contractions over indices of a pair of sites _for each_ assignment of the remaining $N-2$ indices\n\n- Still exponential, but exponentially better than $O(4^N)$!\n\n---\n\n- Use this to instantiate a `LinearOperator` which is passed into eigenvalue solver ([`scipy.sparse.linalg.eigsh`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.eigsh.html#scipy.sparse.linalg.eigsh))\n\n::: {#b41e0b08 .cell execution_count=7}\n``` {.python .cell-code}\n\"\"\"\nby Glen Evenbly (c) for www.tensors.net, (v1.2) - last modified 06/2020\n\"\"\"\n\nfrom scipy.sparse.linalg import LinearOperator, eigsh\nfrom timeit import default_timer as timer\n\n# Simulation parameters\nmodel = 'XX'  # select 'XX' model of 'ising' model\nNsites = 18  # number of lattice sites\nusePBC = True  # use periodic or open boundaries\nnumval = 1  # number of eigenstates to compute\n\n# Define Hamiltonian (quantum XX model)\nd = 2  # local dimension\nsX = np.array([[0, 1.0], [1.0, 0]])\nsY = np.array([[0, -1.0j], [1.0j, 0]])\nsZ = np.array([[1.0, 0], [0, -1.0]])\nsI = np.array([[1.0, 0], [0, 1.0]])\nif model == 'XX':\n  hloc = (np.real(np.kron(sX, sX) + np.kron(sY, sY))).reshape(2, 2, 2, 2)\n  EnExact = -4 / np.sin(np.pi / Nsites)  # Note: only for PBC\nelif model == 'ising':\n  hloc = (-np.kron(sX, sX) + 0.5 * np.kron(sZ, sI) + 0.5 * np.kron(sI, sZ)\n          ).reshape(2, 2, 2, 2)\n  EnExact = -2 / np.sin(np.pi / (2 * Nsites))  # Note: only for PBC\n\n\n# cast the Hamiltonian 'H' as a linear operator\ndef doApplyHamClosed(psiIn):\n  return doApplyHam(psiIn, hloc, Nsites, usePBC)\n\n\nH = LinearOperator((2**Nsites, 2**Nsites), matvec=doApplyHamClosed)\n\n# do the exact diag\nstart_time = timer()\nEnergy, psi = eigsh(H, k=numval, which='SA')\ndiag_time = timer() - start_time\n\n# check with exact energy\nEnErr = Energy[0] - EnExact  # should equal to zero\n\nprint('NumSites: %d, Time: %1.2f, Energy: %e, EnErr: %e' %\n      (Nsites, diag_time, Energy[0], EnErr))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumSites: 18, Time: 4.21, Energy: -2.303508e+01, EnErr: 1.065814e-14\n```\n:::\n:::\n\n\n---\n\n-  Check out [Glen Evenbly's site](https://www.tensors.net/) is you'd like to learn more about these methods!\n\n",
    "supporting": [
      "linear_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {}
  }
}