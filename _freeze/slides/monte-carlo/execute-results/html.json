{
  "hash": "c41a079f2643582c8c64cd0b0a53fb3d",
  "result": {
    "markdown": "---\nnumber-sections: false\nformat:\n  revealjs: \n    slide-number: true\n    hash: true\n    center: true\n    auto-stretch: false\n    theme: default\n    html-math-method: mathjax\n    preview-links: true\n\n---\n\n# Monte Carlo methods\n\nA set of algorithms that use *randomness* in an essential way\n\n---\n\n## Sampling from a distribution\n\n- Suppose we have a source of samples of random variable $X$ described by a particular probability density function $p_X$\n\n- Common shorthand notation is $x\\sim p_X$\n\n- By definition probability of drawing a sample in the region $[x, x+dx]$ is $p_X(x)dx$\n\n---\n\n- Now map samples using a function $f$\n\n- What is the probability density $p_Y$ of $y=f(x)$? \n\n- New probability density is defined in same way: probability of $y$ lying in region $[y, y+dy]$ is $p_Y(y)dy$\n\n---\n\n- Since $x$ is mapped deterministically to $y$ these two probabilities must be the same\n$$\np_X(x)dx = p_Y(y)dy\n$$\nor\n$$\np_Y(y)=p_X(x)\\Bigg\\lvert \\frac{dx}{dy}\\Bigg\\rvert= \\frac{p_X(x)}{|f'(x)|},\\qquad x=f^{-1}(y)\n$$\n\n- Can create samples from an arbitrary probability distribution by choosing an invertible map $f$ appropriately\n\n---\n\n$$\np_Y(y)=p_X(x)\\Bigg\\lvert \\frac{dx}{dy}\\Bigg\\rvert= \\frac{p_X(x)}{|f'(x)|},\\qquad x=f^{-1}(y)\n$$\n\n- If $p_X$ is [standard uniform distribution](https://en.wikipedia.org/wiki/Continuous_uniform_distribution) on $[0,1]$ then $f(x)$ is the inverse of the cummulative probability distribution of $Y$\n\n$$\nf^{-1}(y) = \\int^y_{-\\infty} p_Y(y')dy'\n$$\n\n---\n\n$$\np_Y(y)=p_X(x)\\Bigg\\lvert \\frac{dx}{dy}\\Bigg\\rvert= \\frac{p_X(x)}{|f'(x)|},\\qquad x=f^{-1}(y)\n$$\n\n- Same approach works in higher dimensions: $\\big\\lvert \\frac{dx}{dy}\\big\\rvert$ is replaced by the inverse of the Jacobian determinant.\n\n---\n\n## [Box--Muller transform](https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform)\n\n - Take two independent samples from a standard uniform distribution $u_{1,2}$ and form\n$$\n\\begin{align}\nx &= \\sqrt{-2\\log u_1}\\cos(2\\pi u_2)\\\\\ny &= \\sqrt{-2\\log u_1}\\sin(2\\pi u_2).\n\\end{align}\n$$\n$x$ and $y$ are independent samples from a [standard normal distribution](https://en.wikipedia.org/wiki/Standard_normal_distribution).\n\n---\n\n- Various functions available in [`numpy.random`](https://numpy.org/doc/stable/reference/random/index.html#module-numpy.random) module to generate random arrays drawn from a variety of distributions\n\n- [Box--Muller now retired in favour of [Ziggurat algorithm](https://en.wikipedia.org/wiki/Ziggurat_algorithm)]\n\n---\n\n```python\nimport numpy.random as random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nmu, sigma = 0, 0.1 # mean and standard deviation\ns = random.normal(mu, sigma, size=10000)\ncount, bins, ignored = plt.hist(s, 30, density=True)\nplt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *\n               np.exp( - (bins - mu)**2 / (2 * sigma**2) ),\n         linewidth=2, color='r')\nplt.xlabel(\"Value\")\nplt.ylabel(\"Frequency\")\nplt.show()\n```\n\n---\n\n::: {.cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![](monte-carlo_files/figure-revealjs/cell-2-output-1.png){width=816 height=422}\n:::\n:::\n\n\n---\n\n- For complex multivariate (i.e. high dimensional) distributions there is no general recipe to construct an appropriate $f$\n\n- One very recent application of these ideas is in machine learning models called [normalizing flows](https://arxiv.org/abs/1908.09257) that use a mapping $f$ parameterized by a neural network. \n\n- Workhorse for sampling from complicated distributions is _Markov chain Monte Carlo_\n\n---\n\n# Monte Carlo method\n\n- General term for variety of numerical methods that use randomness in some way. \n\n- Two main classes encountered in physics are:\n\n  1.  Interpret a numerical evaluation as an expectation value of some random variable and use sampling to estimate it. [Monte Carlo integration](https://en.wikipedia.org/wiki/Monte_Carlo_integration) is an example of this idea.\n\n  2.  Sampling from a complex probability distribution (which may include taking expectation values). Example: [Markov chain Monte Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo).\n\n---\n\n## Monte Carlo integration\n\n- Dumb way to find $\\pi$\n\n```python\nmax_samples = 10000\ninside = 0\nareas = []\nfor sample in range(1, max_samples + 1):\n    x = random.uniform(-1, 1)\n    y = random.uniform(-1, 1)\n    \n    if x ** 2 + y ** 2 <= 1:\n        inside += 1\n    areas.append(4 * inside / sample)\n\nplt.plot(np.arange(1, max_samples + 1), areas)\nplt.plot(np.arange(1, max_samples + 1), np.pi * np.ones(max_samples), linestyle='dashed')\nplt.show()\n```\n\n---\n\n::: {.cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](monte-carlo_files/figure-revealjs/cell-3-output-1.png){width=794 height=404}\n:::\n:::\n\n\n- Important feature of MC: relative error with $N$ samples is typically $\\propto N^{-1/2}$ because the variance of a sum of $N$ [iid](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) variables is $\\propto N^{1/2}$\n\n---\n\n- Suppose we have a multidimensional integral to evaluate over some domain $D$ of volume $V_D$\n$$\nI(f,D) = \\int_D f(\\mathbf{x}) d\\mathbf{x}\n$$\n\n- If we can sample points uniformly within $D$, then an estimate for the integral is\n\n$$\nI(f,D) = \\frac{V_D}{N}\\sum_{i=1}^N f(\\mathbf{x}_i)\n$$\n\n---\n\n$$\nI(f,D) = \\frac{V_D}{N}\\sum_{i=1}^N f(\\mathbf{x}_i)\n$$\n\n- Why does this work? Uniform distribution has constant probability density $1/V_D$ so average of $f(\\mathbf{x}_i)$ with respect to this uniform distribution is just our integral\n\n$$\n\\bar f = \\frac{1}{V_D}\\int f(\\mathbf{x})d\\mathbf{x}\n$$\n\n- Take many samples to estimate this average\n\n---\n\n- In our simple example $f(\\mathbf{x})$ would be a \"top hat\" function that is one inside the circle\n\n- Monte Carlo integration comes into its own for high dimensional problems. For low dimensional integrals quadrature methods in [`scipy.integrate`](https://docs.scipy.org/doc/scipy/tutorial/integrate.html) are preferable:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom scipy import integrate\nintegrate.quadrature(np.cos, 0, np.pi / 2)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n(0.9999999999999536, 3.9611425250996035e-11)\n```\n:::\n:::\n\n\n- As for ODE solvers, there is a lot of detail in the implementation to do with how intervals are chosen, etc.\n\n## Importance sampling\n\n- If our function $f(\\mathbf{x})$ has regions where it is very small, there is not much point in sampling its value there\n\n- If we can sample from a distribution where samples tend to fall in the region where $f(\\mathbf{x})$ is large, it will probably be better to use that\n\n---\n\n- In this case we calculate _weighted_ average using the probability density $p_\\text{sample}(\\mathbf{x})$ of the sampling distribution\n\n$$\nI(f,D, p_\\text{sample}) = \\frac{1}{N}\\sum_{i=1}^N \\frac{f(\\mathbf{x}_i)}{p_\\text{sample}(\\mathbf{x}_i)}\n$$\n\n- This works because the average of each term is just the integral we want\n\n$$\n\\overline{\\frac{f(\\mathbf{x})}{p_\\text{sample}(\\mathbf{x})}} = \\int \\frac{f(\\mathbf{x})}{p_\\text{sample}(\\mathbf{x})} p_\\text{sample}(\\mathbf{x})d\\mathbf{x} = \\int f(\\mathbf{x})d\\mathbf{x}\n$$\n\n---\n\n$$\nI(f,D, p_\\text{sample}) = \\frac{1}{N}\\sum_{i=1}^N \\frac{f(\\mathbf{x}_i)}{p_\\text{sample}(\\mathbf{x}_i)}\n$$\n\n- Benefit of this approach is that it can lead to a drastic reduction in the variance of the estimator\n\n- Extreme example: if $f(\\mathbf{x})\\propto p_\\text{sample}(\\mathbf{x})$, and even a single sample leads to perfect estimate with no uncertainty! \n\n---\n\n- General technique is called [Importance sampling](https://en.wikipedia.org/wiki/Importance_sampling)\n\n- In naive form, needs both an explicit form for $p_\\text{sample}(\\mathbf{x})$ and the ability to generate samples, which is rather restrictive\n\n- There are many elaborations of this idea, including multiple distributions as well as adaptive sampling to \"discover\" the right region for sampling\n\n## Markov chain Monte Carlo {#sec-mcmc}\n\n- Suppose you want to generate configurations at random (i.e. with a uniform distribution) from a \"gas\" of hard disks \n\n![Coins in a shoe box (gas of hard disks)](../assets/hard-spheres.png)\n\n---\n\n- First guess: start adding coins at random, and if you get an overlap, try again until you don't\n\n- Obviously this will become inefficient as the box fills up, and most attempts fail. \n\n- *Worse, it doesn't in fact yield a uniform distribution!*\n\n<!-- TODO Why not? See @widom1966random for an explanation -->\n\n---\n\n::: {#exm-metropolis}\n# Metropolis algorithm for hard disks\n\n1.  Fix the number of disks and an initial configuration (some regular lattice configuration, say).\n2.  Pick a disk at random and attempt (or *propose*) to move it by a small random amount (i.e. random direction; random small magnitude).\n3.  If this results in the moved disk intersecting another, *reject* the move, leaving the disk where it is. Otherwise, *accept* the move.\n4.  Repeat 2. and 3. many times.\n:::\n\n---\n\n![Accepted and rejected moves for hard disks. From ](../assets/metropolis.png).\n\n---\n\n- Simplest example of the [Metropolis--Hastings algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm), the first Markov chain Monte Carlo (MCMC) algorithm.\n\n- Goal of MCMC is to come up with a sequential random process (a **Markov chain**) that generates (usually after many steps) a sample from a particular distribution\n\n---\n\n## Markov chains\n\nYou know the [random walk](https://en.wikipedia.org/wiki/Random_walk), perhaps as a model for diffusion\n\n- At each step make a move in a random direction, independently of your earlier moves\n\n- After many steps these random moves gives rise to a distribution of possible locations\n\n- A random walk is the simplest example of a _Markov chain_\n\n---\n\n![](../assets/Random_walk_25000.svg)\n\n---\n\n- A [Markov chain](https://en.wikipedia.org/wiki/Markov_chain) is a sequence of random variables $X_n$, each having a distribution that is is conditional on the value of the previous one\n\n- Defined in terms of **transition probabilities** $p(X_{n}=x_n|X_{n-1}=x_{n-1})$ (hence a \"chain\")\n\n- Pprobability of a particular sequence $X_1=x_1\\ldots X_n=x_n$ is therefore\n\n$$\np(x_n|x_{n-1})p(x_{n-1}|x_{n-2})\\cdots p(x_2|x_{1})p^{(1)}(x_1)\n$$\n\n- $X_1$ has no \"parent\" so is not conditional on any other value\n\n---\n\n- What is **marginal distribution** $p^{(n)}(x_n)$ of the final variable? \n\n- For a random walk this is easy, as $x_n$ typically represents a displacement that is a sum of iid increments\n\n- In general, marginal distribution is\n\n$$\np^{(n)}(x_n)=\\sum_{x_{n-1},\\ldots x_1}p(x_n|x_{n-1})p(x_{n-1}|x_{n-2})\\cdots p(x_2|x_{1})p^{(1)}(x_1)\n$$\n\n- Sums over all possible values that the random variables might take in the **state space** of the problem\n\n- Could be finite or infinite in number.\n\n---\n\n$$\np^{(n)}(x_n)=\\sum_{x_{n-1},\\ldots x_1}p(x_n|x_{n-1})p(x_{n-1}|x_{n-2})\\cdots p(x_2|x_{1})p^{(1)}(x_1)\n$$\n\n- Marginal distribution results from acting $n-1$ times on the vector of values of $p^{(1)}_j\\equiv p^{(1)}(j)$ with **transition matrix** with elements $\\mathsf{P}_{jk}=p(j|k)$\n\n$$\n\\mathbf{p}^{(n)} = \\mathsf{P}^{n-1}\\mathbf{p}^{(1)}.\n$$\n\n- In a single step the marginal probabilities are updated as\n\n$$\n\\mathbf{p}^{(n)} = \\mathsf{P}^{n}\\mathbf{p}^{(n-1)}\n$$\n\n---\n\n- $\\mathsf{P}$ has some structure:\n\n  - Matrix elements are positive, as they represent probabilities, and each row sums to one \n  $$\n  \\sum_j \\mathsf{P}_{jk} = 1.\n  $$\n  (since $\\mathsf{P}_{jk}=p(j|k)$)\n  - Such matrices are called [stochastic](https://en.wikipedia.org/wiki/Stochastic_matrix)\n\n---\n\n$$\n\\mathbf{p}^{(n)} = \\mathsf{P}^{n}\\mathbf{p}^{(n-1)}\n$$\n\n- After many steps $\\mathbf{p}^{(n)}$ tends to converge to a **stationary distribution** $p^{(n)}\\to\\boldsymbol{\\pi}$\n\n- If it exists, this distribution must satisfy\n\n$$\n\\boldsymbol{\\pi} = \\mathsf{P}\\boldsymbol{\\pi}.\n$$\n\n- An eigenvector of $\\mathsf{P}$ with eigenvalue one. This property is guaranteed by the [Perron--Frobenius theorem](https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem)\n\n---\n\n> I would like to generate samples from some $\\boldsymbol{\\pi}$ Can I find a $\\mathsf{P}$ that has $\\boldsymbol{\\pi}$ as a stationary distribution?\n\n- If so I can simulate the Markov chain for a long time and end up with a sample from $\\boldsymbol{\\pi}$.\n\n- Trivial answer: $\\mathsf{P}_{jk}=\\boldsymbol{\\pi}_j$. That is, jump straight to the stationary distribution no matter what starting state\n\n- __But__ we are interested in highly complicated distributions over large state spaces\n\n- What we really want is to approach a complicated distribution by making many transitions with *simple* distributions\n\n---\n\n$$\n\\mathsf{P}_{jk}\\pi_k = p(j|k)\\pi_k = p(j,k)\n$$\nis the joint distribution of seeing state $k$ followed by state $j$ in the stationary distribution\n\n- *Reversible* Markov chain is one where $p(j,k)=p(k,j)$\n\n- Any transition is equally likely to happen forward in time as backward\n\n- Combining reversibility with definition of stationary state yields the condition of [detailed balance](https://en.wikipedia.org/wiki/Detailed_balance)\n\n$$\n \\mathsf{P}_{jk}\\pi_k = \\pi_j\\mathsf{P}_{kj}.\n$$\n\n---\n\n$$\n \\mathsf{P}_{jk}\\pi_k = \\pi_j\\mathsf{P}_{kj}.\n$$\n\n- Stronger than conditionfor a stationary state: easier to check\n\n- Metropolis algorithm for hard disks satisfies detailed balance for a uniform distribution\n\n- When $\\boldsymbol{\\pi}$ has more structure, designing an appropriate transition matrix is harder\n\n---\n\n- Idea is to generalize hard disk approach by separating transition into a *proposal* distribution $p_\\text{prop}(j|k)$ and an *acceptance* distribution $p_\\text{acc}(a=0,1|j\\leftarrow k)$ that gives probability of a move from $k$ to $j$ being accepted ($a=1$) or rejected ($a=0$)\n\n- Probability of moving from $k$ to $j$ is then\n\n$$\np(j|k) = p_\\text{acc}(a=1|j\\leftarrow k) p_\\text{prop}(j|k).\n$$\n\n- Substituting this into the detailed balance condition\n$$\n\\frac{p_\\text{acc}(a=1|j\\leftarrow k)}{p_\\text{acc}(a=1|k\\leftarrow j)} = \\frac{\\pi_j}{\\pi_k}\\frac{p_\\text{prop}(k|j)}{p_\\text{prop}(j|k)}.\n$$\n\n---\n\n$$\n\\frac{p_\\text{acc}(a=1|j\\leftarrow k)}{p_\\text{acc}(a=1|k\\leftarrow j)} = \\frac{\\pi_j}{\\pi_k}\\frac{p_\\text{prop}(k|j)}{p_\\text{prop}(j|k)}.\n$$\n\n- Any $p_\\text{acc}$ that satisfies this relation for all $j$ and $k$ will do\n\n- Metropolis choice is\n\n$$\np_\\text{acc}(a=1|j \\leftarrow k) = \\min\\left(1,  \\frac{\\pi_j}{\\pi_k}\\frac{p_\\text{prop}(k|j)}{p_\\text{prop}(j|k)}\\right)\n$$\n\n---\n\n::: {#exm-metropolis-gen}\n# Metropolis algorithm\n\n1.  Starting from state $k$ sample a next state $j$ from the proposal distribution $p_\\text{prop}(j|k)$.\n2.  Accept the proposal with probability $p_\\text{acc}(a=1|j \\leftarrow k)$ and move to state $j$. Otherwise reject the proposal and stay in state $k$.\n3.  Repeat 1. and 2. many times.\n:::\n\n---\n\n- MCMC has benefit of being [embarrassingly parallel](https://en.wikipedia.org/wiki/Embarrassingly_parallel)\n\n- If you want to average something over $\\boldsymbol{\\pi}$, just run the algorithm many times independently and average the results\n\n- This is perfect for parallel computing\n\n---\n\n- To perform a move one has to sample from $p_\\text{prop}(j|k)$ _and_ from $p_\\text{acc}(a|j \\leftarrow k)$\n\n- Proposal has to be tractable, like the small shift in position for the hard disk case\n\n- Many of the $j$s suggested may correspond to very small $\\pi_j$, and therefore a very low acceptance probability\n\n- For hard disks at high density many proposed moves will give rise to overlap of disks and be rejected\n\n- Many steps are required to have one successful update of the simulation. This kind of slowdown is a common feature of MCMC methods applied to complex distributions\n\n\n# Statistical mechanics {#sec-statmech}\n\n- Natural source of complex distributions in physics\n\n- Probability of finding a statistical mechanical system in a microstate $\\mathbf{x}$ with energy $\\mathcal{E}(\\mathbf{x})$ is\n$$\np(\\mathbf{x})=\\frac{\\exp\\left[-\\beta \\mathcal{E}(\\mathbf{x})\\right]}{Z},\n$$\n$Z$ is normalizing constant (partition function)\n\n- *Central problem* of statistical mechanics is computing ensemble averages of physical quantities\n\n- *Principal difficulty* is intractability of these averages\n\n---\n\n- For a classical gas, $\\mathbf{x}$ gives position of each gas molecule $\\mathbf{x}=(\\mathbf{x}_1,\\ldots \\mathbf{x}_N)$ and average is a $3N$-dimensional integral\n\n- Integral is tractable is when gas is noninteracting (ideal)\n$$\n\\mathcal{E}(\\mathbf{x}) = \\sum_{n=1}^N \\mathcal{E}_1(\\mathbf{x}_n)\n$$\nas it factorizes over the particle coordinates\n- As soon as we introduce interactions things get a lot harder\n$$\n\\mathcal{E}(\\mathbf{x}) = \\sum_{n<m}^N \\mathcal{E}_2(\\mathbf{x}_n,\\mathbf{x}_m)\n$$\n\n\n---\n\n- Same issue arises in models involving discrete random variables\n\n- In [Ising model](https://en.wikipedia.org/wiki/Ising_model) a configuration corresponds to fixing values of $N$ \"spins\" $\\sigma_n=\\pm 1$ with an energy function of the form\n\n$$\n\\mathcal{E}(\\sigma)=\\sum_n h_n\\sigma_n + \\sum_{m<n} J_{mn}\\sigma_m\\sigma_n.\n$$\n\n- Coupling between spins causes problems / interest\n\n---\n\n- Most pessimistic assessment is that to calculate an average we are going to have sum over $2^N$ configurations\n\n- Computing the partition function $Z$ that normalizes the average (or which gives the free energy via $F=-k_\\text{B}T\\log Z$) is another such sum\n\n- Monte Carlo simulation is a much more attractive alternative\n\n- MCMC can be used to generate samples from $p(\\sigma)$ which are then used to estimate the averages of interest (e.g. average energy $\\langle\\mathcal{E}(\\sigma)\\rangle$, average magnetization $\\langle\\sum_n \\sigma_n\\rangle$, etc.)\n\n## MCMC updates for the Ising model\n\n- Simple proposal: pick each spin in turn in some order and try to flip it.\n\n- Form of $p(\\sigma)$ means that, although we cannot compute the probabilities explicitly, we can calculate *ratios*\n\n- For two configurations that differ only by $\\sigma_n=\\pm 1$ we have\n$$\n\\begin{align}\n\\frac{p(\\sigma_n=1|\\sigma_{m\\neq n})}{p(\\sigma_n=-1|\\sigma_{m\\neq n})} &= \\exp\\left[-2\\beta \\left(h_n+\\sum_{m\\neq n} J_{mn}\\sigma_m\\right)\\right]\\\\\n&\\equiv \\exp\\left[-\\beta\\Delta \\mathcal{E}\\right]\n\\end{align}\n$$\nwhere $\\Delta \\mathcal{E}$ is the energy difference \n\n---\n\n- Alternative to Metropolis is **Heat bath algorithm** (or [Glauber dynamics](https://en.wikipedia.org/wiki/Glauber_dynamics) or [Gibbs sampling](https://en.wikipedia.org/wiki/Gibbs_sampling)) \n\n- Since we can calculate the influence of the spin's environment (the \"bath\"), we can just choose the spin's orientation with corresponding probabilities\n\n- Only two probabilities so the ratio is all we need and we get\n\n\n$$\np(\\sigma_n=\\pm 1|\\sigma_{m\\neq n}) = \\frac{1}{1+ e^{\\pm\\beta \\Delta \\mathcal{E}}}\n$$ \n\n---\n\n::: {#exm-heat-bath}\n# Heat bath algorithm\n\n1.  Pick a spin $n$\n2.  Compute $\\Delta E$, the energy difference between $\\sigma_n=\\pm 1$.\n3.  Set $\\sigma_n=\\pm 1$ with probabilities given above\n4.  Repeat 1-3 many times\n:::\n\n---\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nclass IsingModel:\n    def __init__(self, L):\n        self.L = L\n        self.spins = np.random.choice(a=[1, -1], size=(L, L))\n        stagger = np.empty(self.L, dtype = bool)\n        stagger[::2] = True\n        stagger[1::2] = False\n        self.mask = np.logical_xor(stagger[:, np.newaxis], stagger[np.newaxis, :])\n\n    def gibbs_update(self, beta, sublattice):\n        fields = np.roll(self.spins, 1, 0) + np.roll(self.spins, -1, 0) + np.roll(self.spins, 1, 1) + np.roll(self.spins, -1, 1)\n        delta_E = 2 * fields\n        spin_up_probabilities = 1 / (1 + np.exp(- beta * delta_E))\n        new_spins = 2 * (np.random.rand(self.L, self.L) < spin_up_probabilities) - 1\n        self.spins = np.choose(np.logical_xor(sublattice, self.mask), [self.spins, new_spins])\n\n    def glauber_update(self, beta):\n        x, y = np.random.randint(self.L, size=2)\n        fields = 0\n        for neighbour in [((x + 1) % self.L, y), ((x - 1) % self.L, y), (x, (y + 1) % self.L), (x, (y - 1) % self.L)]:\n            fields += self.spins[neighbour]\n        delta_E = 2 * fields\n        spin_up_probability = 1 / (1 + np.exp(- beta * delta_E))        \n        if np.random.rand() < spin_up_probability:\n            self.spins[x, y] = 1\n        else:\n            self.spins[x, y] = -1\n\n    def wolff_update(self, beta):\n        initial_x, initial_y = np.random.randint(self.L, size=2)\n        initial_spin = self.spins[initial_x, initial_y]\n        cluster = deque([(initial_x, initial_y)])\n        add_prob = 1 - np.exp(-2 * beta)\n\n        while len(cluster) != 0:\n            x, y = cluster.popleft()\n            if self.spins[x, y] == initial_spin:\n                self.spins[x, y] *= -1\n                for neighbour in (((x + 1) % self.L, y), ((x - 1) % self.L, y), (x, (y + 1) % self.L), (x, (y - 1) % self.L)):\n                    if self.spins[neighbour] == initial_spin:\n                        if np.random.rand() < add_prob:\n                            cluster.append(neighbour)\n```\n:::\n\n\n---\n\n\n```{=html}\n<script src = \"https://cdn.jsdelivr.net/npm/p5@1.4.1/lib/p5.js\"></script>\n```\n\n```{=html}\n<script src=\"https://cdn.jsdelivr.net/pyodide/v0.22.0/full/pyodide.js\"></script>\n```\n\n```{=html}\n<script src = \"../assets/ising.js\"></script>\n```\n\n\n::: {#ising-simulation align=\"center\"}\n:::\n\n",
    "supporting": [
      "monte-carlo_files"
    ],
    "filters": [],
    "includes": {}
  }
}