{
  "hash": "16e305657a2c6121103fd7ae13d5d2a4",
  "result": {
    "markdown": "---\nnumber-sections: false\nformat:\n  revealjs: \n    slide-number: true\n    hash: true\n    center: true\n    auto-stretch: false\n    theme: default\n    html-math-method: mathjax\n    preview-links: true\n---\n\n# Algorithms and computational complexity\n\n## First example: multiplication\n\n- Big numbers harder than small numbers. How much harder? \n\n+---+---+---+---+---+\n|   |   | 1 | 2 | 3 |\n|   |   |   |   |   |\n|   | × | 3 | 2 | 1 |\n+===+===+===+===+===+\n| _ | _ | 1 | 2 | 3 |\n|   |   |   |   |   |\n| _ | 2 | 4 | 6 |   |\n|   |   |   |   |   |\n| 3 | 6 | 9 |   |   |\n|   |   |   |   |   |\n+---+---+---+---+---+\n| 3 | 9 | 4 | 8 | 3 |\n|   |   |   |   |   |\n+---+---+---+---+---+\n\n---\n\n- For $n$ digits have to perform $n^2$ single digit multiplications\n\n- Add together $n$ resulting $n$-digit numbers\n\n - Overall number of operations is proportional to $n^2$: $\\times 2$ number of digits will make problem four times harder\n\n- Exactly how long this takes will depend on many things, but you can't get away from the basic quadratic scaling law of this algorithm\n\n## Defining complexity\n\n- The __complexity__ of a problem refers to this _scaling of the number of steps involved_\n\n- Difficulty of particular task (or calculation) may vary considerably —  $100\\times 100$ is easy, for example\n\n- Instead ask about how a particular _general_ algorithm performs on a _class_ of tasks\n\n- In CS multiplication of $n$ digit numbers is a __problem__. _Particular pair_ of $n$ digit numbers is an __instance__\n\n-  Above algorithm for multiplication that has __quadratic complexity__, or \"$O(n^2)$ complexity\" (say \"order $n$ squared\"). \n\n---\n\n- Description only keeps track of how the difficulty scales with the size of the problem\n\n    1. Allows us to gloss over what exactly we mean by a _step_. Are we working in base ten or binary? Looking the digit multiplications up in a table or doing them from scratch?\n\n    2. Don't have to worry about how the algorithm is implemented exactly in software or hardware, what language used, and so on\n\n    3. It is important to know whether our code is going to run for twice as long, four times as long, or $2^{10}$ times as long\n\n## Best / worst / average case\n\n- Consider _search_: finding an item in an (unordered) list of length $n$. How hard is this? \n\n- Have to check every item until you find the one you are looking for, so this suggests the complexity is $O(n)$\n\n- Could be lucky and get it first try (or in first ten tries). The _best case complexity_ of search is $O(1)$.\n\n- Worst thing that could happen is that the sought item is last: the _worst case complexity_ is $O(n)$\n\n- On average, find your item near the middle of the list on attempt $\\sim n/2$, so the _average case complexity_ is $O(n/2)$. This is the same as $O(n)$ (constants don't matter)\n\n---\n\n- Thus for _linear search_ we have:\n\n|              | Complexity |\n|--------------|------------|\n| Best case    |   $O(1)$   |\n| Worst case   |   $O(n)$   |\n| Average case |   $O(n)$   |\n\n---\n\nWe can check the average case performance experimentally by using randomly chosen lists:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\ndef linear_search(x, val):\n    \"Return True if val is in x, otherwise return False\"\n    for item in x:\n        if item == val:\n            return True\n    return False\n```\n:::\n\n\n---\n\n::: {.cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](complexity_files/figure-revealjs/cell-3-output-1.png){width=823 height=444}\n:::\n:::\n\n\n---\n\n- \"Experimental noise\" arises because don't have full control over exactly what computer is doing at any moment: lots of other processes running. \n\n- Takes a while to reach the linear regime: overhead associated with starting the program\n\n---\n\n## Polynomial complexity\n\n- You've already learnt a lot of algorithms in mathematics (even if you don't think of them this way) \n\n- Let's revisit some them through lens of computational complexity\n\n---\n\n## Matrix-vector multiplication\n\n- Multiplying a $n$-dimensional vector by a $n\\times n$ matrix? \n\n$$\n\\begin{align}\n\\sum_{j=1}^n M_{ij}v_j\n\\end{align}\n$$\n\n- Sum contains $n$ terms, and have to perform $n$ such sums\n\n- Thus the complexity of this operation is $O(n^2)$. \n\n---\n\n## Matrix-matrix multiplication\n\n$$\n\\sum_{j} A_{ij}B_{jk}\n$$\n\n- Involves $n$ terms for each of the $n^2$ assignments of $i$ and $k$. Complexity: $O(n^3)$\n\n--- \n\n- To calculate $M_1 M_2\\cdots M_n \\mathbf{v}$, do _not_ calculate the matrix product first, but instead\n\n$$\n M_1\\left(M_2\\cdots \\left(M_n \\mathbf{v}\\right)\\right)\n$$\n\n[Wikipedia has a nice summary](https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations) of computational complexity of common mathematical operations\n\n---\n\n- If algorithm has complexity $O(n^p)$ for some $p$ it has _polynomial complexity_\n\n- Useful heuristic is that if you have $p$ nested loops that range over $\\sim n$, the complexity is $O(n^p)$ \n\n## Better than linear?\n\n- Seems obvious that for search you can't do better than linear\n\n- What if the list is _ordered_? (numerical for numbers, or lexicographic for strings)\n\n- Extra structure allows gives [binary search](https://en.wikipedia.org/wiki/Binary_search_algorithm) that you may have seen before\n\n- Look in middle of list and see if item you seek should be in the top half or bottom half\n\n- Take the relevant half and divide it in half again to determine which quarter of the list your item is in, and so on\n\n---\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndef binary_search(x, val):\n    \"\"\"Peform binary search on x to find val. If found returns position, otherwise returns None.\"\"\"\n\n    # Intialise end point indices\n    lower, upper = 0, len(x) - 1\n\n    # If values is outside of interval, return None \n    if val < x[lower] or val > x[upper]:\n        return None\n\n    # Perform binary search\n    while True:\n                \n        # Compute midpoint index (integer division)\n        midpoint = (upper + lower)//2\n\n        # Check which side of x[midpoint] val lies, and update midpoint accordingly\n        if val < x[midpoint]:\n            upper = midpoint - 1\n        elif val > x[midpoint]:\n            lower = midpoint + 1\n        elif val == x[midpoint]:  # found, so return\n            return midpoint\n       \n        # In this case val is not in list (return None)\n        if upper < lower:\n            return None\n```\n:::\n\n\n---\n\n::: {.cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](complexity_files/figure-revealjs/cell-5-output-1.png){width=800 height=444}\n:::\n:::\n\n\n---\n\n- If length is a power of 2 i.e. $n=2^p$, we are going to need $p$ bisections to locate our value\n\n- Complexity is $O(\\log n)$ (we don't need to specify the base as overall constants don't matter)\n\n---\n\n## Exponentiation by squaring\n\n- _Exponentiation_ is problem of raising a number $b$ (the base) to the $n$th power\n\n- Multiply the number by itself $n$ times: linear scaling\n\n- There's a quicker way, since\n$$\n\\begin{align}\nb^2 &= b\\cdot b\\\\\nb^4 &= b^2\\cdot b^2\\\\\nb^4 &= b^4\\cdot b^4\n\\end{align}\n$$\n- Only have to do _three_ multiplications! \n\n---\n\n- Exponentiation by this method (called [exponentiation by squaring](https://en.wikipedia.org/wiki/Exponentiation_by_squaring)) is $O(\\log n)$\n\n- To handle powers that aren't a power of $2$\n\n$$\nb^n = \\begin{cases}\n    b^{n/2} \\cdot b^{n/2} & \\text{if $n$ even} \\\\\n    b \\cdot b^{n-1} & \\text{if $n$ odd}\n\\end{cases}\n$$\n\n---\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ndef exp(b, n):\n    if n == 0:\n        return 1\n    elif n % 2 == 0:\n        return exp(b, n // 2)**2\n    else:\n        return b * exp(b, n - 1) \n\nexp(2, 6)\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n64\n```\n:::\n:::\n\n\n- Implementation is _recursive_: `exp(b, n)` _calls itself_\n\n- Only calls itself with _lower values of the exponent $n$_\n\n- Process continues until we hit $n=0$, and 1 is returned by the first part of the `if ... else`\n\n---\n\n- Any recursive function has to have a _base case_ to avoid an infinite regress\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndef exp_no_base_case(b, n):\n    if n % 2 == 0:\n        return exp_no_base_case(b, n // 2)**2\n    else:\n        return b * exp_no_base_case(b, n - 1) \n\nexp_no_base_case(2, 6)\n```\n\n::: {.cell-output .cell-output-error}\n```\nRecursionError: maximum recursion depth exceeded in comparison\n```\n:::\n:::\n\n\n--- \n\n- Exponentiation can be done efficiently\n\n- Finding the logarithm can't!\n\n- More precisely, work with modular arithmetic e.g. do all operations modulo some prime $p$\n\n- Then for $b, y=0,\\ldots p-1$ we are guaranteed that there is some number $x$ such that $b^x=y$: [discrete logarithm](https://en.wikipedia.org/wiki/Discrete_logarithm)\n\n- Finding this number is hard: no known method for computing it efficiently\n\n- Certain [public-key cryptosystems](https://en.wikipedia.org/wiki/Public-key_cryptography) are based on the difficulty of the discrete log (for carefully chosen $b$, $p$ and $y$)\n\n## Exponential complexity\n\n- [Fibonacci numbers](https://en.wikipedia.org/wiki/Fibonacci_number)\n$$\n 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233 ...\n$$\n\n$$\n\\text{Fib}(n) = \\text{Fib}(n-1) + \\text{Fib}(n-2)\n$$\n\n- $\\text{Fib}(n)$ is defined in terms of lower values of $n$, so a recursive definition possible\n\n---\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ndef fib(n):\n    if n == 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fib(n - 1) + fib(n - 2)\n\nfib(13)\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n233\n```\n:::\n:::\n\n\n- First two terms are base cases\n\n- Actually a terrible way of calculating $\\text{Fib}(n)$!\n\n---\n\n![Recursive tree for calculating Fibonacci numbers](../assets/fibonacci.png)\n\n- There are huge amounts of duplication! \n\n---\n\n- Complexity of this algorithm actually grows _exponentially_ with $n$: because of branching structure algorithm is $O(2^n)$. \n\n- Calculating Fibonacci numbers the sensible way (i.e. the way you do it in your head) gives an $O(n)$ algorithm\n\n---\n\n- Exp complexity not just down to poor algos!\n\n- Possible to come up with problems that definitely _can't_ be solved faster than exponentially \n\n- [Towers of Hanoi](https://en.wikipedia.org/wiki/Tower_of_Hanoi) is one famous example\n\n---\n\n- Simulation of quantum system with $n$ qubits believed to have complexity $O(2^n)$\n\n- Big part of hype surrounding quantum computers\n\n---\n\n- $\\exists$ problems whose solution, once found, is easy to check\n\n- Discrete logarithm is one example\n\n- Checking involves exponentiation, and exponentiation is $O(\\log n)$ in size of numbers, or $O(n)$ in number of digits\n\n- Question of whether efficient (i.e. polynomial) algorithms _always_ exist for problems which are easy to check _the_ outstanding problem in computer science: [P _vs_ NP](https://en.wikipedia.org/wiki/P_versus_NP_problem)\n\n- P is class of problems with polynomial time algorithms and NP is class with solutions checkable in polynomial time\n\n- Are these two classes the same or do they differ? \n\n---\n\n- Computer scientists obsess about P vs. NP, but finding an algorithm that changes the exponent e.g. from cubic to quadratic, is still a big deal!\n\n---\n\n## Sorting\n\n- Turning a list or array into a sorted list (conventionally in _ascending_ order):\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nrandom_array = np.random.randint(0,100, 10)\nsorted(random_array)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n[30, 36, 44, 45, 52, 64, 73, 80, 95, 95]\n```\n:::\n:::\n\n\n- What is Python actually _doing_? \n\n- _Many_ sorting algorithms. See [Wikipedia](https://en.wikipedia.org/wiki/Sorting_algorithm) for an extensive list\n\n---\n\n## Bubble sort\n\n- Repeatedly pass through array, comparing neighbouring pairs of elements and switching them if they are out of order\n\n- After first pass the largest element is in the rightmost position (largest index)\n\n- Second pass can finish before reaching last element, as it is already in place\n\n- After second pass final two elements are correctly ordered\n\n- Continue until array is sorted\n\n- [Animation of bubble sort](https://www.sortvisualizer.com/bubblesort/)\n\n---\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ndef bubble_sort(A):\n    \"Sort A and return\"\n    A = A.copy()\n    n = len(A)\n    while n > 0:\n        for i in range(n - 1):\n            # Swap data if in wrong order\n            if A[i] > A[i + 1]:\n                A[i + 1], A[i] = A[i], A[i + 1]\n        n = n - 1\n\n    return A\n```\n:::\n\n\n---\n\nWhat is complexity of bubble sort? \n\n- There are two nested loops: one to implement each pass and one to loop over the $n-1$ passes\n\n- Suggests that complexity is quadratic i.e. $O(n^2)$. A numerical check verifies this:\n\n---\n\n::: {.cell execution_count=10}\n\n::: {.cell-output .cell-output-display}\n![](complexity_files/figure-revealjs/cell-11-output-1.png){width=823 height=424}\n:::\n:::\n\n\n---\n\n- If you watch the [animation of bubble sort](https://www.sortvisualizer.com/bubblesort/) you might get a bit bored, as it _slowly_ carries the next largest element to the end\n\n- Can we do better? \n\n---\n\n- How fast _could_ a sorting algorithm be? \n\n- Can't be faster than $O(n)$: at the very least one has to look at each element\n\n- While one can't actually achieve linear scaling, many algorithms which achieve the next best thing: $O(n\\log n)$. \n\n---\n\n## Quicksort\n\n- Uses two key ideas:\n\n    1. Possible in $O(n)$ steps to partition an array into those elements larger (or equal) and those elements smaller than a given value (called the _pivot_).\n    2. Acting recursively on each partition requires only $O(\\log n)$ partitions to completely sort array\n\n---\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\ndef quicksort(A, lo=0, hi=None):\n    \"Sort A and return sorted array\"\n\n    # Initialise data the first time function is called    \n    if hi is None:\n        hi = len(A) - 1\n        A = A.copy()\n\n    # Sort    \n    if lo < hi:\n        p = partition(A, lo,  hi)\n        quicksort(A, lo, p - 1)\n        quicksort(A, p + 1, hi)\n    return A\n\n\ndef partition(A, lo, hi):\n    \"Partitioning function for use in quicksort\"\n    pivot = A[hi]\n    i = lo\n    for j in range(lo,  hi):\n        if A[j] <= pivot:\n            A[i], A[j] = A[j], A[i]\n            i += 1\n    A[i], A[hi] = A[hi], A[i]\n    return i\n```\n:::\n\n\n- See [this discussion](https://en.wikipedia.org/wiki/Quicksort#Lomuto_partition_scheme) of the partitioning scheme for more\n\n---\n\n::: {.cell execution_count=12}\n\n::: {.cell-output .cell-output-display}\n![](complexity_files/figure-revealjs/cell-13-output-1.png){width=823 height=424}\n:::\n:::\n\n\n---\n\n- Interesting example of differences between best, worst and average case complexities\n\n    1. Best case: $O(n\\log n)$\n    2. Worst case: $O(n^2)$\n    3. Average case: $O(n\\log n)$\n\n- Worst case occurs when the array is _already sorted_\n\n- Pivot is chosen as the last element of the array, so one partition is always empty in this case\n\n- Instead of problem being cut roughly in half at each stage, it is only reduced in size by 1\n\n---\n\nNumPy's `sort` uses quicksort, whereas Python's `sorted` uses a hybrid algorithm called [Timsort](https://en.wikipedia.org/wiki/Timsort), which also has $O(n\\log n)$ average case performance\n\n::: {.cell execution_count=13}\n\n::: {.cell-output .cell-output-display}\n![](complexity_files/figure-revealjs/cell-14-output-1.png){width=823 height=424}\n:::\n:::\n\n\n---\n\n## Divide and conquer  \n\n- Quicksort, binary search, and exponentiation by squaring are all examples of [divide and conquer algorithms](https://en.wikipedia.org/wiki/Divide-and-conquer_algorithm)\n\n- Achieve performance by breaking task into two (or more) sub-problems of same type\n\n---\n\n## Karatsuba algorithm\n\n- Recall \"obvious\" method for multiplication has quadratic complexity\n\n- Try a divide and conquer type approach by splitting an $n$-digit number as follows\n$$\nx = x_1 B^m + x_0\n$$\n- $B$ is base and $m=\\lceil n\\rceil$\n\n- In base 10 $x=12345$ is written as $12 * 1000 + 345$\n\n---\n\n- Do this for two  $n$-digit numbers $x$ and $y$, then\n\n$$\nxy = x_1 y_1 B^{2m} + (x_1 y_0 + x_0 y_1) B^{m} + x_0 y_0,\n$$\n\n- Requires computation of four products\n\n- Now divide and conquer, splitting up $x_0$, $x_1$, $y_0$, $y_1$ in the same way\n\n-  Continues to a depth of $\\sim\\log_2 n$ until we end up with single digit numbers. What's the complexity?\n\n$$\n4^{\\log_2 n} = n^2\n$$\n\n---\n\n- So we gained nothing by being fancy! \n\n- _But_ Karatsuba noticed that since\n$$\nx_1 y_0 + x_0 y_1 = (x_1 + x_0)(y_1 + y_0) - x_y y_0 - x_1 y_1\n$$\nyou can in fact get away with _three_ multiplications instead of four (together with some additions)\n\n- Divide and conquer approach; end up with complexity\n\n$$\n3^{\\log_2 n} = n^{\\log_2 3} \\approx n^{1.58}\n$$\n\n\n\n\n<!-- Simple example from Leetcode\n\nAnalaysis of algorithms\n\nExample of finding a unique item in list\n\nBreadth first and depth first\n\nImportance of choosing a data structure to match algorithm\n\nExamples: queue in Wolff. Was there a Numpy-ish way to do this faster? Priority queue in waiting time algo\n\nFFT uses\n\nhttps://en.wikipedia.org/wiki/Orthogonal_frequency-division_multiplexing\n\nNeedleman-Wunsch \n\nExamples\n\n1. Multiplication [Karatsuba](https://en.wikipedia.org/wiki/Multiplication_algorithm#Karatsuba_multiplication)\n\n3. Linear algebra\n5. FFT\n7. Euclidean algorithm (GCD) (SICP)\n\nReferences. Nature of computation, grokking algos\n\nInsertion in a list etc. -->\n\n<!-- # Space vs. time complexity -->\n\n",
    "supporting": [
      "complexity_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {}
  }
}