[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Part II Computational Physics",
    "section": "",
    "text": "These are the materials for the Part II Physics course Computational Physics, taught in Lent Term 2023 at the University of Cambridge.\n\n\nThe course of eight Lectures will take place at 10.00 on Mondays and Fridays in the Pippard Lecture Theatre. After the lectures there will be four computing exercises to be completed in the last four weeks of full Lent term; one per week. The exercises count for 0.2 units or further work, or roughly 2% of your final mark for the year. Thus each exercise should only take you a few hours.\nThe schedule is as follows\n\nFirst lecture: Monday 23th January\nLast lecture: Friday 17th February\nFirst exercise: Friday 17th February – Friday 24th February\nSecond exercise: Friday 24th February – Friday 3rd March\nThird exercise: Friday 3rd March – Friday 10th March\nFourth exercise: Friday 10th March – Friday 17th March (last day of full Lent term)\n\n\n\nAdditionally, you may choose to offer a Computational Physics project for one unit of further work. This involves choosing a problem from the project list. You will analyse the problem, write and test Python code to investigate it, then write up your work in a report. \nStudents may start their project work once the project list is published by 17th February. The deadline for submission of the project report is 16:00 on the first Monday of Full Easter term (1st May 2023).\n\n\n\n\nThis course assumes a basic knowledge of the Python language, including variables, control flow, and writing and using functions, at the level of last year’s IB course (which had an excellent handout).\nIf you want an absolute bare bones intro to Python try the first half of this tutorial (which then goes on to introduce NumPy).\n\n\n\nIn this course you will learn\n\nAbout the Python scientific stack (based on the NumPy library)\nIts use in implementing some common algorithms in computational physics.\nBasic ideas of computational complexity used in the analysis of algorithms\n\n\n\n\nHere’s a list of topics that I’d like to cover. We make not have time for all of them.\n\nSetup. Running Python. Notebooks. Language overview\nNumPy and friends\nFloating point and all that\nSoving differential equations with SciPy\nMonte Carlo methods\nIntroduction to algorithms and complexity\nThe fast Fourier transform\nLinear algebra with NumPy\nAutomatic differentiation\n\n\n\n\n\n\n…were prepared using Quarto. Each chapter should be thought of as a Jupyter notebook (actually, they are Jupyter notebooks), so you’ll probably only see import numpy as np once in each chapter, for example.\nThe code used to generate this site is in this GitHub repo. Please use issues to submit any typos and discussions to discuss the content.\nIn several places I’ve used examples from an earlier version of the course by David Buscher."
  },
  {
    "objectID": "slides/getting-going.html#goals",
    "href": "slides/getting-going.html#goals",
    "title": "Part II Computational Physics",
    "section": "Goals",
    "text": "Goals\nIn this course you will learn\n\nAbout the Python scientific stack (based on NumPy)\nIts use in implementing some common algorithms in computational physics\nBasic ideas of computational complexity used in the analysis of algorithms"
  },
  {
    "objectID": "slides/getting-going.html#prerequisites",
    "href": "slides/getting-going.html#prerequisites",
    "title": "Part II Computational Physics",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nAssume a knowledge of the Python language, including variables, control flow, and writing and using functions\nRefer to last year’s IB course (which had an excellent handout)…\n…and of course the internet\nFor an absolutely bare bones intro to Python try the first half of this tutorial"
  },
  {
    "objectID": "slides/getting-going.html#computational-physics.tripos.org",
    "href": "slides/getting-going.html#computational-physics.tripos.org",
    "title": "Part II Computational Physics",
    "section": "computational-physics.tripos.org",
    "text": "computational-physics.tripos.org\n\nLecture notes (in progress)\nThese slides (if you want them)\nCode at github.com/AustenLamacraft/part-ii-computational-physics\nSubmit typos to the GH repo issues and use discussions to discuss…"
  },
  {
    "objectID": "slides/getting-going.html#housekeeping",
    "href": "slides/getting-going.html#housekeeping",
    "title": "Part II Computational Physics",
    "section": "Housekeeping",
    "text": "Housekeeping\n\nEight lectures. Mondays and Fridays at 10.00 in the Pippard\nAfter the lectures: four computing exercises\nTo be completed in the last four weeks of full Lent term; one per week\nExercises count for 0.2 units or further work, or roughly 2% of your final mark for the year\nEach exercise should only take you a few hours."
  },
  {
    "objectID": "slides/getting-going.html#schedule",
    "href": "slides/getting-going.html#schedule",
    "title": "Part II Computational Physics",
    "section": "Schedule",
    "text": "Schedule\n\nFirst lecture: Monday 23th January\nLast lecture: Friday 17th February\nFirst exercise: Friday 17th February – Friday 24th February\nSecond exercise: Friday 24th February – Friday 3rd March\nThird exercise: Friday 3rd March – Friday 10th March\nFourth exercise: Friday 10th March – Friday 17th March (last day of full Lent term)"
  },
  {
    "objectID": "slides/getting-going.html#computing-project",
    "href": "slides/getting-going.html#computing-project",
    "title": "Part II Computational Physics",
    "section": "Computing Project",
    "text": "Computing Project\n\nYou may choose to offer a Computational Physics project for one unit of further work\nChoose a problem from the project list. Analyse the problem, write and test Python code to investigate it, then write up your work in a report\nProject list is published by 17th February\nDeadline for submission of the project report is 16:00 on the first Monday of Full Easter term (1st May 2023)"
  },
  {
    "objectID": "slides/getting-going.html#finding-your-way",
    "href": "slides/getting-going.html#finding-your-way",
    "title": "Part II Computational Physics",
    "section": "Finding your way",
    "text": "Finding your way\n\nEveryone finds their own workflow for coding (language, editor, etc.)\nThis is a roundup of some popular tools in the Python ecosystem"
  },
  {
    "objectID": "slides/getting-going.html#your-coding-environment",
    "href": "slides/getting-going.html#your-coding-environment",
    "title": "Part II Computational Physics",
    "section": "Your coding environment",
    "text": "Your coding environment\n\nYou will need to install the Python language (or run online)\nI recommend the Anaconda distribution\nComes with all parts of the toolkit we’ll need such as Jupyter notebooks and the major libraries NumPy and SciPy"
  },
  {
    "objectID": "slides/getting-going.html#ipython",
    "href": "slides/getting-going.html#ipython",
    "title": "Part II Computational Physics",
    "section": "IPython",
    "text": "IPython\n\nIf you the above with python nice colour scheme is absent\nThis is called syntax highlighting and provides a visual guide to the syntax of the language\nIPython is an interactive shell that provides syntax highlighting and much more\nIf you have installed IPython (it comes with Anaconda) you can start it from the command line with ipython"
  },
  {
    "objectID": "slides/getting-going.html#helpful-features-of-ipython",
    "href": "slides/getting-going.html#helpful-features-of-ipython",
    "title": "Part II Computational Physics",
    "section": "Helpful features of IPython:",
    "text": "Helpful features of IPython:\n\nTab completion: hit tab to autocomplete. Particularly useful for viewing all properties or methods of an object:\n\n\n\nTyping ?obj or obj? prints detailed information about the object obj (?? provides additional detail)"
  },
  {
    "objectID": "slides/getting-going.html#running-a-python-program",
    "href": "slides/getting-going.html#running-a-python-program",
    "title": "Part II Computational Physics",
    "section": "Running a Python program",
    "text": "Running a Python program\n\nPython code in a file with a .py extension can be run from the command line with\n\npython hello_world.py\nor\npython -m hello_world\n\nIn the latter case -m option tells interpreter to look for a module called hello_world"
  },
  {
    "objectID": "slides/getting-going.html#importing-code",
    "href": "slides/getting-going.html#importing-code",
    "title": "Part II Computational Physics",
    "section": "Importing code",
    "text": "Importing code\n\nA Python module is a file containing definition and statements\nBreaking long code into modules is good practice for writing clear and reusable software\nUsers may not want to see the details of a function in order to be able to us it"
  },
  {
    "objectID": "slides/getting-going.html#packages",
    "href": "slides/getting-going.html#packages",
    "title": "Part II Computational Physics",
    "section": "Packages",
    "text": "Packages\n\nA collection of modules in a folder is called a package\nYou can import a package in the same way and access all the modules using the same . notation i.e. package.module1, package.module2, etc..\nSince explicit namespaces are preferred to avoid ambiguity use shorthands for the package or module you are importing:\n\n\nimport numpy as np\nnp.arange(10)\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\n(You can call it what you like, of course!)"
  },
  {
    "objectID": "slides/getting-going.html#installing-libraries",
    "href": "slides/getting-going.html#installing-libraries",
    "title": "Part II Computational Physics",
    "section": "Installing libraries",
    "text": "Installing libraries\n\n99% of the code you run will have been written by somebody else in the form of a library\nPackage installation is handled by the command line utilities pip or conda, the latter being the package manager for the Anaconda distribution\nIf you have NumPy and SciPy installed you won’t need to worry about this too much"
  },
  {
    "objectID": "slides/getting-going.html#editors",
    "href": "slides/getting-going.html#editors",
    "title": "Part II Computational Physics",
    "section": "Editors",
    "text": "Editors\n\nModern editors come with a huge number of tools that make writing code much easier\nSyntax highlighting, code completion, parameter information and documentation popups as you type\nThese go under the general heading IntelliSense\nThe latest hotness is GitHub Copilot: AI code suggestions\n(imo) these are all part of a continuum of productivity enhancements that enable people to write better code faster. Try them out!\nI use Visual Studio Code"
  },
  {
    "objectID": "slides/getting-going.html#notebooks",
    "href": "slides/getting-going.html#notebooks",
    "title": "Part II Computational Physics",
    "section": "Notebooks",
    "text": "Notebooks\n\nSoftware developers write .py files, modules and packages\nScientists and others doing more exploratory work tend to favour a Notebook format that mixes code, text, and plots\nDominant option is Jupyter notebook, which comes with the Anaconda distribution\nStart from command line with jupyter notebook (or from the Anaconda Navigator application)\nOpens a notebook as a web page in your browser, where it can be edited and saved. The default extension is .ipynb"
  },
  {
    "objectID": "notes/assignments.html",
    "href": "notes/assignments.html",
    "title": "Part II Computational Physics",
    "section": "",
    "text": "Autocorrelation times of various algorithms. Scaling with system size."
  },
  {
    "objectID": "notes/getting-going.html",
    "href": "notes/getting-going.html",
    "title": "Part II Computational Physics",
    "section": "",
    "text": "Everyone finds their own workflow for coding, depending on their preferred language, editor, how they run their code, and so on. The aim of the sections below is to give a roundup of some popular tools in the Python ecosystem.\n\n\n\nTo run Python code on your computer you will need to have installed the Python language. I recommend the Anaconda distribution as it comes with all the parts of the toolkit we’ll need such as Jupyter notebooks and the major libraries NumPy and SciPy.\nTry running python at the command line. You should get something like\nPython 3.9.12 (main, Apr  5 2022, 01:53:17) \n[Clang 12.0.0 ] :: Anaconda, Inc. on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \nYou should confirm that you are using Python 3 (the command python3 will also work and guarantee this if you happen to have Python 2 as the default). The prompt >>> indicates that you have started the Python interactive shell or REPL and are good to go:\n\nprint(\"Hello world!\")\n1 + 2\n\nHello world!\n\n\n3\n\n\nTo leave and return to the command line, you can run quit() or exit().\n\n\n\nIf you ran the above command from within python you may have noticed that the nice colour scheme that you see above was absent. This is called syntax highlighting and provides a visual guide to the syntax of the language.\nIPython is an interactive shell that provides syntax highlighting and much more. If you have installed IPython (it comes with Anaconda) you can start it from the command line with ipython.\nAmong the most helpful features of IPython are:\n\nTab completion: hit tab to autocomplete. This is particularly useful for viewing all properties or methods of an object: \nTyping ?obj or obj? prints detailed information about the object obj (?? provides additional detail).\nCertain magic commands prefixed by % that provide certain additional functionality. For example, %timeit finds the executation time of a single line statement, which is useful when profiling the performance of code:\n\n\n%timeit L = [n ** 2 for n in range(1000)]\n\n226 µs ± 3.58 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n%timeit automatically runs several times to give some statistics on the execution time. For multiple lines you can use the %%timeit magic.\nYou can find much more exploring the documentation.\n\n\n\nPython code in a file with a .py extension can be run from the command line with python hello_world.py or python -m hello_world. In the latter case the -m option tells the interpreter to look for a module called hello_world. More on modules below.\nFrom the IPython shell you can instead use run hello_world.py or just run hello_world.\n\n\n\n\nA Python module is just a file containing definition and statements. Breaking long code into modules is good practice for writing clear and reusable software. Users may not want to delve into the details of some function you have written in order to be able to us it, and separating the corresponding code into a separate file is a hygienic way to handle this.\nThus if I make the file hello_world.py containing the function:\n\ndef hello():\n    print(\"Hello world!\")\n\nI can run this function by first importing the module:\n\nimport hello_world\nhello_world.hello()\n\nHello world!\n\n\nNotice that the function hello is accessed from the hello_world namespace. This is to avoid any confusion that may arise if more that one imported module has a function of the same name. If you are confident that’s not an issue and want more concise code you can do this:\n\nfrom hello_world import hello\nhello()\n\nHello world!\n\n\nor even import everything with the wildcard *:\n\nfrom hello_world import *\nhello()\n\nHello world!\n\n\nThe issue with the latter is that it may introduce a whole bunch of names that may interfere with things you already defined.\nA collection of modules in a folder is called a package. You can import a package in the same way and access all the modules using the same . notation i.e. package.module1, package.module2, etc..\nSince explicit namespaces are preferred to avoid ambiguity it’s common to introduce shorthand names for the package or module you are importing, hence the ubiquitous:\n\nimport numpy as np\nnp.arange(10)\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n(You can call it what you like, of course!)\nFor details about where the interpreter looks to find modules you try to import are in the documentation.\n\n\n\n99% of the code 1 you run will have been written by somebody else in the form of a library (a collection of modules or packages). Package installation is handled by the command line utilities pip or conda, the latter being the package manager for the Anaconda distribution. If you have NumPy and SciPy installed you won’t need to worry about this too much in this course.\n\n\n\nModern editors come with a huge number of tools that make writing code much easier, and you would be crazy not to take advantage of them. These range from the visual cues provided by syntax highlighting – which we’ve already met – to code completion, parameter information and documentation popups as you type. These go under the general heading IntelliSense. The latest hotness is GitHub Copilot, which uses AI to make code suggestions. In my view, these are all part of a continuum of productivity enhancements that enable people to write better code faster. Use them (wisely).\nI use Visual Studio Code.\n\n\n\nWhile software developers write .py files, modules and packages, scientists and others doing more exploratory work tend to favour a Notebook format that mixes code, text, and plots. The dominant option is the Jupyter notebook, which comes with the Anaconda distribution and can be started from the command line with jupyter notebook (or from the Anaconda Navigator application). This will open the notebook as a web page in your browser, where it can be edited and saved. The default extension is .ipynb.\nJupyter notebooks can actually run code in different languages (the processes running a particular language is called a kernel), but the default process is IPython with all the benefits described above.\nThe text cells can be formatted using Markdown and also support \\(\\LaTeX\\) equations, which is pretty handy for us.\nGoogle has their own cloud version of the Jupyter notebook called Colab. You can try it out for free, though you have to pay for significant compute. The “next generation” of the Jupyter notebook is called JupyterLab and can be started with jupyter lab. Notebook files can be opened in either Jupyter Lab or Jupyter Notebook"
  },
  {
    "objectID": "notes/monte-carlo.html",
    "href": "notes/monte-carlo.html",
    "title": "Part II Computational Physics",
    "section": "",
    "text": "Many physical phenomena, notably those falling within the domains of statistical mechanics and quantum theory, depend in an essential way on randomness. The simulation of these phenomena therefore requires algorithms that incorporate random (or pseudo-random) elements in the most efficient way.\n\n\nLet’s suppose that we have a source of samples of a real valued random variable \\(X\\) that follows a particular probability density function \\(p_X\\) 1. This means that the probability of drawing a sample in the region \\([x, x+dx]\\) is \\(p_X(x)dx\\). If we now map the samples using a function \\(f\\), what is the probability density \\(p_Y\\) of \\(y=f(x)\\)? The new probability density is defined in just the same way: the probability of \\(y\\) lying in the region \\([y, y+dy]\\) is \\(p_Y(y)dy\\). Since \\(x\\) is being mapped deterministically to \\(y\\) these two probabilities are therefore the same\n\\[\np_X(x)dx = p_Y(y)dy\n\\]\nor\n\\[\np_Y(y)=p_X(x)\\Bigg\\lvert \\frac{dx}{dy}\\Bigg\\rvert= \\frac{p_X(x)}{|f'(x)|},\\qquad x=f^{-1}(y)\n\\]\nThis formula shows that we can create samples from an arbitrary probability distribution by choosing an invertible map \\(f\\) appropriately. If \\(p_X\\) is a standard uniform distribution on \\([0,1]\\) then \\(f(x)\\) is the inverse of the cummulative probability distribution of \\(Y\\) i.e.\n\\[\nf^{-1}(y) = \\int^y_{-\\infty} p_Y(y')dy'\n\\]\nThe same approach works in higher dimensions: \\(\\big\\lvert \\frac{dx}{dy}\\big\\rvert\\) is replaced by the inverse of the Jacobian determinant.\nThe Box–Muller transform is one example of this idea. Take two independent samples from a standard uniform distribution \\(u_{1,2}\\) and form\n\\[\n\\begin{align}\nx &= \\sqrt{-2\\log u_1}\\cos(2\\pi u_2)\\\\\ny &= \\sqrt{-2\\log u_1}\\sin(2\\pi u_2).\n\\end{align}\n\\]\n\\(x\\) and \\(y\\) are independent samples from a standard normal distribution.\nVarious functions are available in the numpy.random module to generate random arrays drawn from a variety of distributions. Box–Muller has now been retired in favour of the Ziggurat algorithm.\n\nimport numpy.random as random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nmu, sigma = 0, 0.1 # mean and standard deviation\ns = random.normal(mu, sigma, size=10000)\ncount, bins, ignored = plt.hist(s, 30, density=True)\nplt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *\n               np.exp( - (bins - mu)**2 / (2 * sigma**2) ),\n         linewidth=2, color='r')\nplt.xlabel(\"Value\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\nFor complex multivariate (i.e. high dimensional) distributions there is no general recipe to construct an appropriate \\(f\\). One very recent application of these ideas is in machine learning models called normalizing flows that use a mapping \\(f\\) parameterized by a neural network. The workhorse for sampling from complicated distributions is Markov chain Monte Carlo, as we discuss in Section 1.2.2.\n\n\n\nMonte Carlo is the general prefix applied to variety of numerical methods that use randomness in some way. Two of the main classes of problem encountered in physics that come under this heading are:\n\nInterpret a numerical evaluation as an expectation value of some random variable and use sampling to estimate it. Monte Carlo integration is an example of this idea.\nSampling from a complex probability distribution (which may include taking expectation values). Example: Markov chain Monte Carlo.\n\n\n\nThe technique is exemplified by the following fairly dumb way of estimating \\(\\pi\\)\n\nmax_samples = 10000\ninside = 0\nareas = []\nfor sample in range(1, max_samples + 1):\n    x = random.uniform(-1, 1)\n    y = random.uniform(-1, 1)\n    \n    if x ** 2 + y ** 2 <= 1:\n        inside += 1\n    areas.append(4 * inside / sample)\n\nplt.plot(np.arange(1, max_samples + 1), areas)\nplt.plot(np.arange(1, max_samples + 1), np.pi * np.ones(max_samples), linestyle='dashed')\nplt.show()\n\n\n\n\nIn terms of integration, you can think of this as a way to compute the integral of a function which is one inside the unit disc, and zero outside it.\nAlthough it’s a silly method, this does illustrate one important feature of Monte Carlo methods in general: that the relative error with \\(N\\) samples is typically \\(\\propto N^{-1/2}\\) (thus at the 1% level for \\(10^4\\) samples) because the variance of a sum of \\(N\\) iid variables is \\(\\propto N^{1/2}\\).\n\nMonte Carlo integration comes into its own for high dimensional problems. For low dimensional integrals the quadrature methods in scipy.integrate are preferable.\n\n\n\nSuppose you want to generate configurations at random (i.e. with a uniform distribution) from a “gas” of hard disks 2.\n\n\n\nCoins in a shoe box (gas of hard disks). From Krauth (1998)\n\n\nIt’s harder than it looks! The first guess you might have is to start adding coins at random, and if you get an overlap, try again until you don’t. Obviously this will become inefficient as the box fills up, and most attempts fail. Worse, it doesn’t in fact yield a uniform distribution!\n\nHere’s an approach that works:\n\nExample 1 (Metropolis algorithm for hard disks)  \n\nFix the number of disks and an initial configuration (some regular lattice configuration, say).\nPick a disk at random and attempt (or propose) to move it by a small random amount (i.e. random direction; random small magnitude).\nIf this results in the moved disk intersecting another, reject the move, leaving the disk where it is. Otherwise, accept the move.\nRepeat 2. and 3. many times.\n\n\n.\nThis is the simplest example of the Metropolis–Hastings algorithm, the first Markov chain Monte Carlo (MCMC) algorithm.\nMore generally, the goal of MCMC is to come up with a sequential random process (a Markov chain) that generates (usually after many steps) a sample from a particular distribution.\nYou’ve all heard of a random walk, perhaps as a model for diffusion. At each step you make a move in a random direction, independently of your earlier moves. After many steps these random moves gives rise to a distribution of possible locations. A random walk is the simplest example of a Markov chain.\nMore generally, a Markov chain is a sequence of random variables \\(X_n\\) with each having a distribution that is is conditional on the value of the previous one, and so is defined in terms of transition probabilities \\(p(X_{n}=x_n|X_{n-1}=x_{n-1})\\) (hence they form a “chain”). I’m going to immediately drop this cumbersome notation in favour of \\(p(x_n|x_{n-1})\\), a function of \\(x_n\\) and \\(x_{n-1}\\), but in general the function giving the transition probabilities can be different at each step (the random variables could all be different).\nThe probability of a particular sequence \\(X_1=x_1\\ldots X_n=x_n\\) is therefore\n\\[\np(x_n|x_{n-1})p(x_{n-1}|x_{n-2})\\cdots p(x_2|x_{1})p^{(1)}(x_1)\n\\]\n\\(X_1\\) has no “parent” so is not conditional on any other value.\nSuppose we don’t care about the earlier values and just want to know the marginal distribution \\(p^{(n)}(x_n)\\) of the final variable. For a random walk this is easy, as \\(x_n\\) typically represents a displacement that is a sum of iid increments. In general this is not the case, however, as the marginal distribution is\n\\[\np^{(n)}(x_n)=\\sum_{x_{n-1},\\ldots x_1}p(x_n|x_{n-1})p(x_{n-1}|x_{n-2})\\cdots p(x_2|x_{1})p^{(1)}(x_1)\n\\]\n(I’m writing all these expressions for discrete random variables, but the continuous version involving probability density functions is straightforward)\nThe sums are over all possible values that the random variables might take in the state space of the problem. These could be finite or infinite in number.\nThings are not as bad as they appear, however, as the marginal distribution can be interpreted as the result of acting \\(n-1\\) times on the vector of values of \\(p^{(1)}_j\\equiv p^{(1)}(j)\\) with the transition matrix with elements \\(\\mathsf{P}_{jk}=p(j|k)\\)\n\\[\n\\mathbf{p}^{(n)} = \\mathsf{P}^{n-1}\\mathbf{p}^{(1)}.\n\\]\nIn a single step the marginal probabilities are updated as\n\\[\n\\mathbf{p}^{(n)} = \\mathsf{P}^{n}\\mathbf{p}^{(n-1)}.\n\\]\n\\(\\mathsf{P}\\) has some structure. The matrix elements are positive, as they represent probabilities, and each row sums to one\n\\[\n\\sum_j \\mathsf{P}_{jk} = 1.\n\\]\nSuch matrices are called stochastic.\nAlthough \\(p^{(n)}\\) — the probability distribution at the \\(n\\)th step — changes from step to step, you might expect that after many steps it tends to converge to a stationary distribution \\(p^{(n)}\\to\\boldsymbol{\\pi}\\). If it exists, this distribution must satisfy\n\\[\n\\boldsymbol{\\pi} = \\mathsf{P}\\boldsymbol{\\pi}.\n\\tag{1}\\]\nIn other words, it is an eigenvector of \\(\\mathsf{P}\\) with eigenvalue one. This property is guaranteed by the Perron–Frobenius theorem 3.\nThus \\(\\mathsf{P}\\) determines \\(\\boldsymbol{\\pi}\\). MCMC turns this idea on its head and asks: if there is some \\(\\boldsymbol{\\pi}\\) that I would like to generate samples from, can I find a \\(\\mathsf{P}\\) that has it as a stationary distribution?\nThere is a trivial answer to this question. Sure, take \\(\\mathsf{P}_{jk}=\\boldsymbol{\\pi}_j\\). That is, jump straight to the stationary distribution no matter what the starting state. But we are interested in highly complicated distributions over large state spaces (think the Boltzmann distribution for a statistical mechanical system comprised of billions of particles). Thus what we really want is to be able to approach such a complicated distribution by making many transitions with simple distributions.\nOne more idea is useful before returning to concrete algorithms. The quantity\n\\[\n\\mathsf{P}_{jk}\\pi_k = p(j|k)\\pi_k = p(j,k)\n\\]\nis the joint distribution of seeing state \\(k\\) followed by state \\(j\\) in the stationary distribution. A reversible Markov chain is one where \\(p(j,k)=p(k,j)\\). Roughly, you can’t tell the direction of time because any transition is equally likely to happen forward in time as backward. Random physical processes that respect time reversal symmetry are often modeled as reversible Markov processes.\nCombining reversibility with the definition of the stationary state yields the condition of detailed balance\n\\[\n\\mathsf{P}_{jk}\\pi_k = \\pi_j\\mathsf{P}_{kj}.\n\\tag{2}\\]\nThis condition is stronger than the condition Equation 1 for a stationary state. This makes it easier to check: you don’t have to do a sum over a state space index. The Metropolis algorithm Example 1 for the hard disk problem satisfies detailed balance for a stationary distribution that is constant when disks don’t intersect and zero when they do.\nWhen the stationary distribution \\(\\boldsymbol{\\pi}\\) has more structure, designing an appropriate transition matrix is harder. The idea is to generalize the hard disk approach by separating the transition into a proposal distribution \\(p_\\text{prop}(j|k)\\) and an acceptance distribution \\(p_\\text{acc}(a=0,1|j\\leftarrow k)\\) that gives the probability of a move from \\(k\\) to \\(j\\) being accepted (\\(a=1\\)) or rejected (\\(a=0\\)). The probability of moving from \\(k\\) to \\(j\\) is then\n\\[\np(j|k) = p_\\text{acc}(a=1|j\\leftarrow k) p_\\text{prop}(j|k).\n\\]\nSubstituting this into the detailed balance condition Equation 2 gives \\[\n\\frac{p_\\text{acc}(a=1|j\\leftarrow k)}{p_\\text{acc}(a=1|k\\leftarrow j)} = \\frac{\\pi_j}{\\pi_k}\\frac{p_\\text{prop}(k|j)}{p_\\text{prop}(j|k)}.\n\\]\nAny \\(p_\\text{acc}\\) that satisfies this relation for all \\(j\\) and \\(k\\) will do the job. The Metropolis choice is\n\\[\np_\\text{acc}(a=1|j \\leftarrow k) = \\min\\left(1,  \\frac{\\pi_j}{\\pi_k}\\frac{p_\\text{prop}(k|j)}{p_\\text{prop}(j|k)}\\right).\n\\tag{3}\\]\nThis gives an extremely general algorithm, one of the top ten in applied mathematics, according to one list:\n\nExample 2 (Metropolis algorithm)  \n\nStarting from state \\(k\\) sample a next state \\(j\\) from the proposal distribution \\(p_\\text{prop}(j|k)\\).\nAccept the proposal with probability \\(p_\\text{acc}(a=1|j \\leftarrow k)\\) and move to state \\(j\\). Otherwise reject the proposal and stay in state \\(k\\).\nRepeat 1. and 2. many times.\n\n\nMCMC has the benefit of being embarrassingly parallel. If you want to average something over \\(\\boldsymbol{\\pi}\\), just run the algorithm many times independently and average the results. This is perfect for parallel computing.\nThe Metropolis algorithm has an Achilles’ heel, however. To perform a move one has to sample from \\(p_\\text{prop}(j|k)\\) and from \\(p_\\text{acc}(a|j \\leftarrow k)\\). The proposal therefore has to be tractable, like the small shift in position for the hard disk case. This may however, mean that that many of the \\(j\\)s suggested correspond to very small \\(\\pi_j\\), and therefore a very low acceptance probability (c.f. Equation 3). For example, in the hard disk case at high density many proposed moves will give rise to overlap of disks and be rejected. This means that many steps are required to have one successful update of the simulation. This kind of slowdown is a common feature of MCMC methods applied to complex distributions.\nWe’ll see some more examples of MCMC algorithms for statistical mechanical problems in Section 1.3, and ways in which this problem can be avoided.\n\n\n\n\n\nStatistical mechanics is a natural source of such complex distributions in physics. Remember the fundamental principle that the probability of finding a statistical mechanical system in a microstate \\(\\mathbf{x}\\) 4 with energy \\(\\mathcal{E}(\\mathbf{x})\\) is\n\\[\np(\\mathbf{x})=\\frac{\\exp\\left[-\\beta \\mathcal{E}(\\mathbf{x})\\right]}{Z},\n\\tag{4}\\]\nwhere \\(Z\\) is a normalizing constant called the partition function and \\(\\beta=1/k_\\text{B}T\\), where \\(T\\) is the temperature and \\(k_\\text{B}\\) is Boltzmann’s constant.\nThe central problem of statistical mechanics is computing ensemble averages of physical quantities, and the principle difficulty is the intractability of those averages for large systems. For example, if we are dealing with a classical gas, the configuration space point \\(\\mathbf{x}\\) corresponds to the positions of each of the gas molecules \\(\\mathbf{x}=(\\mathbf{x}_1,\\ldots \\mathbf{x}_N)\\) and an average is a \\(3N\\)-dimensional integral. The only situation in which this integral is tractable is when the gas is noninteracting (ideal), in which case the energy function takes the form\n\\[\n\\mathcal{E}(\\mathbf{x}) = \\sum_{n=1}^N \\mathcal{E}_1(\\mathbf{x}_n)\n\\]\nwhere \\(\\mathcal{E}_1(\\mathbf{x})\\) is the single particle energy. In this case the integral factorizes. As soon as we introduce interactions between particles of the form\n\\[\n\\mathcal{E}(\\mathbf{x}) = \\sum_{n<m}^N \\mathcal{E}_2(\\mathbf{x}_n,\\mathbf{x}_m)\n\\]\nthings get a lot harder. The same issue arises in models involving discrete random variables. The canonical example is the Ising model, in which a configuration corresponds to fixing the values of \\(N\\) “spins” \\(\\sigma_n=\\pm 1\\) with an energy function of the form\n\\[\n\\mathcal{E}(\\sigma)=\\sum_n h_n\\sigma_n + \\sum_{m<n} J_{mn}\\sigma_m\\sigma_n.\n\\]\nThe two terms correspond to a (magnetic) field that acts on each spin and a coupling between spins. As in the gas, it’s the latter that causes problems / interest.\nThe Ising model comes in a great many flavours according to how the fields and couplings are chosen. They may reflect a lattice structure: \\(J_{mn}\\neq 0\\) for nearest neighbours, say, or longer range. They may be fixed or random, defining an ensemble of models.\nThe most pessimistic assessment is that to calculate an average we are going to have sum over \\(2^N\\) configurations. Computing the partition function \\(Z\\) that normalizes the average (or which gives the free energy via \\(F=-k_\\text{B}T\\log Z\\)) is another such sum.\nMonte Carlo simulation is a much more attractive alternative. MCMC can be used to generate samples from \\(p(\\sigma)\\) which are then used to estimate the averages of interest (e.g. average energy \\(\\langle\\mathcal{E}(\\sigma)\\rangle\\), average magnetization \\(\\langle\\sum_n \\sigma_n\\rangle\\), etc.).\n\n\nHow does MCMC work in practice for the Ising model? To apply the Metropolis alogorithm Example 2 we can use a simple proposal: pick each spin in turn in some order and try to flip it.\nThe form of \\(p(\\sigma)\\) means that, although we cannot compute the probabilities explicitly, we can calculate ratios, which is all we need for Metropolis. For two configurations that differ only by \\(\\sigma_n=\\pm 1\\) we have\n\\[\n\\begin{align}\n\\frac{p(\\sigma_n=1|\\sigma_{m\\neq n})}{p(\\sigma_n=-1|\\sigma_{m\\neq n})} &= \\exp\\left[-2\\beta \\left(h_n+\\sum_{m\\neq n} J_{mn}\\sigma_m\\right)\\right]\\\\\n&\\equiv \\exp\\left[-\\beta\\Delta \\mathcal{E}\\right],\n\\end{align}\n\\]\nwhere \\(\\Delta \\mathcal{E}\\) is the energy difference between two configurations.\nOne alternative to Metropolis is the Heat bath algorithm (or Glauber dynamics or Gibbs sampling) 5. The idea behind the name is that, since we can calculate the influence of the spin’s environment (the “bath”), we can just choose the spin’s orientation with the corresponding probabilities. Since there are only two probabilities the ratio is all we need and we get\n\\[\np(\\sigma_n=\\pm 1|\\sigma_{m\\neq n}) = \\frac{1}{1+ e^{\\pm\\beta \\Delta \\mathcal{E}}}.\n\\tag{5}\\]\nThe algorithm is then:\n\nExample 3 (Heat bath algorithm)  \n\nPick a spin \\(n\\). 6\nCompute \\(\\Delta E\\), the energy difference between \\(\\sigma_n=\\pm 1\\).\nSet \\(\\sigma_n=\\pm 1\\) with probabilities given by Equation 5.\nRepeat 1-3 many times\n\n\nWhat happens if we try and come up with more complicated proposals, flipping many spins at once? For Metropolis, the problem is that without a cleverly designed proposal we will be suggesting moves that are likely to be rejected. For the heat bath algorithm, the more spins we flip, the more complicated the evaluation of the corresponding probabilities (\\(2^n\\) outcomes if we flip \\(n\\) spins).\nThe good news is that we can do better — much better — than the above algorithms. The Wolff algorithm is one example. This proposes a cluster of spins of the same orientation to be flipped by adding adjacent spins to an initially random chosen spin with probability \\(p_\\text{add}\\). It turns out that for the nearest neighbour Ising model with Ferromagnetic coupling \\(J<0\\) the “magic” value \\(p_\\text{add}=1-e^{2\\beta J}\\) is rejection free: the probability to flip the whole cluster is always one. This makes for an extremely fast algorithm that is not subject to the usual critical slowing down at phase transitions.\n\n\nIsing model code\nclass IsingModel:\n    def __init__(self, L):\n        self.L = L\n        self.spins = np.random.choice(a=[1, -1], size=(L, L))\n        stagger = np.empty(self.L, dtype = bool)\n        stagger[::2] = True\n        stagger[1::2] = False\n        self.mask = np.logical_xor(stagger[:, np.newaxis], stagger[np.newaxis, :])\n\n    def gibbs_update(self, beta, sublattice):\n        fields = np.roll(self.spins, 1, 0) + np.roll(self.spins, -1, 0) + np.roll(self.spins, 1, 1) + np.roll(self.spins, -1, 1)\n        delta_E = 2 * fields\n        spin_up_probabilities = 1 / (1 + np.exp(- beta * delta_E))\n        new_spins = 2 * (np.random.rand(self.L, self.L) < spin_up_probabilities) - 1\n        self.spins = np.choose(np.logical_xor(sublattice, self.mask), [self.spins, new_spins])\n\n    def glauber_update(self, beta):\n        x, y = np.random.randint(self.L, size=2)\n        fields = 0\n        for neighbour in [((x + 1) % self.L, y), ((x - 1) % self.L, y), (x, (y + 1) % self.L), (x, (y - 1) % self.L)]:\n            fields += self.spins[neighbour]\n        delta_E = 2 * fields\n        spin_up_probability = 1 / (1 + np.exp(- beta * delta_E))        \n        if np.random.rand() < spin_up_probability:\n            self.spins[x, y] = 1\n        else:\n            self.spins[x, y] = -1\n\n    def wolff_update(self, beta):\n        initial_x, initial_y = np.random.randint(self.L, size=2)\n        initial_spin = self.spins[initial_x, initial_y]\n        cluster = deque([(initial_x, initial_y)])\n        add_prob = 1 - np.exp(-2 * beta)\n\n        while len(cluster) != 0:\n            x, y = cluster.popleft()\n            if self.spins[x, y] == initial_spin:\n                self.spins[x, y] *= -1\n                for neighbour in (((x + 1) % self.L, y), ((x - 1) % self.L, y), (x, (y + 1) % self.L), (x, (y - 1) % self.L)):\n                    if self.spins[neighbour] == initial_spin:\n                        if np.random.rand() < add_prob:\n                            cluster.append(neighbour)\n\n\n\n\n\n\n\n\n\n\nFigure 1: Glauber dynamics, Block Gibbs sampling and Wolff updates compared. Change the temperature using the slider. The centre of the slider corresponds to the critical temperature \\(k_\\text{B}T = 2|J|/\\log(1+\\sqrt{2})\\sim 2.269|J|\\).\n\n\n\n\n\n\nMonte Carlo simulation is a vast field with practitioners and specialists across the natural sciences, engineering, machine learning, and statistics. In this section I’ll mention a few important topics to give a taste of what’s out there. For much more detail take a look at Krauth (2006) and / or MacKay (2003). The recent set of lectures Monte Carlo Techniques by Timothy Budd also look fantastic."
  },
  {
    "objectID": "notes/numbers.html",
    "href": "notes/numbers.html",
    "title": "Part II Computational Physics",
    "section": "",
    "text": "Since physics is all about numbers we had better develop some understanding of how computers represent them, and the limitations of this representation. Hopefully this example is sufficiently motivating:\n\n0.1  + 0.2 == 0.3\n\nFalse\n\n\nAh…\n\n\nLet’s begin with something simpler\n\n1 + 1 == 2\n\nTrue\n\n\nwhich is a bit more reassuring. Integers can be represented in binary\n\n3 == 0b11\n\nTrue\n\n\nor octal or hexadecimal (with a prefix 0o or 0h). You can get the binary string representing an integer using the bin function\n\nbin(-2)\n\n'-0b10'\n\n\nPython allows for arbitrarily large integers, so there is no possibility of overflow or rounding error\n\n2**100\n\n1267650600228229401496703205376\n\n\nThe only limitation is the memory required to store it.\nNumpy integers are a different story\n\nimport numpy as np\nnp.int64(2**100)\n\nOverflowError: Python int too large to convert to C long\n\n\nSince NumPy is using C the types have to play nicely. The range of integers that can be represented with 32 bit numpy.int32s is \\(\\approx\\pm 2^{31} \\approx \\pm 2.1 × 10^9\\) (one bit is for the sign) and 64 bit numpy.int64s is \\(\\approx\\pm 2^{63} \\approx \\pm 9.2 × 10^{18}\\). Apart from the risk of overflow when working NumPy’s integers there are not other gotchas to worry about.\n\n\n\nThe reason why \\(0.1 + 0.2 \\neq 0.3\\) in Python is that specifying a real number exactly would involve an infinite number of bits, so that any finite representation is necessarily approximate.\nThe representation computers use for the reals is called floating point arithmetic. It is essentially a form of scientific notation, in which a significand (it contains the significant figures) is multiplied by an exponent. The name floating point reflects the fact that the number of digits after the decimal point is not fixed (I’m using the base ten terms for convenience)\nThis representation requires the choice of a base, and Python’s floating point numbers use binary. Numbers with finite binary representations therefore behave nicely\n\n0.125 + 0.25 == 0.375\n\nTrue\n\n\nFor decimal numbers to be represented exactly we’d have to use base ten. This can be achieved with the decimal module. Our \\(0.1+0.2\\) example then works as expected\n\nfrom decimal import *\nDecimal('0.1') + Decimal('0.2')\n\nDecimal('0.3')\n\n\nSince there is nothing to single out the decimal representation in physics (as opposed to, say, finance) we won’t have any need for it.\nA specification for floating point numbers must give\n\nA base (or radix) \\(b\\)\nA precision \\(p\\), the number of digits in the significand \\(c\\). Thus \\(0\\leq c \\leq b^{p}-1\\).\nA range of exponents \\(q\\) specifed by \\(\\text{emin}\\) and \\(\\text{emax}\\) with \\(\\text{emin}\\leq q+p-1 \\leq \\text{emax}\\).\n\nIncluding one bit \\(s\\) for the overall sign, a number then has the form \\((-1)^s\\times c \\times b^q\\). The smallest positive nonzero number that can be represented is therefore \\(b^{1 + \\text{emin} - p}\\) (corresponding to the smallest value of the exponent) and the largest is \\(b^{1 + \\text{emax}} - 1\\).\nThe above representation isn’t unique: for some numbers you could make the significand smaller and the exponent bigger. A unique representation is fixed by choosing the exponent to be as small as possible.\nRepresenting numbers smaller than \\(b^{\\text{emin}}\\) involves a loss of precision, as the number of digits in the significand falls below \\(p\\) and the exponent has taken its minimum value . These are called subnormal numbers. For binary floats, if we stick with the normal numbers and a \\(p\\)-bit significand the leading bit will be 1 and so can be dropped from the representation, which then only requires \\(p-1\\) bits.\nThe specification for the floating point numbers used by Python (and many other languages) is contained in the IEEE Standard for Floating Point Arithmetic IEEE 754. The default Python float uses the 64 bit binary64 representation (often called double precision). Here’s how those 64 bits are used\n\n\\(p=53\\) for the significand, encoded in 52 bits\n11 bits for the exponent\n1 bit for the sign\n\nAnother common representation is the 32 bit binary32 (single precision) with\n\n\\(p=24\\) for the significand, encoded in 23 bits\n8 bits for the exponent\n1 bit for the sign\n\n\n\nIf this all a bit theoretical you can just get NumPy’s finfo function to tell all about the machine precision\n\nnp.finfo(np.float64)\n\nfinfo(resolution=1e-15, min=-1.7976931348623157e+308, max=1.7976931348623157e+308, dtype=float64)\n\n\nNote that \\(2^{-52}=2.22\\times 10^{-16}\\) which accounts for the value \\(10^{-15}\\) of the resolution. This can be checked by finding when a number is close enough to treated as 1.0.\n\nx=1.0\nwhile 1.0 + x != 1.0:\n    x /= 1.01 \nprint(x)\n\n1.099427563084686e-16\n\n\nFor binary32 we have a resolution of \\(10^{-6}\\).\n\nnp.finfo(np.float32)\n\nfinfo(resolution=1e-06, min=-3.4028235e+38, max=3.4028235e+38, dtype=float32)\n\n\nOne lesson from this is that taking small differences between numbers is a potential source of rounding error, as in this somewhat mean exam question\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSolution: \\(x-x'=x(1-\\gamma^{-1})\\sim x\\beta^2/2\\sim 4.2\\text{mm}\\).\n\nimport numpy as np\nfrom scipy.constants import c\nbeta = 384400e3 / (76 * 3600) / c\ngamma = 1/np.sqrt(1 - beta**2)\nprint(1 - np.float32(1/gamma), 1 - np.float64(1/gamma))\n\n0.0 1.0981660025777273e-11\n\n\n\n\n\n\n\n\nAs well as a floating point system, IEEE 754 defines Infinity and NaN (Not a Number)\n\nnp.array([1, -1, 0]) / 0\n\n/var/folders/xs/y8sn45v943s2_62flnxw0p940000gn/T/ipykernel_87522/2604490398.py:1: RuntimeWarning:\n\ndivide by zero encountered in true_divide\n\n/var/folders/xs/y8sn45v943s2_62flnxw0p940000gn/T/ipykernel_87522/2604490398.py:1: RuntimeWarning:\n\ninvalid value encountered in true_divide\n\n\n\narray([ inf, -inf,  nan])\n\n\nThey behave as you might guess\n\n2 * np.inf, 0 * np.inf, np.inf > np.nan\n\n(inf, nan, False)\n\n\nNaNs propagate through subsequent operations\n\n2 * np.nan\n\nnan\n\n\nwhich means that if you get a NaN somewhere in your calculation, you’ll probably end up seeing it somewhere in the output (which is the idea)."
  },
  {
    "objectID": "notes/intro.html",
    "href": "notes/intro.html",
    "title": "Part II Computational Physics",
    "section": "",
    "text": "Introduction\n\nScience is what we understand well enough to explain to a computer. Art is everything else we do.\nDonald Knuth\n\nComputation saturates every corner of physics these days, much as it saturates every corner of everything. Even if we restrict ourselves to the senses most relevant to physicists, the word computation covers a terrific variety of ideas. From the prosaic to the lofty, I could be talking about:\n\nThe tools we use to do computation. Physical hardware, editors, notebooks, etc.\nThe languages we use to write code in order to perform computations.\nThe use of tools to generate documents (e.g. using \\(\\LaTeX\\)) or disseminate knowledge (online).\nThe automated gathering and analysis of experimental data.\nThe numerical techniques that we use to solve particular problems in theoretical physics and mathematics.\nThe limits of what we can achieve with finite resources including time and space (memory). That is, how hard — or complex — are the computational tasks we wish to perform? Can we quantify this?\nThe question of whether physical processes are really the same things as computations. That is: are all processes that happen in the physical universe computable in principal (perhaps on a quantum computer)? This is roughly what is meant by (physical) Church–Turing thesis. This brings us full circle to the first item on the list.\n\nIn this course we’ll have to touch on all of these, except the last one (it’s only eight lectures). Most of the concrete techniques we’ll look at will come from computational physics (i.e. mathematical modelling of physical processes), rather than data analysis, but that’s mostly because of my background.\nFor theoretical physics, computation is used to deal with the awkward fact that physical theories are generally not tractable. You can’t solve Maxwell’s equations, the Navier–Stokes equation, or Schrödinger’s equation in any but the simplest situations. To be blunt, this means that your knowledge of physics, while very nice, is of not all that useful unless you can write a program to solve more complicated problems. Sorry.\nOn the plus side — as the above quote from Donald Knuth suggests — thinking about how to put a piece of physics you think you know into functioning code is a fantastic way to deepen your understanding of the physics itself. Every symbol and every operation has to mean and do exactly what it should for you to succeed.\nIt’s important to understand that this need to apply our mathematical descriptions of nature in more general settings was the principal driving force behind the invention of the computer in the first place. If you’d like to learn more about the early history of electronic computers I’d recommend Dyson (2012).\nIf you’d like to get into the theory of computation more deeply, I can’t recommend Moore and Mertens (2011) highly enough.\n\n\n\n\n\nReferences\n\nDyson, George. 2012. Turing’s Cathedral: The Origins of the Digital Universe. Vintage.\n\n\nMoore, Cristopher, and Stephan Mertens. 2011. The Nature of Computation. Oxford University Press."
  },
  {
    "objectID": "notes/autodiff.html",
    "href": "notes/autodiff.html",
    "title": "Part II Computational Physics",
    "section": "",
    "text": "Intro to backpropagation"
  },
  {
    "objectID": "notes/divide.html",
    "href": "notes/divide.html",
    "title": "Part II Computational Physics",
    "section": "",
    "text": "# Divide and Conquer\nFFT. Use split step as illustration Matrix multiplication"
  },
  {
    "objectID": "notes/linear.html",
    "href": "notes/linear.html",
    "title": "Part II Computational Physics",
    "section": "",
    "text": "Linear algebra\nKrylov subspaces\nSVD and quantum mechanics. Quantum entanglement.\nImage compression using SVD\nhttp://timbaumann.info/svd-image-compression-demo/\nTrebst has nice Einstein example\nPCA for big data\nhttps://www.tensors.net/exact-diagonalization"
  },
  {
    "objectID": "notes/fourier.html",
    "href": "notes/fourier.html",
    "title": "Part II Computational Physics",
    "section": "",
    "text": "The Fast Fourier Transform"
  },
  {
    "objectID": "notes/ode.html",
    "href": "notes/ode.html",
    "title": "Part II Computational Physics",
    "section": "",
    "text": "Newton’s fundamental discovery, the one which he considered necessary to keep secret and published only in the form of an anagram, consists of the following: Data aequatione quotcunque fluentes quantitates involvente, fluxiones invenire; et vice versa. In contemporary mathematical language, this means: “It is useful to solve differential equations”.\nVladimir Arnold, Geometrical Methods in the Theory of Ordinary Differential Equations\n\nWhile Arnold (and Newton) are of course right the problem is that solving differential equations is not possible in general. Even the simplest example of a first order ordinary differential equation (ODE) in a single variable\n\\[\n\\frac{dx}{dt} = f(x, t)\n\\tag{1}\\]\ncannot be solved for general \\(f(x,t)\\) 1. Of course, formulating a physical (or whatever) system in terms of differential equations represents a nontrivial step on the road to understanding it, but a lot remains to be done.\nNumerical analysis of differential equations is a colossal topic in applied maths and we are barely going to scratch the surface. The important thing is to be able to access existing solvers (and implement your own if necessary) and crucially to understand their limitations.\n\n\nThe basic idea behind all ODE solvers is to introduce a discretization of the equation and its solution \\(x_j\\equiv x(t_j)\\) at time points \\(t_j = hj\\) for some step size \\(h\\) and \\(j=0, 1, \\ldots\\). The very simplest approach is called Euler’s method 2 and approximates the derivative on the right hand side of Equation 1 as\n\\[\n\\frac{dx}{dt}\\Bigg|_{t=t_j} \\approx \\frac{x_{j+1} - x_j}{h}.\n\\tag{2}\\]\nRearranging the ODE then gives the update rule\n\\[\nx_{j+1} = x_j + hf(x_j, t_j).\n\\tag{3}\\]\nOnce an initial condition \\(x_0\\) is specified, subsequent values can be obtained by iteration.\nNotice that Equation 2 involved a forward finite difference: the derivative at time \\(t_j\\) was approximated in terms of \\(x_j\\) and \\(x_{j+1}\\) (i.e. one step forward in time). Why do this? So that the update rule Equation 3 is an explicit formula for \\(x_{j+1}\\) in terms of \\(x_j\\). This is called an explicit method. If we had used the backward derivative we would end up with backward Euler method \\[\nx_{j+1} = x_j + hf(x_{j+1}, t_{j+1})\n\\tag{4}\\]\nwhich is implicit. This means that the update requires an additional step to numerically solve for \\(x_{j+1}\\). Although this is more costly, there are benefits to the backward method associated with stability.\n\n\nIn making the approximation Equation 2 we make an \\(O(h^2)\\) local truncation error. To integrate for a fixed time the number of steps required is proportional to \\(h^{-1}\\), which means that the worst case error at fixed time (the global truncation error) is \\(O(h)\\). For this reason Euler’s method is called first order. More sophisticated methods are typically higher order: the SciPy function scipy.integrate.solve_ivp uses a fifth order method by default.\n\n\n\n\nIf you had unlimited computer time you might think you could make the step size \\(h\\) ever smaller in order to make the updates more accurate. This ignores the machine precision \\(\\epsilon\\), discussed in ?@sec-fp-numpy. The rounding error is roughly \\(\\epsilon x_j\\), and if the \\(N\\propto h^{-1}\\) errors in successive steps can be treated as independent random variables, the relative total rounding error will be \\(\\propto \\sqrt{N}\\epsilon=\\frac{\\epsilon}{\\sqrt{h}}\\) and will dominate for \\(h\\) small.\n\n\n\nApart from the relatively low accuracy that comes from using a first order method, the Euler method may additionally be unstable, depending on the equation. This can be demonstrated for the linear equation\n\\[\n\\frac{dx}{dt} = kx\n\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef euler(h, t_max, k=1):\n    \"\"\"\n    Solve the equation x' = k x, with x(0) = 1 using\n    the Euler method. \n\n    Integrate from t=0 to t=t_max using stepsize h for\n    num_steps = t_max / h.\n    \n    Returns two arrays of length num_steps: t, the time coordinate, and x_0, the position.\n    \"\"\"\n    num_steps = int(t_max / h)\n    # Allocate return arrays\n    x = np.zeros(num_steps, dtype=np.float32)\n    t = np.zeros(num_steps, dtype=np.float32)\n    x[0] = 1.0  # Initial condition\n    for i in range(num_steps - 1):\n        x[i+1] = x[i] + k * x[i] * h\n        t[i+1] = t[i] + h  # Time step\n    return t, x\n\nk = -2.3\nt_max = 5\nt, x = euler(1, t_max, k)\nplt.plot(t, x, label=\"h=1 Euler\")\nt, x = euler(0.7, t_max, k)\nplt.plot(t, x, label=\"h=0.7 Euler\")\nt = np.linspace(0, t_max, 100)\nplt.plot(t, np.exp(k * t), label=\"exact solution\")\nplt.title(\"k=-2.3\")\nplt.legend()\nplt.show()\n\n\n\n\nFor a linear equation the Euler update Equation 3 is a simple rescaling\n\\[\nx_{j+1} = x_j(1 + hk)\n\\]\nso the region of stability is \\(|1 + hk|\\leq 1\\). You can check that the backward Euler method Equation 4 eliminates the instability for \\(k<0\\).\n\n\n\n\nComing up with integration schemes is best left to the professionals. Your first port of call for solving ODEs in Python should probably be the integrate module of the SciPy scientific computing library. The function scipy.integrate.solve_ivp provides a versatile API.\nOne important thing to understand is that all these integration schemes apply to systems of first order differential equations. Higher order equations can always be presented as a first order system, at the expense of introducing more equations. For example, in physics we are often concerned with Newton’s equation\n\\[\nm\\frac{d^2 \\mathbf{x}}{dt^2} = \\mathbf{f}(\\mathbf{x},t),\n\\]\nwhich is three second order equations. We turn this into a first order system by introducing the velocity \\(\\mathbf{v}=\\dot{\\mathbf{x}}\\), giving the six equations\n\\[\n\\begin{align}\n\\frac{d\\mathbf{x}}{dt} &= \\mathbf{v}\\\\\nm\\frac{d \\mathbf{v}}{dt} &= \\mathbf{f}(\\mathbf{x},t).\n\\end{align}\n\\]\nAs a simple example, let’s consider the pendulum equation\n\\[\n\\ddot \\theta = -\\sin\\theta\n\\]\nwhich can be cast as\n\\[\n\\begin{align}\n\\dot\\theta &= l\\\\\n\\dot l &= -\\sin\\theta\n\\end{align}\n\\]\nSolving the equation using SciPy just requires us to define a function giving the right hand side of these equations\n\ndef pendulum(t, y): return [y[1], -np.sin(y[0])]\n# The pendulum equation: y[0] is theta and y[1] is l\n\nand then calling solve_ivp\n\nfrom scipy.integrate import solve_ivp\nimport matplotlib.pyplot as plt\n\nt_max = 1000\npendulum_motion = solve_ivp(pendulum, [0, t_max], [2, 0], dense_output=True)\n\nThe option dense_output=True is used to specify that a continuous solution should be found. What this means in practice is that the returned object pendulum_motion has a sol property that is an instance of OdeSolution. sol(t) returns the computed solution at \\(t\\) (this involves interpolation). We can use this to plot the pendulum’s trajectory in the \\(\\theta- l\\) phase plane, along with the contours of the conserved energy function\n\\[\nE(\\theta, l) = \\frac{1}{2}l^2 - \\cos\\theta\n\\]\n\n\nCode for plot\nfig, ax = plt.subplots()\n\ntheta = np.linspace(-1.1 * np.pi, 1.1 * np.pi, 60)\nl = np.linspace(-2, 2, 60)\nE = -np.cos(theta[np.newaxis,:]) + (l[:,np.newaxis])**2 / 2\n# Note the use of broadcasting to obtain the energy as a function of the phase space coordinates\n\nxx, yy = np.meshgrid(theta, l)\n\nax.contourf(xx, yy, E, cmap='Reds')\nt = np.linspace(0, t_max, 10000)\nax.plot(*pendulum_motion.sol(t))\nplt.xlabel(r'$\\theta$')\nplt.ylabel(r'$l$')\nplt.show()\n\n\n\n\n\nThe thickness of the blue line is due to the variation of the energy over the \\(t=1000\\) trajectory (measured in units where the frequency of linear oscillation is \\(2\\pi\\)). Notice that we did not have to specify a time step: this is determined adaptively by the solver to keep the estimate of the local error below atol + rtol * abs(y), where atol and rtol are optional arguments that correspond to the absolute and relative tolerances, with default values of \\(10^{-6}\\) and \\(10^{-3}\\) respectively. The global error is of course much larger. In general, monitoring conserved quantities is a good experimental method for assessing the accuracy of integration.\nThe alternative to dense_output=True is to track “events”, which are user-defined points of interest on the trajectory. We supply solve_ivp with functions event(t, x) whose zeros define the events. We can use events to take a “cross section” of higher dimensional motion. As an example let’s consider the Hénon–Heiles system, a model chaotic system with origins in stellar dynamics\n\\[\n\\begin{align}\n\\dot x &= p_x \\\\\n\\dot p_x &= -x -2\\lambda xy \\\\\n\\dot y &= p_y \\\\\n\\dot p_y &=  - y -\\lambda(x^2-y^2).\n\\end{align}\n\\]\nThese coupled first order systems for the \\(N\\) coordinates and \\(N\\) momenta of a mechanical system with \\(N\\) degrees of freedom are an example of Hamilton’s equations. The phase space is now four dimensional and impossible to visualize.\nThe conserved energy is\n\\[\nE = \\frac{1}{2}\\left(p_x^2+p_y^2 + x^2 + y^2\\right) + \\lambda\\left(x^2y-\\frac{1}{3}y^3\\right)\n\\]\nIf we take a Poincaré section with \\(x=0\\) a system with energy \\(E\\) must lie within the curve defined by\n\\[\nE = \\frac{1}{2}\\left(p_y^2 + y^2\\right) -\\frac{\\lambda}{3}y^3.\n\\]\nStarting from \\(x=0\\) we can generate a section of given \\(E\\) by solving for \\(p_x\\)\n\\[\np_x = \\sqrt{2E-y^2-p_y^2 + \\frac{2\\lambda}{3}y^3}\n\\]\n\ndef henon_heiles(t, z, 𝜆): \n    x, px, y, py = z\n    return [px, -x - 2 * 𝜆 * x * y, py, -y - 𝜆 * (x**2 - y**2)]\n\ndef px(E, y, py, 𝜆):\n    return np.sqrt(2 * E - y**2 - py**2 + 2 * 𝜆 * y**3 / 3)\n\ndef section(t, y, 𝜆): return y[0] # The section with x=0\n\nt_max = 10000\n𝜆 = 1\nhh_motion = []\nfor E in [1/12, 1/8, 1/6]:\n    hh_motion.append(solve_ivp(henon_heiles, [0, t_max], [0, px(E, 0.1, 0.1, 𝜆), 0.1, 0.1], events=section, args=[𝜆], atol=1e-7, rtol=1e-7))\n\nWe can then plot a section of the phase space with increasing energy, showing the transition from regular to chaotic dynamics.\n\n\nCode for plot\nfig, ax = plt.subplots(1, 3)\nenergies = [\"1/12\", \"1/8\", \"1/6\"]\nfor idx, data in enumerate(hh_motion): \n        ax[idx].scatter(*data.y_events[0][:, 2:].T, s=0.1)\n        ax[idx].title.set_text(f\"E={energies[idx]}\")        \n        ax[idx].set_xlabel(r'$y$')\n\nax[0].set_ylabel(r'$p_y$')\nplt.show()\n\n\n\n\n\nHere’s a nice demo on Poincaré sections if you’d like to learn more."
  },
  {
    "objectID": "notes/autograd.html",
    "href": "notes/autograd.html",
    "title": "Part II Computational Physics",
    "section": "",
    "text": "Automatic Differentiation\nKarpathy’s micrograd Michael Nielsen’s book"
  },
  {
    "objectID": "notes/references.html",
    "href": "notes/references.html",
    "title": "Part II Computational Physics",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "notes/numpy.html",
    "href": "notes/numpy.html",
    "title": "Part II Computational Physics",
    "section": "",
    "text": "The NumPy package is the key building block of the Python scientific ecosystem.\nIn this chapter we introduce a few of the key concepts. You should refer to the documentation for details. As with any mature software ecosystem, you should first assume that what you want to achieve can be achieved in a highly optimised way within the existing framework, and only resort to creating your own solution if and when you satisfy yourself that this is not the case.\nThere are a huge number of resources for learning NumPy online. This is one particular nice and compact tutorial.\n\n\nEverything in Python is an object. For example [1,2,3] is a list:\n\nmy_list = [1, 2, 3]\ntype(my_list)\n\nlist\n\n\nYou can think of an object as a container for properties and methods, the latter being functions associated with the object. Properties and methods are accessed with the . syntax. For example, lists have the append method, which adds an element to the end of the list:\n\nmy_list.append(\"boop\")\nmy_list\n\n[1, 2, 3, 'boop']\n\n\nWith IPython you can see all the available methods by hitting tab:\n\n\n\n\n\n\n\nDunder methods\n\n\n\n\n\nYou can list all of an objects properties and methods using dir:\n\ndir(my_list)\n\n['__add__',\n '__class__',\n '__class_getitem__',\n '__contains__',\n '__delattr__',\n '__delitem__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__gt__',\n '__hash__',\n '__iadd__',\n '__imul__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__len__',\n '__lt__',\n '__mul__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__reversed__',\n '__rmul__',\n '__setattr__',\n '__setitem__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'append',\n 'clear',\n 'copy',\n 'count',\n 'extend',\n 'index',\n 'insert',\n 'pop',\n 'remove',\n 'reverse',\n 'sort']\n\n\nNotice that lots of these are methods have a name sandwiched between double underscores and for this reason are called dunder methods (or magic methods, or just special methods). This is to indicate that they are not to be used by you, but by the Python interpreter to implement certain standard functions that apply to many different classes of objects. For instance, when you write len(my_list) to find the length of my_list Python is actually calling the dunder method my_list.__len__ which does the job of actually finding the length.\n\nmy_list.__len__()\n\n4\n\n\nIn this way the same function (len in this case) can operate on many different objects, an example of what is called polymorphism in object oriented programming.\n\n\n\n\n\n\n\nimport numpy as np\n\nThe fundamental object in NumPy is the Array (or ndarray), which you can think of as a multidimensional version of a list. If we were representing two dimensional data — or a matrix — in plain old Python you would use a list of lists.\n\ndata = [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]\n\nHere data[i] represents each row:\n\ndata[1]\n\n[4, 5, 6]\n\n\nIf I wanted to multiply every element by a number I would have to do something like this:\n\nfor row in data:\n    for j, _ in enumerate(row):\n        row[j] *= 2\ndata\n\n[[2, 4, 6], [8, 10, 12], [14, 16, 18], [20, 22, 24]]\n\n\nIf the details here are a bit unfamiliar don’t worry. The point I want to emphasize is: don’t do this.\nNumPy provides us with the tools to perform this kind of operation with minimum code and maximum efficiency. First we create our data as an array. There are numerous NumPy functions that produce arrays. The simplest is numpy.array. It takes data in the “Pythonic” list-of-lists(-of-lists-of… etc.) form and produces the corresponding ndarray\n\nmy_array = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\ntype(my_array)\n\nnumpy.ndarray\n\n\nNow, how do we multiply the whole array by a number? Like this, of course\n\n2 * my_array\n\narray([[ 2,  4,  6],\n       [ 8, 10, 12],\n       [14, 16, 18],\n       [20, 22, 24]])\n\n\nIt even prints nicely\n\nprint(my_array)\n\n[[ 1  2  3]\n [ 4  5  6]\n [ 7  8  9]\n [10 11 12]]\n\n\nArrays can be indexed, similar to lists\n\nprint(my_array[0], my_array[1], my_array[3][1])\n\n[1 2 3] [4 5 6] 11\n\n\nbut — different from a ordinary list of lists — the last one can be much more pleasantly achieved with the syntax\n\nmy_array[3,1]\n\n11\n\n\nWe also have a generalization of the slice syntax\n\nmy_array[1:, 1:]\n\narray([[ 5,  6],\n       [ 8,  9],\n       [11, 12]])\n\n\nSlicing can be mixed with integer indexing\n\nmy_array[1:, 1]\n\narray([ 5,  8, 11])\n\n\nNumPy offers all sorts of fancy indexing options for slicing and dicing your data: see the documentation for details.\nA fundamental property of an array is its shape:\n\n# [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]\nmy_array.shape\n\n(4, 3)\n\n\nThe way to read off the shape of an array is as follows. To begin with you encounter a number of [ corresponding to the rank of the array (two in the above example). You then scan over a number of entries that give the rightmost (innermost) dimension in the shape tuple before closing ] (3 here). After a number of 1D arrays [...] equal to the next innermost dimension (4 here), we have another closing ], and so on.\nIt’s definitely something that will take a bit of time getting used to!\nNotice that slicing does not change the rank of the array\n\nmy_array[1:, 1:].shape\n\n(3, 2)\n\n\nbut integer indexing does\n\nmy_array[1:, 1].shape\n\n(3,)\n\n\n\n\n\n\n\n\nWhat’s with the comma?\n\n\n\n\n\nIt’s to distinguish the tuple (3,) giving the shape from (3) which is just the number 3 in brackets\n\ntype((3,))\n\ntuple\n\n\n\ntype((3))\n\nint\n\n\n\n\n\nIt wouldn’t be very convenient if you always had to create your arrays from nested lists using np.array. Fortunately NumPy has lots of methods to create arrays with a given shape and populated in different ways:\n\na = np.zeros((2,2))\nprint(a)\n\nb = np.ones((2,2))\nprint(b)\n\nc = np.full((2,2), 5)\nprint(c)\n\nd = np.random.random((2,2)) # random numbers uniformly in [0.0, 1.0)\nprint(d)\n\n[[0. 0.]\n [0. 0.]]\n[[1. 1.]\n [1. 1.]]\n[[5 5]\n [5 5]]\n[[0.01579599 0.06067654]\n [0.50654924 0.49035426]]\n\n\nThere are also lots of methods to change the shape of arrays, for example\n\nnumpy.reshape to change the shape of an array.\nnumpy.expand_dims to insert new axes of length one.\nnumpy.squeeze (the opposite) to remove new axes of length one.\n\nLet’s see an example of the first one:\n\nmy_array.reshape(2, 2, 3)\n\narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]]])\n\n\nReshaping only works if the shapes are compatible. In this case it’s OK because the original shape was \\((4,3)\\) and \\(4\\times 3 = 2\\times 2\\times 3\\). If the shapes aren’t compatible, we’ll get an error\n\nmy_array.reshape(2, 3, 3)\n\nValueError: cannot reshape array of size 12 into shape (2,3,3)\n\n\nA NumPy array has a dtype property that gives the datatype. If the array was created from data, this will be inferred\n\nmy_array.dtype\n\ndtype('int64')\n\n\nFunctions that construct arrays also have an optional argument to specify the datatype\n\nmy_float_array = np.array([1,2,3], dtype=np.float64)\nmy_float_array.dtype\n\ndtype('float64')\n\n\nImportantly, complex numbers are supported\n\nmy_float_array = np.array([1.1 + 2.3j,2.2,3.6])\nmy_float_array.dtype\n\ndtype('complex128')\n\n\n\n\nIt’s not hard to come up with examples of array-like data in physics. The position, velocity, or acceleration of a particle will be three dimensional vectors, so have shape (3,). If we had \\(N\\) particles you could either put them in a \\(3N\\) dimensional vector, but for reasons that will become clear it would be better to store them in an array of shape (N,3), so that the first index indexes the particle number and the second the particle coordinate.\nAn \\(N\\times M\\) matrix has shape (N,M). The Riemann curvature tensor in General Relativity \\(R_{abcd}\\) has shape (4,4,4,4).\nOften we deal with fields which are functions of space and time e.g. the electric potential \\(\\phi(\\mathbf{r},t)\\). On a computer we often approximate these using a grid of space-time points \\(N_x\\times N_y \\times N_z\\times N_t\\). Thus the values of a scalar field could be stored in an array of shape (N_x,N_y,N_z,N_t). If we had a vector field like \\(\\mathbf{E}(\\mathbf{r},t)\\) this would be (N_x,N_y,N_z,N_t,3). You get the idea.\nA very useful method to create a grid of coordinate values (at which you can evaluate a function, say) is as follows\n\n# Grid of x, y points\nnx, ny = 64, 64\nx = np.linspace(-2, 2, nx)\ny = np.linspace(-2, 2, ny)\nX, Y = np.meshgrid(x, y)\n\n\n\nDipole electric field\n# Example from https://scipython.com/blog/visualizing-a-vector-field-with-matplotlib/\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle\n\ndef E(q, r0, x, y):\n    \"\"\"Return the electric field vector E=(Ex,Ey) due to charge q at r0.\"\"\"\n    den = np.hypot(x-r0[0], y-r0[1])**3\n    return q * (x - r0[0]) / den, q * (y - r0[1]) / den\n\n# Dipole\ncharges = [(1, (1, 0)), (-1, (-1, 0))]\n\n# Electric field vector, E=(Ex, Ey), as separate components\nEx, Ey = np.zeros((ny, nx)), np.zeros((ny, nx))\nfor charge in charges:\n    ex, ey = E(*charge, x=X, y=Y)\n    Ex += ex\n    Ey += ey\n\nfig = plt.figure()\nax = fig.add_subplot(111)\n\n# Plot the streamlines with an appropriate arrow style\ncolor = 2 * np.log(np.hypot(Ex, Ey))\nax.streamplot(x, y, Ex, Ey, color=color, linewidth=1,\n              density=2, arrowstyle='->', arrowsize=1.5)\n\n# Add filled circles for the charges themselves\ncharge_colors = {True: '#aa0000', False: '#0000aa'}\nfor q, pos in charges:\n    ax.add_artist(Circle(pos, 0.05, color=charge_colors[q>0]))\n\nax.set_xlabel('$x$')\nax.set_ylabel('$y$')\nax.set_xlim(-2,2)\nax.set_ylim(-2,2)\nax.set_aspect('equal')\nplt.show()\n\n\n\n\n\n\n\n\n\nI’ve already hinted at the payoff from all of this. On lists, multiplication by an integer concatentates multiple copies\n\n2 * [1, 2, 3]\n\n[1, 2, 3, 1, 2, 3]\n\n\nwhich is sometimes useful. But in numerical applications what we really want is this\n\n2 * np.array([1, 2, 3])\n\narray([2, 4, 6])\n\n\nThis illustrates a general feature of NumPy that all mathematical operations are performed elementwise on arrays!\n\nprint(np.array([1, 2, 3]) + np.array([4, 5, 6]))\nprint(np.array([1, 2, 3])**2)\nprint(np.sqrt(np.array([1, 2, 3])))\n\n[5 7 9]\n[1 4 9]\n[1.         1.41421356 1.73205081]\n\n\nThis avoids the need to write nested loops to perform some operation on each element of some multidimensional data. Of course, the loops are still there, it’s just that NumPy handles them in highly optimized C rather than Python. Code which operates in this way – rather than with explicit loops – is often described as vectorized, and in NumPy-speak vectorized functions are called ufuncs, short for universal functions (you can write your own if you need to). As a basic principle you should never use a Python loop to access your data in NumPy code. Loops may appear at a high level in stepping through time steps in a simulation, for example.\n\n\nVectorization is even more versatile than the above examples might suggest. Broadcasting is a powerful protocol that allows us to combine arrays of different shapes. Thus we can add a number to an array\n\nnp.array([1, 2, 3]) + 2.3\n\narray([3.3, 4.3, 5.3])\n\n\nMore generally, elementwise operations can be performed on two arrays of the same rank if in each dimension the sizes either match or one array has size 1.\n\n# These have shape (2, 3) and (1, 3)\nnp.array([[1, 2, 3], [4, 5, 6]]) + np.array([[4, 3, 2]])\n\narray([[5, 5, 5],\n       [8, 8, 8]])\n\n\nIn fact, we can simplify this last example\n\n# These have shape (2, 3) and (3,)\nnp.array([[1, 2, 3], [4, 5, 6]]) + np.array([4, 3, 2])\n\narray([[5, 5, 5],\n       [8, 8, 8]])\n\n\nBroadcasting two arrays follows these rules:\n\nIf the arrays do not have the same rank, prepend the shape of the lower rank array with 1s until both shapes have the same length.\nThe two arrays are said to be compatible in a dimension if they have the same size in the dimension, or if one of the arrays has size 1 in that dimension.\nThe arrays can be broadcast together if they are compatible in all dimensions. After broadcasting, each array behaves as if it had shape equal to the elementwise maximum of shapes of the two input arrays.\nIn any dimension where one array had size 1 and the other array had size greater than 1, the first array behaves as if it were copied along that dimension.\n\nThe documentation has more detail.\n\n\n\n\nThere are various specialized Python plotting libraries but the entry-level option is the catchily named Matplotlib. The pyplot module provides a plotting system that is similar to MATLAB (I’m told)\n\nimport matplotlib.pyplot as plt\n\nThis is probably the second most common import you will make. Here’s a simple example of the plot function, used to plot 2D data\n\n# Compute the x and y coordinates for points on a sine curve\nx = np.arange(0, 3 * np.pi, 0.1)\ny = np.sin(x)\n\n# Plot the points using matplotlib\nplt.plot(x, y)\nplt.show()\n\n\n\n\nNote: you must call plt.show() to make graphics appear. Here’s a fancier example with some labelling\n\n# Compute the x and y coordinates for points on sine and cosine curves\nx = np.arange(0, 3 * np.pi, 0.1)\ny_sin = np.sin(x)\ny_cos = np.cos(x)\n\n# Plot the points using matplotlib\nplt.plot(x, y_sin)\nplt.plot(x, y_cos)\nplt.xlabel('x axis label')\nplt.ylabel('y axis label')\nplt.title('Sine and Cosine')\nplt.legend(['Sine', 'Cosine'])\nplt.show()\n\n\n\n\nOften you’ll want to make several related plots and present them together, which can be achieved using the subplot function\n\nimport matplotlib.pyplot as plt\n\n# Compute the x and y coordinates for points on sine and cosine curves\nx = np.arange(0, 3 * np.pi, 0.1)\ny_sin = np.sin(x)\ny_cos = np.cos(x)\n\n# Set up a subplot grid that has height 2 and width 1,\n# and set the first such subplot as active.\nplt.subplot(2, 1, 1)\n\n# Make the first plot\nplt.plot(x, y_sin)\nplt.title('Sine')\n\n# Set the second subplot as active, and make the second plot.\nplt.subplot(2, 1, 2)\nplt.plot(x, y_cos)\nplt.title('Cosine')\n\n# Show the figure.\nplt.show()\n\n\n\n\n\n\nPixels in an image are encoded as a triple of RGB values in the range [0,255] i.e. 8 bits of type uint8 (the “u” is for “unsigned”). Tinting an image gives us a nice example of broadcasting\n\nimg = plt.imread('assets/lucian.jpeg')\n\nimg_tinted = img * [1, 0.55, 1]\n\n# Show the original image\nplt.subplot(1, 2, 1)\nplt.imshow(img)\nplt.title(\"Lucian\")\n\n# Show the tinted image\nplt.subplot(1, 2, 2)\nplt.title(\"Pink Panther\")\n# Having multiplied by floats, \n# we must cast the image to uint8 before displaying it.\nplt.imshow(np.uint8(img_tinted))\n\nplt.show()\nimg.shape, img.dtype\n\n\n\n\n((4032, 3024, 3), dtype('uint8'))\n\n\nThis is a standard 12 megapixel image.\n\n\n\n\nIn the course of your work you are likely to produce, as well as consume lots of data. While it’s good practice to keep notebooks capable of reproducing any of your analyses, this could be time consuming and resource heavy for larger computations. Thus at some point you’ll probably want to save and load data. For example, after saving the data of a large scale simulation you’d like to load it and perform some analysis.\nNumPy comes with its own save and load functions and associated binary format .npy. The benefit of using these is that after loading you get back a NumPy array ready to be used.\nA related function savez allows several arrays to be saved and then loaded as a dictionary-like object.\n\nrandom_matrix_1 = np.random.rand(4, 4)\nrandom_matrix_2 = np.random.rand(4, 4)\nnp.savez(\"assets/my-matrices\", first_matrix=random_matrix_1, second_matrix=random_matrix_2)\n%ls assets\n\nfibonacci.png     ising.js          metropolis.png    tab-complete.png\nhard-spheres.png  ising.py          my-matrices.npz\nia-question.png   lucian.jpeg       tab-complete.gif\n\n\n\nmy_matrix_file = np.load(\"assets/my-matrices.npz\")\nmy_matrix_file['first_matrix']\n\narray([[0.74126786, 0.15860146, 0.00161297, 0.28728838],\n       [0.73435138, 0.49455274, 0.89492342, 0.84795395],\n       [0.49881023, 0.77262989, 0.07185997, 0.95687876],\n       [0.23865614, 0.26021596, 0.24698979, 0.81666716]])"
  },
  {
    "objectID": "notes/dynamic.html",
    "href": "notes/dynamic.html",
    "title": "Part II Computational Physics",
    "section": "",
    "text": "Dynamic Programming"
  },
  {
    "objectID": "slides/numpy.html#preamble-objects-in-python",
    "href": "slides/numpy.html#preamble-objects-in-python",
    "title": "Part II Computational Physics",
    "section": "Preamble: objects in Python",
    "text": "Preamble: objects in Python\n\nEverything in Python is an object\nFor example [1,2,3] is a list:\n\n\nmy_list = [1, 2, 3]\ntype(my_list)\n\nlist\n\n\n\nObject is container for properties and methods (functions associated with object), accessed with . syntax.\ne.g. lists have append method:\n\n\nmy_list.append(\"boop\")\nmy_list\n\n[1, 2, 3, 'boop']"
  },
  {
    "objectID": "slides/numpy.html#arrays",
    "href": "slides/numpy.html#arrays",
    "title": "Part II Computational Physics",
    "section": "Arrays",
    "text": "Arrays\n\nFundamental object in NumPy is Array (or ndarray), multidimensional version of a list\nIn plain old Python a matrix would be a list of lists.\n\n\ndata = [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]\n\n\ndata[i] represents each row:\n\n\ndata[1]\n\n[4, 5, 6]"
  },
  {
    "objectID": "slides/numpy.html#mathematical-operations-with-arrays",
    "href": "slides/numpy.html#mathematical-operations-with-arrays",
    "title": "Part II Computational Physics",
    "section": "Mathematical operations with arrays",
    "text": "Mathematical operations with arrays\n\nOn lists\n\n\n2 * [1, 2, 3]\n\n[1, 2, 3, 1, 2, 3]\n\n\n\nIn numerical applications what we really want is\n\n\n2 * np.array([1, 2, 3])\n\narray([2, 4, 6])"
  },
  {
    "objectID": "slides/numpy.html#plotting-with-matplotlib",
    "href": "slides/numpy.html#plotting-with-matplotlib",
    "title": "Part II Computational Physics",
    "section": "Plotting with Matplotlib",
    "text": "Plotting with Matplotlib\n\nVarious specialized Python plotting libraries\n“entry-level” option is Matplotlib](https://matplotlib.org/)\npyplot module provides a plotting system that is similar to MATLAB (I’m told)\n\n\nimport matplotlib.pyplot as plt\n\n\nProbably the second most common import you will make!"
  },
  {
    "objectID": "slides/numpy.html#saving-and-loading-data",
    "href": "slides/numpy.html#saving-and-loading-data",
    "title": "Part II Computational Physics",
    "section": "Saving and loading data",
    "text": "Saving and loading data\n\nAt some point you’ll probably want to save and load data\nNumPy comes with its own save and load functions and associated binary format .npy\nThe benefit of using these is that after loading you get back a NumPy array ready to be used"
  },
  {
    "objectID": "slides/numpy.html#indexing",
    "href": "slides/numpy.html#indexing",
    "title": "Part II Computational Physics",
    "section": "Indexing",
    "text": "Indexing\n\nmy_array\n\narray([[ 1,  2,  3],\n       [ 4,  5,  6],\n       [ 7,  8,  9],\n       [10, 11, 12]])\n\n\n\nArrays can be indexed, similar to lists\n\n\nprint(my_array[0], my_array[1], my_array[3][1])\n\n[1 2 3] [4 5 6] 11\n\n\n\nBetter syntax for the last one\n\n\nmy_array[3,1]\n\n11"
  },
  {
    "objectID": "slides/numpy.html#shape",
    "href": "slides/numpy.html#shape",
    "title": "Part II Computational Physics",
    "section": "Shape",
    "text": "Shape\n\nA fundamental property of an array is shape:\n\n\n# [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]\nmy_array.shape\n\n(4, 3)\n\n\n\n\nFirst a number of [ corresponding to the rank of the array (two in the above example)\nThen number of entries giving rightmost (innermost) dimension in shape before closing ] (3 here)\nAfter a number of 1D arrays [...] equal to the next innermost dimension (4 here), we have another closing ], and so on"
  },
  {
    "objectID": "slides/numpy.html#other-ways-to-make-arrays",
    "href": "slides/numpy.html#other-ways-to-make-arrays",
    "title": "Part II Computational Physics",
    "section": "Other ways to make arrays",
    "text": "Other ways to make arrays\n\nNumPy has lots of methods to create arrays\n\n\na = np.zeros((2,2))\nprint(a)\nb = np.ones((2,2))\nprint(b)\nc = np.full((2,2), 5)\nprint(c)\nd = np.random.random((2,2)) # random numbers uniformly in [0.0, 1.0)\nprint(d)\neye = np.eye(2) # Identity matrix\nprint(eye)\n\n[[0. 0.]\n [0. 0.]]\n[[1. 1.]\n [1. 1.]]\n[[5 5]\n [5 5]]\n[[0.69341218 0.95626721]\n [0.97628693 0.67103196]]\n[[1. 0.]\n [0. 1.]]"
  },
  {
    "objectID": "slides/numpy.html#shape-shifting",
    "href": "slides/numpy.html#shape-shifting",
    "title": "Part II Computational Physics",
    "section": "Shape shifting",
    "text": "Shape shifting\n\nnumpy.reshape to change the shape of an array\nnumpy.expand_dims to insert new axes of length one.\nnumpy.squeeze (the opposite) to remove new axes of length one."
  },
  {
    "objectID": "slides/numpy.html#dtype",
    "href": "slides/numpy.html#dtype",
    "title": "Part II Computational Physics",
    "section": "dtype",
    "text": "dtype\n\nArrays have dtype property that gives datatype\nIf array was created from data, this will be inferred\n\n\nmy_array.dtype\n\ndtype('int64')\n\n\n\nFunctions constructing arrays have optional dtype\n\n\nmy_float_array = np.array([1,2,3], dtype=np.float64)\nmy_float_array.dtype\n\ndtype('float64')\n\n\n\nImportantly, complex numbers are supported\n\n\nmy_float_array = np.array([1.1 + 2.3j,2.2,3.6])\nmy_float_array.dtype\n\ndtype('complex128')"
  },
  {
    "objectID": "slides/numpy.html#examples-of-array-like-data",
    "href": "slides/numpy.html#examples-of-array-like-data",
    "title": "Part II Computational Physics",
    "section": "Examples of array-like data",
    "text": "Examples of array-like data\n\nPosition, velocity, or acceleration of particle will be three dimensional vectors, so have shape (3,)\nWith \\(N\\) particles could use a \\(3N\\) dimensional vector\nBetter: an array of shape (N,3)First index indexes particle number and second particle coordinate.\n\\(N\\times M\\) matrix has shape (N,M)\nRiemann curvature tensor in General Relativity \\(R_{abcd}\\) has shape (4,4,4,4)"
  },
  {
    "objectID": "slides/numpy.html#broadcasting",
    "href": "slides/numpy.html#broadcasting",
    "title": "Part II Computational Physics",
    "section": "Broadcasting…",
    "text": "Broadcasting…\n\n…is a powerful protocol for combining arrays of different shapes, generalizing this kind of thing\n\n\nnp.array([1, 2, 3]) + 2.3\n\narray([3.3, 4.3, 5.3])"
  },
  {
    "objectID": "slides/numpy.html#example-playing-with-images",
    "href": "slides/numpy.html#example-playing-with-images",
    "title": "Part II Computational Physics",
    "section": "Example: playing with images",
    "text": "Example: playing with images\n\nPixels in an image encoded as a triple of RGB values in the range [0,255] i.e. 8 bits of type uint8 (the “u” is for “unsigned”)\nTinting an image gives a nice example of broadcasting"
  }
]