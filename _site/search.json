[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course outline",
    "section": "",
    "text": "These are the materials for the Part II Physics course Computational Physics, taught in Lent Term 2023 at the University of Cambridge."
  },
  {
    "objectID": "index.html#computing-project",
    "href": "index.html#computing-project",
    "title": "Course outline",
    "section": "1.1 Computing Project",
    "text": "1.1 Computing Project\nAdditionally, you may choose to offer a Computational Physics project for one unit of further work. This involves choosing a problem from the project list. You will analyse the problem, write and test Python code to investigate it, then write up your work in a report. \nStudents may start their project work once the project list is published by 17th February. The deadline for submission of the project report is 16:00 on the first Monday of Full Easter term (1st May 2023)."
  },
  {
    "objectID": "index.html#these-notes",
    "href": "index.html#these-notes",
    "title": "Course outline",
    "section": "4.1 These notes…",
    "text": "4.1 These notes…\n…were prepared using Quarto. Each chapter should be thought of as a Jupyter notebook (actually, they are Jupyter notebooks), so you’ll probably only see import numpy as np once in each chapter, for example.\nThe code used to generate this site is in this GitHub repo. Please use issues to submit any typos and discussions to discuss the content.\nIn several places I’ve used examples from an earlier version of the course by David Buscher."
  },
  {
    "objectID": "slides/getting-going.html#goals",
    "href": "slides/getting-going.html#goals",
    "title": "Getting going",
    "section": "Goals",
    "text": "Goals\nIn this course you will learn\n\nAbout the Python scientific stack (based on NumPy)\nIts use in implementing some common algorithms in computational physics\nBasic ideas of computational complexity used in the analysis of algorithms"
  },
  {
    "objectID": "slides/getting-going.html#prerequisites",
    "href": "slides/getting-going.html#prerequisites",
    "title": "Getting going",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nAssume a knowledge of the Python language, including variables, control flow, and writing and using functions\nRefer to last year’s IB course (which had an excellent handout)…\n…and of course the internet\nFor an absolutely bare bones intro to Python try the first half of this tutorial"
  },
  {
    "objectID": "slides/getting-going.html#computational-physics.tripos.org",
    "href": "slides/getting-going.html#computational-physics.tripos.org",
    "title": "Getting going",
    "section": "computational-physics.tripos.org",
    "text": "computational-physics.tripos.org\n\nLecture notes (in progress)\nThese slides (if you want them)\nCode at github.com/AustenLamacraft/part-ii-computational-physics\nSubmit typos to the GH repo issues and use discussions to discuss…"
  },
  {
    "objectID": "slides/getting-going.html#housekeeping",
    "href": "slides/getting-going.html#housekeeping",
    "title": "Getting going",
    "section": "Housekeeping",
    "text": "Housekeeping\n\nEight lectures. Mondays and Fridays at 10.00 in the Pippard\nAfter the lectures: four computing exercises\nTo be completed in the last four weeks of full Lent term; one per week\nExercises count for 0.2 units or further work, or roughly 2% of your final mark for the year\nEach exercise should only take you a few hours."
  },
  {
    "objectID": "slides/getting-going.html#schedule",
    "href": "slides/getting-going.html#schedule",
    "title": "Getting going",
    "section": "Schedule",
    "text": "Schedule\n\nFirst lecture: Monday 23th January\nLast lecture: Friday 17th February\nFirst exercise: Friday 17th February – Friday 24th February\nSecond exercise: Friday 24th February – Friday 3rd March\nThird exercise: Friday 3rd March – Friday 10th March\nFourth exercise: Friday 10th March – Friday 17th March (last day of full Lent term)"
  },
  {
    "objectID": "slides/getting-going.html#computing-project",
    "href": "slides/getting-going.html#computing-project",
    "title": "Getting going",
    "section": "Computing Project",
    "text": "Computing Project\n\nYou may choose to offer a Computational Physics project for one unit of further work\nChoose a problem from the project list. Analyse the problem, write and test Python code to investigate it, then write up your work in a report\nProject list is published by 17th February\nDeadline for submission of the project report is 16:00 on the first Monday of Full Easter term (1st May 2023)"
  },
  {
    "objectID": "slides/getting-going.html#finding-your-way",
    "href": "slides/getting-going.html#finding-your-way",
    "title": "Getting going",
    "section": "Finding your way",
    "text": "Finding your way\n\nEveryone finds their own workflow for coding (language, editor, etc.)\nThis is a roundup of some popular tools in the Python ecosystem"
  },
  {
    "objectID": "slides/getting-going.html#your-coding-environment",
    "href": "slides/getting-going.html#your-coding-environment",
    "title": "Getting going",
    "section": "Your coding environment",
    "text": "Your coding environment\n\nYou will need to install the Python language (or run online)\nI recommend the Anaconda distribution\nComes with all parts of the toolkit we’ll need such as Jupyter notebooks and the major libraries NumPy and SciPy"
  },
  {
    "objectID": "slides/getting-going.html#ipython",
    "href": "slides/getting-going.html#ipython",
    "title": "Getting going",
    "section": "IPython",
    "text": "IPython\n\nIf you the above with python nice colour scheme is absent\nThis is called syntax highlighting and provides a visual guide to the syntax of the language\nIPython is an interactive shell that provides syntax highlighting and much more\nIf you have installed IPython (it comes with Anaconda) you can start it from the command line with ipython"
  },
  {
    "objectID": "slides/getting-going.html#helpful-features-of-ipython",
    "href": "slides/getting-going.html#helpful-features-of-ipython",
    "title": "Getting going",
    "section": "Helpful features of IPython:",
    "text": "Helpful features of IPython:\n\nTab completion: hit tab to autocomplete. Particularly useful for viewing all properties or methods of an object:\n\n\n\nTyping ?obj or obj? prints detailed information about the object obj (?? provides additional detail)"
  },
  {
    "objectID": "slides/getting-going.html#running-a-python-program",
    "href": "slides/getting-going.html#running-a-python-program",
    "title": "Getting going",
    "section": "Running a Python program",
    "text": "Running a Python program\n\nPython code in a file with a .py extension can be run from the command line with\n\npython hello_world.py\nor\npython -m hello_world\n\nIn the latter case -m option tells interpreter to look for a module called hello_world"
  },
  {
    "objectID": "slides/getting-going.html#importing-code",
    "href": "slides/getting-going.html#importing-code",
    "title": "Getting going",
    "section": "Importing code",
    "text": "Importing code\n\nA Python module is a file containing definition and statements\nBreaking long code into modules is good practice for writing clear and reusable software\nUsers may not want to see the details of a function in order to be able to us it"
  },
  {
    "objectID": "slides/getting-going.html#packages",
    "href": "slides/getting-going.html#packages",
    "title": "Getting going",
    "section": "Packages",
    "text": "Packages\n\nA collection of modules in a folder is called a package\nYou can import a package in the same way and access all the modules using the same . notation i.e. package.module1, package.module2, etc..\nSince explicit namespaces are preferred to avoid ambiguity use shorthands for the package or module you are importing:\n\n\nimport numpy as np\nnp.arange(10)\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\n(You can call it what you like, of course!)"
  },
  {
    "objectID": "slides/getting-going.html#installing-libraries",
    "href": "slides/getting-going.html#installing-libraries",
    "title": "Getting going",
    "section": "Installing libraries",
    "text": "Installing libraries\n\n99% of the code you run will have been written by somebody else in the form of a library\nPackage installation is handled by the command line utilities pip or conda, the latter being the package manager for the Anaconda distribution\nIf you have NumPy and SciPy installed you won’t need to worry about this too much"
  },
  {
    "objectID": "slides/getting-going.html#editors",
    "href": "slides/getting-going.html#editors",
    "title": "Getting going",
    "section": "Editors",
    "text": "Editors\n\nModern editors come with a huge number of tools that make writing code much easier\nSyntax highlighting, code completion, parameter information and documentation popups as you type\nThese go under the general heading IntelliSense\nThe latest hotness is GitHub Copilot: AI code suggestions\n(imo) these are all part of a continuum of productivity enhancements that enable people to write better code faster. Try them out!\nI use Visual Studio Code"
  },
  {
    "objectID": "slides/getting-going.html#notebooks",
    "href": "slides/getting-going.html#notebooks",
    "title": "Getting going",
    "section": "Notebooks",
    "text": "Notebooks\n\nSoftware developers write .py files, modules and packages\nScientists and others doing more exploratory work tend to favour a Notebook format that mixes code, text, and plots\nDominant option is Jupyter notebook, which comes with the Anaconda distribution\nStart from command line with jupyter notebook (or from the Anaconda Navigator application)\nOpens a notebook as a web page in your browser, where it can be edited and saved. The default extension is .ipynb"
  },
  {
    "objectID": "slides/monte-carlo.html#sampling-from-a-distribution",
    "href": "slides/monte-carlo.html#sampling-from-a-distribution",
    "title": "Part II Computational Physics",
    "section": "Sampling from a distribution",
    "text": "Sampling from a distribution\n\nSuppose we have a source of samples of random variable \\(X\\) described by a particular probability density function \\(p_X\\)\nCommon shorthand notation is \\(x\\sim p_X\\)\nBy definition probability of drawing a sample in the region \\([x, x+dx]\\) is \\(p_X(x)dx\\)"
  },
  {
    "objectID": "slides/monte-carlo.html#boxmuller-transform",
    "href": "slides/monte-carlo.html#boxmuller-transform",
    "title": "Part II Computational Physics",
    "section": "Box–Muller transform",
    "text": "Box–Muller transform\n\nTake two independent samples from a standard uniform distribution \\(u_{1,2}\\) and form \\[\n\\begin{align}\nx &= \\sqrt{-2\\log u_1}\\cos(2\\pi u_2)\\\\\ny &= \\sqrt{-2\\log u_1}\\sin(2\\pi u_2).\n\\end{align}\n\\] \\(x\\) and \\(y\\) are independent samples from a standard normal distribution."
  },
  {
    "objectID": "slides/monte-carlo.html#monte-carlo-integration",
    "href": "slides/monte-carlo.html#monte-carlo-integration",
    "title": "Part II Computational Physics",
    "section": "Monte Carlo integration",
    "text": "Monte Carlo integration\n\nDumb way to find \\(\\pi\\)\n\nmax_samples = 10000\ninside = 0\nareas = []\nfor sample in range(1, max_samples + 1):\n    x = random.uniform(-1, 1)\n    y = random.uniform(-1, 1)\n    \n    if x ** 2 + y ** 2 <= 1:\n        inside += 1\n    areas.append(4 * inside / sample)\n\nplt.plot(np.arange(1, max_samples + 1), areas)\nplt.plot(np.arange(1, max_samples + 1), np.pi * np.ones(max_samples), linestyle='dashed')\nplt.show()"
  },
  {
    "objectID": "slides/monte-carlo.html#importance-sampling",
    "href": "slides/monte-carlo.html#importance-sampling",
    "title": "Part II Computational Physics",
    "section": "Importance sampling",
    "text": "Importance sampling\n\nIf our function \\(f(\\mathbf{x})\\) has regions where it is very small, there is not much point in sampling its value there\nIf we can sample from a distribution where samples tend to fall in the region where \\(f(\\mathbf{x})\\) is large, it will probably be better to use that"
  },
  {
    "objectID": "slides/monte-carlo.html#sec-mcmc",
    "href": "slides/monte-carlo.html#sec-mcmc",
    "title": "Part II Computational Physics",
    "section": "Markov chain Monte Carlo",
    "text": "Markov chain Monte Carlo\n\nSuppose you want to generate configurations at random (i.e. with a uniform distribution) from a “gas” of hard disks\n\n\n\n\nCoins in a shoe box (gas of hard disks)"
  },
  {
    "objectID": "slides/monte-carlo.html#markov-chains",
    "href": "slides/monte-carlo.html#markov-chains",
    "title": "Part II Computational Physics",
    "section": "Markov chains",
    "text": "Markov chains\nYou know the random walk, perhaps as a model for diffusion\n\nAt each step make a move in a random direction, independently of your earlier moves\nAfter many steps these random moves gives rise to a distribution of possible locations\nA random walk is the simplest example of a Markov chain"
  },
  {
    "objectID": "slides/monte-carlo.html#mcmc-updates-for-the-ising-model",
    "href": "slides/monte-carlo.html#mcmc-updates-for-the-ising-model",
    "title": "Part II Computational Physics",
    "section": "MCMC updates for the Ising model",
    "text": "MCMC updates for the Ising model\n\nSimple proposal: pick each spin in turn in some order and try to flip it.\nForm of \\(p(\\sigma)\\) means that, although we cannot compute the probabilities explicitly, we can calculate ratios\nFor two configurations that differ only by \\(\\sigma_n=\\pm 1\\) we have \\[\n\\begin{align}\n\\frac{p(\\sigma_n=1|\\sigma_{m\\neq n})}{p(\\sigma_n=-1|\\sigma_{m\\neq n})} &= \\exp\\left[-2\\beta \\left(h_n+\\sum_{m\\neq n} J_{mn}\\sigma_m\\right)\\right]\\\\\n&\\equiv \\exp\\left[-\\beta\\Delta \\mathcal{E}\\right]\n\\end{align}\n\\] where \\(\\Delta \\mathcal{E}\\) is the energy difference"
  },
  {
    "objectID": "slides/complexity.html#first-example-multiplication",
    "href": "slides/complexity.html#first-example-multiplication",
    "title": "Part II Computational Physics",
    "section": "First example: multiplication",
    "text": "First example: multiplication\n\nBig numbers harder than small numbers. How much harder?\n\n\n\n\n\n\n\n\n\n\n\n\n×\n1\n3\n2\n2\n3\n1\n\n\n\n\n_\n_\n3\n_\n2\n6\n1\n4\n9\n2\n6\n3\n\n\n3\n9\n4\n8\n3"
  },
  {
    "objectID": "slides/complexity.html#defining-complexity",
    "href": "slides/complexity.html#defining-complexity",
    "title": "Part II Computational Physics",
    "section": "Defining complexity",
    "text": "Defining complexity\n\nThe complexity of a problem refers to this scaling of the number of steps involved\nDifficulty of particular task (or calculation) may vary considerably — \\(100\\times 100\\) is easy, for example\nInstead ask about how a particular general algorithm performs on a class of tasks\nIn CS multiplication of \\(n\\) digit numbers is a problem. Particular pair of \\(n\\) digit numbers is an instance\nAbove algorithm for multiplication that has quadratic complexity, or “\\(O(n^2)\\) complexity” (say “order \\(n\\) squared”)."
  },
  {
    "objectID": "slides/complexity.html#best-worst-average-case",
    "href": "slides/complexity.html#best-worst-average-case",
    "title": "Part II Computational Physics",
    "section": "Best / worst / average case",
    "text": "Best / worst / average case\n\nConsider search: finding an item in an (unordered) list of length \\(n\\). How hard is this?\nHave to check every item until you find the one you are looking for, so this suggests the complexity is \\(O(n)\\)\nCould be lucky and get it first try (or in first ten tries). The best case complexity of search is \\(O(1)\\).\nWorst thing that could happen is that the sought item is last: the worst case complexity is \\(O(n)\\)\nOn average, find your item near the middle of the list on attempt \\(\\sim n/2\\), so the average case complexity is \\(O(n/2)\\). This is the same as \\(O(n)\\) (constants don’t matter)"
  },
  {
    "objectID": "slides/complexity.html#polynomial-complexity",
    "href": "slides/complexity.html#polynomial-complexity",
    "title": "Part II Computational Physics",
    "section": "Polynomial complexity",
    "text": "Polynomial complexity\n\nYou’ve already learnt a lot of algorithms in mathematics (even if you don’t think of them this way)\nLet’s revisit some them through lens of computational complexity"
  },
  {
    "objectID": "slides/complexity.html#matrix-vector-multiplication",
    "href": "slides/complexity.html#matrix-vector-multiplication",
    "title": "Part II Computational Physics",
    "section": "Matrix-vector multiplication",
    "text": "Matrix-vector multiplication\n\nMultiplying a \\(n\\)-dimensional vector by a \\(n\\times n\\) matrix?\n\n\\[\n\\begin{align}\n\\sum_{j=1}^n M_{ij}v_j\n\\end{align}\n\\]\n\nSum contains \\(n\\) terms, and have to perform \\(n\\) such sums\nThus the complexity of this operation is \\(O(n^2)\\)."
  },
  {
    "objectID": "slides/complexity.html#matrix-matrix-multiplication",
    "href": "slides/complexity.html#matrix-matrix-multiplication",
    "title": "Part II Computational Physics",
    "section": "Matrix-matrix multiplication",
    "text": "Matrix-matrix multiplication\n\\[\n\\sum_{j} A_{ij}B_{jk}\n\\]\n\nInvolves \\(n\\) terms for each of the \\(n^2\\) assignments of \\(i\\) and \\(k\\). Complexity: \\(O(n^3)\\)"
  },
  {
    "objectID": "slides/complexity.html#better-than-linear",
    "href": "slides/complexity.html#better-than-linear",
    "title": "Part II Computational Physics",
    "section": "Better than linear?",
    "text": "Better than linear?\n\nSeems obvious that for search you can’t do better than linear\nWhat if the list is ordered? (numerical for numbers, or lexicographic for strings)\nExtra structure allows gives binary search that you may have seen before\nLook in middle of list and see if item you seek should be in the top half or bottom half\nTake the relevant half and divide it in half again to determine which quarter of the list your item is in, and so on"
  },
  {
    "objectID": "slides/complexity.html#exponentiation-by-squaring",
    "href": "slides/complexity.html#exponentiation-by-squaring",
    "title": "Part II Computational Physics",
    "section": "Exponentiation by squaring",
    "text": "Exponentiation by squaring\n\nExponentiation is problem of raising a number \\(b\\) (the base) to the \\(n\\)th power\nMultiply the number by itself \\(n\\) times: linear scaling\nThere’s a quicker way, since \\[\n\\begin{align}\nb^2 &= b\\cdot b\\\\\nb^4 &= b^2\\cdot b^2\\\\\nb^4 &= b^4\\cdot b^4\n\\end{align}\n\\]\nOnly have to do three multiplications!"
  },
  {
    "objectID": "slides/complexity.html#exponential-complexity",
    "href": "slides/complexity.html#exponential-complexity",
    "title": "Part II Computational Physics",
    "section": "Exponential complexity",
    "text": "Exponential complexity\n\nFibonacci numbers \\[\n0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233 ...\n\\]\n\n\\[\n\\text{Fib}(n) = \\text{Fib}(n-1) + \\text{Fib}(n-2)\n\\]\n\n\\(\\text{Fib}(n)\\) is defined in terms of lower values of \\(n\\), so a recursive definition possible"
  },
  {
    "objectID": "slides/complexity.html#sorting",
    "href": "slides/complexity.html#sorting",
    "title": "Part II Computational Physics",
    "section": "Sorting",
    "text": "Sorting\n\nTurning a list or array into a sorted list (conventionally in ascending order):\n\n\nrandom_array = np.random.randint(0,100, 10)\nsorted(random_array)\n\n[8, 12, 14, 14, 25, 30, 52, 59, 61, 73]\n\n\n\nWhat is Python actually doing?\nMany sorting algorithms. See Wikipedia for an extensive list"
  },
  {
    "objectID": "slides/complexity.html#bubble-sort",
    "href": "slides/complexity.html#bubble-sort",
    "title": "Part II Computational Physics",
    "section": "Bubble sort",
    "text": "Bubble sort\n\nRepeatedly pass through array, comparing neighbouring pairs of elements and switching them if they are out of order\nAfter first pass the largest element is in the rightmost position (largest index)\nSecond pass can finish before reaching last element, as it is already in place\nAfter second pass final two elements are correctly ordered\nContinue until array is sorted\nAnimation of bubble sort"
  },
  {
    "objectID": "slides/complexity.html#quicksort",
    "href": "slides/complexity.html#quicksort",
    "title": "Part II Computational Physics",
    "section": "Quicksort",
    "text": "Quicksort\n\nUses two key ideas:\n\nPossible in \\(O(n)\\) steps to partition an array into those elements larger (or equal) and those elements smaller than a given value (called the pivot).\nActing recursively on each partition requires only \\(O(\\log n)\\) partitions to completely sort array"
  },
  {
    "objectID": "slides/complexity.html#divide-and-conquer",
    "href": "slides/complexity.html#divide-and-conquer",
    "title": "Part II Computational Physics",
    "section": "Divide and conquer",
    "text": "Divide and conquer\n\nQuicksort, binary search, and exponentiation by squaring are all examples of divide and conquer algorithms\nAchieve performance by breaking task into two (or more) sub-problems of same type"
  },
  {
    "objectID": "slides/complexity.html#karatsuba-algorithm",
    "href": "slides/complexity.html#karatsuba-algorithm",
    "title": "Part II Computational Physics",
    "section": "Karatsuba algorithm",
    "text": "Karatsuba algorithm\n\nRecall “obvious” method for multiplication has quadratic complexity\nTry a divide and conquer type approach by splitting an \\(n\\)-digit number as follows \\[\nx = x_1 B^m + x_0\n\\]\n\\(B\\) is base and \\(m=\\lceil n\\rceil\\)\nIn base 10 \\(x=12345\\) is written as \\(12 * 1000 + 345\\)"
  },
  {
    "objectID": "slides/numpy.html#preamble-objects-in-python",
    "href": "slides/numpy.html#preamble-objects-in-python",
    "title": "Part II Computational Physics",
    "section": "Preamble: objects in Python",
    "text": "Preamble: objects in Python\n\nEverything in Python is an object\nFor example [1,2,3] is a list:\n\n\nmy_list = [1, 2, 3]\ntype(my_list)\n\nlist\n\n\n\nObject is container for properties and methods (functions associated with object), accessed with . syntax.\ne.g. lists have append method:\n\n\nmy_list.append(\"boop\")\nmy_list\n\n[1, 2, 3, 'boop']"
  },
  {
    "objectID": "slides/numpy.html#arrays",
    "href": "slides/numpy.html#arrays",
    "title": "Part II Computational Physics",
    "section": "Arrays",
    "text": "Arrays\n\nFundamental object in NumPy is Array (or ndarray), multidimensional version of a list\nIn plain old Python a matrix would be a list of lists.\n\n\ndata = [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]\n\n\ndata[i] represents each row:\n\n\ndata[1]\n\n[4, 5, 6]"
  },
  {
    "objectID": "slides/numpy.html#indexing",
    "href": "slides/numpy.html#indexing",
    "title": "Part II Computational Physics",
    "section": "Indexing",
    "text": "Indexing\n\nmy_array\n\narray([[ 1,  2,  3],\n       [ 4,  5,  6],\n       [ 7,  8,  9],\n       [10, 11, 12]])\n\n\n\nArrays can be indexed, similar to lists\n\n\nprint(my_array[0], my_array[1], my_array[3][1])\n\n[1 2 3] [4 5 6] 11\n\n\n\nBetter syntax for the last one\n\n\nmy_array[3,1]\n\n11"
  },
  {
    "objectID": "slides/numpy.html#shape",
    "href": "slides/numpy.html#shape",
    "title": "Part II Computational Physics",
    "section": "Shape",
    "text": "Shape\n\nA fundamental property of an array is shape:\n\n\n# [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]\nmy_array.shape\n\n(4, 3)\n\n\n\n\nFirst a number of [ corresponding to the rank of the array (two in the above example)\nThen number of entries giving rightmost (innermost) dimension in shape before closing ] (3 here)\nAfter a number of 1D arrays [...] equal to the next innermost dimension (4 here), we have another closing ], and so on"
  },
  {
    "objectID": "slides/numpy.html#other-ways-to-make-arrays",
    "href": "slides/numpy.html#other-ways-to-make-arrays",
    "title": "Part II Computational Physics",
    "section": "Other ways to make arrays",
    "text": "Other ways to make arrays\n\nNumPy has lots of methods to create arrays\n\n\na = np.zeros((2,2))\nprint(a)\nb = np.ones((2,2))\nprint(b)\nc = np.full((2,2), 5)\nprint(c)\nd = np.random.random((2,2)) # random numbers uniformly in [0.0, 1.0)\nprint(d)\neye = np.eye(2) # Identity matrix\nprint(eye)\n\n[[0. 0.]\n [0. 0.]]\n[[1. 1.]\n [1. 1.]]\n[[5 5]\n [5 5]]\n[[0.88033089 0.21886361]\n [0.82283125 0.64978079]]\n[[1. 0.]\n [0. 1.]]"
  },
  {
    "objectID": "slides/numpy.html#shape-shifting",
    "href": "slides/numpy.html#shape-shifting",
    "title": "Part II Computational Physics",
    "section": "Shape shifting",
    "text": "Shape shifting\n\nnumpy.reshape to change the shape of an array\nnumpy.expand_dims to insert new axes of length one.\nnumpy.squeeze (the opposite) to remove new axes of length one."
  },
  {
    "objectID": "slides/numpy.html#dtype",
    "href": "slides/numpy.html#dtype",
    "title": "Part II Computational Physics",
    "section": "dtype",
    "text": "dtype\n\nArrays have dtype property that gives datatype\nIf array was created from data, this will be inferred\n\n\nmy_array.dtype\n\ndtype('int64')\n\n\n\nFunctions constructing arrays have optional dtype\n\n\nmy_float_array = np.array([1,2,3], dtype=np.float64)\nmy_float_array.dtype\n\ndtype('float64')\n\n\n\nImportantly, complex numbers are supported\n\n\nmy_float_array = np.array([1.1 + 2.3j,2.2,3.6])\nmy_float_array.dtype\n\ndtype('complex128')"
  },
  {
    "objectID": "slides/numpy.html#examples-of-array-like-data",
    "href": "slides/numpy.html#examples-of-array-like-data",
    "title": "Part II Computational Physics",
    "section": "Examples of array-like data",
    "text": "Examples of array-like data\n\nPosition, velocity, or acceleration of particle will be three dimensional vectors, so have shape (3,)\nWith \\(N\\) particles could use a \\(3N\\) dimensional vector\nBetter: an array of shape (N,3). First index indexes particle number and second particle coordinate.\n\\(N\\times M\\) matrix has shape (N,M)\nRiemann curvature tensor in General Relativity \\(R_{abcd}\\) has shape (4,4,4,4)"
  },
  {
    "objectID": "slides/numpy.html#mathematical-operations-with-arrays",
    "href": "slides/numpy.html#mathematical-operations-with-arrays",
    "title": "Part II Computational Physics",
    "section": "Mathematical operations with arrays",
    "text": "Mathematical operations with arrays\n\nOn lists\n\n\n2 * [1, 2, 3]\n\n[1, 2, 3, 1, 2, 3]\n\n\n\nIn numerical applications what we really want is\n\n\n2 * np.array([1, 2, 3])\n\narray([2, 4, 6])"
  },
  {
    "objectID": "slides/numpy.html#broadcasting",
    "href": "slides/numpy.html#broadcasting",
    "title": "Part II Computational Physics",
    "section": "Broadcasting…",
    "text": "Broadcasting…\n\n…is a powerful protocol for combining arrays of different shapes, generalizing this kind of thing\n\n\nnp.array([1, 2, 3]) + 2.3\n\narray([3.3, 4.3, 5.3])"
  },
  {
    "objectID": "slides/numpy.html#plotting-with-matplotlib",
    "href": "slides/numpy.html#plotting-with-matplotlib",
    "title": "Part II Computational Physics",
    "section": "Plotting with Matplotlib",
    "text": "Plotting with Matplotlib\n\nVarious specialized Python plotting libraries\n“entry-level” option is Matplotlib\npyplot module provides a plotting system that is similar to MATLAB (I’m told)\n\n\nimport matplotlib.pyplot as plt\n\n\nProbably the second most common import you will make!"
  },
  {
    "objectID": "slides/numpy.html#example-playing-with-images",
    "href": "slides/numpy.html#example-playing-with-images",
    "title": "Part II Computational Physics",
    "section": "Example: playing with images",
    "text": "Example: playing with images\n\nPixels in an image encoded as a triple of RGB values in the range [0,255] i.e. 8 bits of type uint8 (the “u” is for “unsigned”)\nTinting an image gives a nice example of broadcasting"
  },
  {
    "objectID": "slides/numpy.html#saving-and-loading-data",
    "href": "slides/numpy.html#saving-and-loading-data",
    "title": "Part II Computational Physics",
    "section": "Saving and loading data",
    "text": "Saving and loading data\n\nAt some point you’ll probably want to save and load data\nNumPy comes with its own save and load functions and associated binary format .npy\nThe benefit of using these is that after loading you get back a NumPy array ready to be used"
  },
  {
    "objectID": "slides/numbers-and-odes.html#integers",
    "href": "slides/numbers-and-odes.html#integers",
    "title": "Part II Computational Physics",
    "section": "Integers",
    "text": "Integers\n\nSomething simpler\n\n\n1 + 1 == 2\n\nTrue\n\n\n\nIntegers can be represented in binary\n\n\n3 == 0b11 # Ooctal `0o` or hexadecimal `0h`\n\nTrue\n\n\n\nBinary string representation using bin function\n\n\nbin(-2)\n\n'-0b10'"
  },
  {
    "objectID": "slides/numbers-and-odes.html#floating-point-numbers",
    "href": "slides/numbers-and-odes.html#floating-point-numbers",
    "title": "Part II Computational Physics",
    "section": "Floating point numbers",
    "text": "Floating point numbers\n\n\\(0.1 + 0.2 \\neq 0.3\\) in Python is that specifying a real number exactly would involve an infinite number of bits\nAny finite representation necessarily approximate\nRepresentation for reals is called floating point arithmetic\nEssentially scientific notation\n\n\\[\\text{significand}  \\times \\text{exponent}\n\\]\n\nNamed floating point because number of digits after decimal point not fixed"
  },
  {
    "objectID": "slides/numbers-and-odes.html#sec-fp-numpy",
    "href": "slides/numbers-and-odes.html#sec-fp-numpy",
    "title": "Part II Computational Physics",
    "section": "Floating point numbers in NumPy",
    "text": "Floating point numbers in NumPy\n\nNumPy’s finfo function tells all machine precision\n\n\nnp.finfo(np.float64)\n\nfinfo(resolution=1e-15, min=-1.7976931348623157e+308, max=1.7976931348623157e+308, dtype=float64)\n\n\n\nNote that \\(2^{-52}=2.22\\times 10^{-16}\\) which accounts for resolution \\(10^{-15}\\)\nThis can be checked by finding when a number is close enough to treated as 1.0.\n\n\nx=1.0\nwhile 1.0 + x != 1.0:\n    x /= 1.01 \nprint(x)\n\n1.099427563084686e-16"
  },
  {
    "objectID": "slides/numbers-and-odes.html#the-dreaded-nan",
    "href": "slides/numbers-and-odes.html#the-dreaded-nan",
    "title": "Part II Computational Physics",
    "section": "The dreaded NaN",
    "text": "The dreaded NaN\n\nAs well as a floating point system, IEEE 754 defines Infinity and NaN (Not a Number)\n\n\nnp.array([1, -1, 0]) / 0\n\narray([ inf, -inf,  nan])\n\n\n\nThey behave as you might guess\n\n\n2 * np.inf, 0 * np.inf, np.inf > np.nan\n\n(inf, nan, False)"
  },
  {
    "objectID": "slides/numbers-and-odes.html#eulers-method",
    "href": "slides/numbers-and-odes.html#eulers-method",
    "title": "Part II Computational Physics",
    "section": "Euler’s method",
    "text": "Euler’s method\n\\[\n\\frac{dx}{dt} = f(x, t)\n\\]\n\nSimplest approach: approximate LHS of ODE\n\n\\[\n\\frac{dx}{dt}\\Bigg|_{t=t_j} \\approx \\frac{x_{j+1} - x_j}{h}\n\\]\n\\[\nx_{j+1} = x_j + hf(x_j, t_j)\n\\]"
  },
  {
    "objectID": "slides/numbers-and-odes.html#using-scipy",
    "href": "slides/numbers-and-odes.html#using-scipy",
    "title": "Part II Computational Physics",
    "section": "Using SciPy",
    "text": "Using SciPy\n\nComing up with integration schemes is best left to the professionals\nTry integrate module of the SciPy library\nscipy.integrate.solve_ivp provides a versatile API"
  },
  {
    "objectID": "slides/numbers-and-odes.html#hénonheiles-system",
    "href": "slides/numbers-and-odes.html#hénonheiles-system",
    "title": "Part II Computational Physics",
    "section": "Hénon–Heiles system",
    "text": "Hénon–Heiles system\n\nModel chaotic system with origins in stellar dynamics\n\n\\[\n\\begin{align}\n\\dot x &= p_x \\\\\n\\dot p_x &= -x -2\\lambda xy \\\\\n\\dot y &= p_y \\\\\n\\dot p_y &=  - y -\\lambda(x^2-y^2).\n\\end{align}\n\\]\n\nExample of Hamilton’s equations\nPhase space is now four dimensional and impossible to visualize."
  },
  {
    "objectID": "notes/assignments.html",
    "href": "notes/assignments.html",
    "title": "Part II Computational Physics",
    "section": "",
    "text": "Autocorrelation times of various algorithms. Scaling with system size."
  },
  {
    "objectID": "notes/getting-going.html",
    "href": "notes/getting-going.html",
    "title": "Getting going",
    "section": "",
    "text": "1 Finding your way\nEveryone finds their own workflow for coding, depending on their preferred language, editor, how they run their code, and so on. The aim of the sections below is to give a roundup of some popular tools in the Python ecosystem.\n\n\n2 Your coding environment\nTo run Python code on your computer you will need to have installed the Python language. I recommend the Anaconda distribution as it comes with all the parts of the toolkit we’ll need such as Jupyter notebooks and the major libraries NumPy and SciPy.\nTry running python at the command line. You should get something like\nPython 3.9.12 (main, Apr  5 2022, 01:53:17) \n[Clang 12.0.0 ] :: Anaconda, Inc. on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \nYou should confirm that you are using Python 3 (the command python3 will also work and guarantee this if you happen to have Python 2 as the default). The prompt >>> indicates that you have started the Python interactive shell or REPL and are good to go:\n\nprint(\"Hello world!\")\n1 + 2\n\nHello world!\n\n\n3\n\n\nTo leave and return to the command line, you can run quit() or exit().\n\n\n3 IPython\nIf you ran the above command from within python you may have noticed that the nice colour scheme that you see above was absent. This is called syntax highlighting and provides a visual guide to the syntax of the language.\nIPython is an interactive shell that provides syntax highlighting and much more. If you have installed IPython (it comes with Anaconda) you can start it from the command line with ipython.\nAmong the most helpful features of IPython are:\n\nTab completion: hit tab to autocomplete. This is particularly useful for viewing all properties or methods of an object: \nTyping ?obj or obj? prints detailed information about the object obj (?? provides additional detail).\nCertain magic commands prefixed by % that provide certain additional functionality. For example, %timeit finds the executation time of a single line statement, which is useful when profiling the performance of code:\n\n\n%timeit L = [n ** 2 for n in range(1000)]\n\n235 µs ± 6.51 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n%timeit automatically runs several times to give some statistics on the execution time. For multiple lines you can use the %%timeit magic.\nYou can find much more exploring the documentation.\n\n\n4 Running a Python program\nPython code in a file with a .py extension can be run from the command line with python hello_world.py or python -m hello_world. In the latter case the -m option tells the interpreter to look for a module called hello_world. More on modules below.\nFrom the IPython shell you can instead use run hello_world.py or just run hello_world.\n\n\n\n5 Importing code\nA Python module is just a file containing definition and statements. Breaking long code into modules is good practice for writing clear and reusable software. Users may not want to delve into the details of some function you have written in order to be able to us it, and separating the corresponding code into a separate file is a hygienic way to handle this.\nThus if I make the file hello_world.py containing the function:\n\ndef hello():\n    print(\"Hello world!\")\n\nI can run this function by first importing the module:\n\nimport hello_world\nhello_world.hello()\n\nHello world!\n\n\nNotice that the function hello is accessed from the hello_world namespace. This is to avoid any confusion that may arise if more that one imported module has a function of the same name. If you are confident that’s not an issue and want more concise code you can do this:\n\nfrom hello_world import hello\nhello()\n\nHello world!\n\n\nor even import everything with the wildcard *:\n\nfrom hello_world import *\nhello()\n\nHello world!\n\n\nThe issue with the latter is that it may introduce a whole bunch of names that may interfere with things you already defined.\nA collection of modules in a folder is called a package. You can import a package in the same way and access all the modules using the same . notation i.e. package.module1, package.module2, etc..\nSince explicit namespaces are preferred to avoid ambiguity it’s common to introduce shorthand names for the package or module you are importing, hence the ubiquitous:\n\nimport numpy as np\nnp.arange(10)\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n(You can call it what you like, of course!)\nFor details about where the interpreter looks to find modules you try to import are in the documentation.\n\n\n6 Installing libraries\n99% of the code 1 you run will have been written by somebody else in the form of a library (a collection of modules or packages). Package installation is handled by the command line utilities pip or conda, the latter being the package manager for the Anaconda distribution. If you have NumPy and SciPy installed you won’t need to worry about this too much in this course.\n\n\n7 Editors\nModern editors come with a huge number of tools that make writing code much easier, and you would be crazy not to take advantage of them. These range from the visual cues provided by syntax highlighting – which we’ve already met – to code completion, parameter information and documentation popups as you type. These go under the general heading IntelliSense. The latest hotness is GitHub Copilot, which uses AI to make code suggestions. In my view, these are all part of a continuum of productivity enhancements that enable people to write better code faster. Use them (wisely).\nI use Visual Studio Code.\n\n\n8 Notebooks\nWhile software developers write .py files, modules and packages, scientists and others doing more exploratory work tend to favour a Notebook format that mixes code, text, and plots. The dominant option is the Jupyter notebook, which comes with the Anaconda distribution and can be started from the command line with jupyter notebook (or from the Anaconda Navigator application). This will open the notebook as a web page in your browser, where it can be edited and saved. The default extension is .ipynb.\nJupyter notebooks can actually run code in different languages (the processes running a particular language is called a kernel), but the default process is IPython with all the benefits described above.\nThe text cells can be formatted using Markdown and also support \\(\\LaTeX\\) equations, which is pretty handy for us.\nGoogle has their own cloud version of the Jupyter notebook called Colab. You can try it out for free, though you have to pay for significant compute. The “next generation” of the Jupyter notebook is called JupyterLab and can be started with jupyter lab. Notebook files can be opened in either Jupyter Lab or Jupyter Notebook\n\n\n\n\n\n\nFootnotes\n\n\nThis is a conservative estimate↩︎"
  },
  {
    "objectID": "notes/monte-carlo.html",
    "href": "notes/monte-carlo.html",
    "title": "Monte Carlo methods",
    "section": "",
    "text": "Many physical phenomena, notably those falling within the domains of statistical mechanics and quantum theory, depend in an essential way on randomness. The simulation of these phenomena therefore requires algorithms that incorporate random (or pseudo-random) elements in the most efficient way."
  },
  {
    "objectID": "notes/monte-carlo.html#monte-carlo-integration",
    "href": "notes/monte-carlo.html#monte-carlo-integration",
    "title": "Monte Carlo methods",
    "section": "2.1 Monte Carlo integration",
    "text": "2.1 Monte Carlo integration\nThe technique is exemplified by the following fairly dumb way of estimating \\(\\pi\\)\n\nmax_samples = 10000\ninside = 0\nareas = []\nfor sample in range(1, max_samples + 1):\n    x = random.uniform(-1, 1)\n    y = random.uniform(-1, 1)\n    \n    if x ** 2 + y ** 2 <= 1:\n        inside += 1\n    areas.append(4 * inside / sample)\n\nplt.plot(np.arange(1, max_samples + 1), areas)\nplt.plot(np.arange(1, max_samples + 1), np.pi * np.ones(max_samples), linestyle='dashed')\nplt.show()\n\n\n\n\nIn terms of integration, you can think of this as a way to compute the integral of a function which is one inside the unit disc, and zero outside it.\nAlthough it’s a silly method, this does illustrate one important feature of Monte Carlo methods in general: that the relative error with \\(N\\) samples is typically \\(\\propto N^{-1/2}\\) (thus at the 1% level for \\(10^4\\) samples) because the variance of a sum of \\(N\\) iid variables is \\(\\propto N^{1/2}\\).\nThe general setting of Monte Carlo integration is as follows. Suppose we have a multidimensional integral to evaluate over some domain \\(D\\)\n\\[\nI(f,D) = \\int_D f(\\mathbf{x}) d\\mathbf{x}\n\\]\nIf we can sample points uniformly within \\(D\\), then an estimate for the integral is\n\\[\nI(f,D) = \\frac{V_D}{N}\\sum_{i=1}^N f(\\mathbf{x}_i)\n\\]\nwhere \\(N\\) is the number of samples and \\(V_D\\) is the (hyper-)volume of \\(D\\). Why does this work? Because the uniform distribution has constant probability density \\(1/V_D\\) so the average of \\(f(\\mathbf{x}_i)\\) with respect to this uniform distribution is simply related to the integral we are trying to calculate\n\\[\n\\bar f = \\frac{1}{V_D}\\int f(\\mathbf{x})d\\mathbf{x}.\n\\]\nBy taking many samples and averaging \\(f(\\mathbf{x}_i)\\) we can estimate this average. In the simple example that we started with \\(f(\\mathbf{x})\\) would be a “top hat” function that is one inside the circle. As in that example, the relative error is \\(\\propto N^{-1/2}\\), whatever the dimension.\nFor this reason Monte Carlo integration comes into its own for high dimensional problems. For low dimensional integrals the quadrature methods in scipy.integrate are preferable:\n\nfrom scipy import integrate\nintegrate.quadrature(np.cos, 0, np.pi / 2)\n\n(0.9999999999999536, 3.9611425250996035e-11)\n\n\nAs for ODE solvers, there is a lot of detail in the implementation to do with how intervals are chosen, and so on."
  },
  {
    "objectID": "notes/monte-carlo.html#importance-sampling",
    "href": "notes/monte-carlo.html#importance-sampling",
    "title": "Monte Carlo methods",
    "section": "2.2 Importance sampling",
    "text": "2.2 Importance sampling\nMonte Carlo integration is not restricted to sampling from the uniform distribution. If our function \\(f(\\mathbf{x})\\) has regions where it is very small, there is not much point in sampling its value there. If there is a distribution we can sample from where samples tend to fall in the region where \\(f(\\mathbf{x})\\) is large, it will probably be better to use that. In this case we calculate the weighted average using the probability density \\(p_\\text{sample}(\\mathbf{x})\\) from where the samples are drawn\n\\[\nI(f,D, p_\\text{sample}) = \\frac{1}{N}\\sum_{i=1}^N \\frac{f(\\mathbf{x}_i)}{p_\\text{sample}(\\mathbf{x}_i)}\n\\]\nThe reason this works is that the average of one of the terms in the sum is just the integral we want\n\\[\n\\overline{\\frac{f(\\mathbf{x})}{p_\\text{sample}(\\mathbf{x})}} = \\int \\frac{f(\\mathbf{x})}{p_\\text{sample}(\\mathbf{x})} p_\\text{sample}(\\mathbf{x})d\\mathbf{x} = \\int f(\\mathbf{x})d\\mathbf{x}\n\\]\nThe benefit of this approach is that it can lead to a drastic reduction in the variance of the estimator. To take an extreme example: if \\(f(\\mathbf{x})\\propto p_\\text{sample}(\\mathbf{x})\\), and even a single sample leads to perfect estimate with no uncertainty! This observation is not useful, if you knew \\(p_\\text{sample}(\\mathbf{x})\\) you would know the constant factor by which \\(f(\\mathbf{x})\\) differs, but it illustrates the point about variance reduction.\nThis general technique is called Importance sampling. To apply the above approach, one needs both an explicit form for \\(p_\\text{sample}(\\mathbf{x})\\) and the ability to generate samples, which is rather restrictive. There are many elaborations of the basic idea, however, including multiple distributions as well as adaptive sampling to “discover” the right region for sampling."
  },
  {
    "objectID": "notes/monte-carlo.html#sec-mcmc",
    "href": "notes/monte-carlo.html#sec-mcmc",
    "title": "Monte Carlo methods",
    "section": "2.3 Markov chain Monte Carlo",
    "text": "2.3 Markov chain Monte Carlo\nSuppose you want to generate configurations at random (i.e. with a uniform distribution) from a “gas” of hard disks 2.\n\n\n\nCoins in a shoe box (gas of hard disks). From Krauth (1998)\n\n\nIt’s harder than it looks! The first guess you might have is to start adding coins at random, and if you get an overlap, try again until you don’t. Obviously this will become inefficient as the box fills up, and most attempts fail. Worse, it doesn’t in fact yield a uniform distribution! 3\nHere’s an approach that works:\n\nExample 1 (Metropolis algorithm for hard disks)  \n\nFix the number of disks and an initial configuration (some regular lattice configuration, say).\nPick a disk at random and attempt (or propose) to move it by a small random amount (i.e. random direction; random small magnitude).\nIf this results in the moved disk intersecting another, reject the move, leaving the disk where it is. Otherwise, accept the move.\nRepeat 2. and 3. many times.\n\n\n.\nThis is the simplest example of the Metropolis–Hastings algorithm, the first Markov chain Monte Carlo (MCMC) algorithm.\nMore generally, the goal of MCMC is to come up with a sequential random process (a Markov chain) that generates (usually after many steps) a sample from a particular distribution.\nYou’ve all heard of a random walk, perhaps as a model for diffusion. At each step you make a move in a random direction, independently of your earlier moves. After many steps these random moves gives rise to a distribution of possible locations. A random walk is the simplest example of a Markov chain.\nMore generally, a Markov chain is a sequence of random variables \\(X_n\\) with each having a distribution that is is conditional on the value of the previous one, and so is defined in terms of transition probabilities \\(p(X_{n}=x_n|X_{n-1}=x_{n-1})\\) (hence they form a “chain”). I’m going to immediately drop this cumbersome notation in favour of \\(p(x_n|x_{n-1})\\), a function of \\(x_n\\) and \\(x_{n-1}\\), but in general the function giving the transition probabilities can be different at each step (the random variables could all be different).\nThe probability of a particular sequence \\(X_1=x_1\\ldots X_n=x_n\\) is therefore\n\\[\np(x_n|x_{n-1})p(x_{n-1}|x_{n-2})\\cdots p(x_2|x_{1})p^{(1)}(x_1)\n\\]\n\\(X_1\\) has no “parent” so is not conditional on any other value.\nSuppose we don’t care about the earlier values and just want to know the marginal distribution \\(p^{(n)}(x_n)\\) of the final variable. For a random walk this is easy, as \\(x_n\\) typically represents a displacement that is a sum of iid increments. In general this is not the case, however, as the marginal distribution is\n\\[\np^{(n)}(x_n)=\\sum_{x_{n-1},\\ldots x_1}p(x_n|x_{n-1})p(x_{n-1}|x_{n-2})\\cdots p(x_2|x_{1})p^{(1)}(x_1)\n\\]\n(I’m writing all these expressions for discrete random variables, but the continuous version involving probability density functions is straightforward)\nThe sums are over all possible values that the random variables might take in the state space of the problem. These could be finite or infinite in number.\nThings are not as bad as they appear, however, as the marginal distribution can be interpreted as the result of acting \\(n-1\\) times on the vector of values of \\(p^{(1)}_j\\equiv p^{(1)}(j)\\) with the transition matrix with elements \\(\\mathsf{P}_{jk}=p(j|k)\\)\n\\[\n\\mathbf{p}^{(n)} = \\mathsf{P}^{n-1}\\mathbf{p}^{(1)}.\n\\]\nIn a single step the marginal probabilities are updated as\n\\[\n\\mathbf{p}^{(n)} = \\mathsf{P}^{n}\\mathbf{p}^{(n-1)}.\n\\]\n\\(\\mathsf{P}\\) has some structure. The matrix elements are positive, as they represent probabilities, and each row sums to one\n\\[\n\\sum_j \\mathsf{P}_{jk} = 1.\n\\]\nSuch matrices are called stochastic.\nAlthough \\(p^{(n)}\\) — the probability distribution at the \\(n\\)th step — changes from step to step, you might expect that after many steps it tends to converge to a stationary distribution \\(p^{(n)}\\to\\boldsymbol{\\pi}\\). If it exists, this distribution must satisfy\n\\[\n\\boldsymbol{\\pi} = \\mathsf{P}\\boldsymbol{\\pi}.\n\\tag{1}\\]\nIn other words, it is an eigenvector of \\(\\mathsf{P}\\) with eigenvalue one. This property is guaranteed by the Perron–Frobenius theorem 4.\nThus \\(\\mathsf{P}\\) determines \\(\\boldsymbol{\\pi}\\). MCMC turns this idea on its head and asks: if there is some \\(\\boldsymbol{\\pi}\\) that I would like to generate samples from, can I find a \\(\\mathsf{P}\\) that has it as a stationary distribution?\nThere is a trivial answer to this question. Sure, take \\(\\mathsf{P}_{jk}=\\boldsymbol{\\pi}_j\\). That is, jump straight to the stationary distribution no matter what the starting state. But we are interested in highly complicated distributions over large state spaces (think the Boltzmann distribution for a statistical mechanical system comprised of billions of particles). Thus what we really want is to be able to approach such a complicated distribution by making many transitions with simple distributions.\nOne more idea is useful before returning to concrete algorithms. The quantity\n\\[\n\\mathsf{P}_{jk}\\pi_k = p(j|k)\\pi_k = p(j,k)\n\\]\nis the joint distribution of seeing state \\(k\\) followed by state \\(j\\) in the stationary distribution. A reversible Markov chain is one where \\(p(j,k)=p(k,j)\\). Roughly, you can’t tell the direction of time because any transition is equally likely to happen forward in time as backward. Random physical processes that respect time reversal symmetry are often modeled as reversible Markov processes.\nCombining reversibility with the definition of the stationary state yields the condition of detailed balance\n\\[\n\\mathsf{P}_{jk}\\pi_k = \\pi_j\\mathsf{P}_{kj}.\n\\tag{2}\\]\nThis condition is stronger than the condition Equation 1 for a stationary state. This makes it easier to check: you don’t have to do a sum over a state space index. The Metropolis algorithm Example 1 for the hard disk problem satisfies detailed balance for a stationary distribution that is constant when disks don’t intersect and zero when they do.\nWhen the stationary distribution \\(\\boldsymbol{\\pi}\\) has more structure, designing an appropriate transition matrix is harder. The idea is to generalize the hard disk approach by separating the transition into a proposal distribution \\(p_\\text{prop}(j|k)\\) and an acceptance distribution \\(p_\\text{acc}(a=0,1|j\\leftarrow k)\\) that gives the probability of a move from \\(k\\) to \\(j\\) being accepted (\\(a=1\\)) or rejected (\\(a=0\\)). The probability of moving from \\(k\\) to \\(j\\) is then\n\\[\np(j|k) = p_\\text{acc}(a=1|j\\leftarrow k) p_\\text{prop}(j|k).\n\\]\nSubstituting this into the detailed balance condition Equation 2 gives \\[\n\\frac{p_\\text{acc}(a=1|j\\leftarrow k)}{p_\\text{acc}(a=1|k\\leftarrow j)} = \\frac{\\pi_j}{\\pi_k}\\frac{p_\\text{prop}(k|j)}{p_\\text{prop}(j|k)}.\n\\]\nAny \\(p_\\text{acc}\\) that satisfies this relation for all \\(j\\) and \\(k\\) will do the job. The Metropolis choice is\n\\[\np_\\text{acc}(a=1|j \\leftarrow k) = \\min\\left(1,  \\frac{\\pi_j}{\\pi_k}\\frac{p_\\text{prop}(k|j)}{p_\\text{prop}(j|k)}\\right).\n\\tag{3}\\]\nThis gives an extremely general algorithm, one of the top ten in applied mathematics, according to one list:\n\nExample 2 (Metropolis algorithm)  \n\nStarting from state \\(k\\) sample a next state \\(j\\) from the proposal distribution \\(p_\\text{prop}(j|k)\\).\nAccept the proposal with probability \\(p_\\text{acc}(a=1|j \\leftarrow k)\\) and move to state \\(j\\). Otherwise reject the proposal and stay in state \\(k\\).\nRepeat 1. and 2. many times.\n\n\nMCMC has the benefit of being embarrassingly parallel. If you want to average something over \\(\\boldsymbol{\\pi}\\), just run the algorithm many times independently and average the results. This is perfect for parallel computing.\nThe Metropolis algorithm has an Achilles’ heel, however. To perform a move one has to sample from \\(p_\\text{prop}(j|k)\\) and from \\(p_\\text{acc}(a|j \\leftarrow k)\\). The proposal therefore has to be tractable, like the small shift in position for the hard disk case. This may however, mean that that many of the \\(j\\)s suggested correspond to very small \\(\\pi_j\\), and therefore a very low acceptance probability (c.f. Equation 3). For example, in the hard disk case at high density many proposed moves will give rise to overlap of disks and be rejected. This means that many steps are required to have one successful update of the simulation. This kind of slowdown is a common feature of MCMC methods applied to complex distributions.\nWe’ll see some more examples of MCMC algorithms for statistical mechanical problems in Section 3, and ways in which this problem can be avoided."
  },
  {
    "objectID": "notes/monte-carlo.html#mcmc-updates-for-the-ising-model",
    "href": "notes/monte-carlo.html#mcmc-updates-for-the-ising-model",
    "title": "Monte Carlo methods",
    "section": "3.1 MCMC updates for the Ising model",
    "text": "3.1 MCMC updates for the Ising model\nHow does MCMC work in practice for the Ising model? To apply the Metropolis alogorithm Example 2 we can use a simple proposal: pick each spin in turn in some order and try to flip it.\nThe form of \\(p(\\sigma)\\) means that, although we cannot compute the probabilities explicitly, we can calculate ratios, which is all we need for Metropolis. For two configurations that differ only by \\(\\sigma_n=\\pm 1\\) we have\n\\[\n\\begin{align}\n\\frac{p(\\sigma_n=1|\\sigma_{m\\neq n})}{p(\\sigma_n=-1|\\sigma_{m\\neq n})} &= \\exp\\left[-2\\beta \\left(h_n+\\sum_{m\\neq n} J_{mn}\\sigma_m\\right)\\right]\\\\\n&\\equiv \\exp\\left[-\\beta\\Delta \\mathcal{E}\\right],\n\\end{align}\n\\]\nwhere \\(\\Delta \\mathcal{E}\\) is the energy difference between two configurations.\nOne alternative to Metropolis is the Heat bath algorithm (or Glauber dynamics or Gibbs sampling) 6. The idea behind the name is that, since we can calculate the influence of the spin’s environment (the “bath”), we can just choose the spin’s orientation with the corresponding probabilities. Since there are only two probabilities the ratio is all we need and we get\n\\[\np(\\sigma_n=\\pm 1|\\sigma_{m\\neq n}) = \\frac{1}{1+ e^{\\pm\\beta \\Delta \\mathcal{E}}}.\n\\tag{5}\\]\nThe algorithm is then:\n\nExample 3 (Heat bath algorithm)  \n\nPick a spin \\(n\\). 7\nCompute \\(\\Delta E\\), the energy difference between \\(\\sigma_n=\\pm 1\\).\nSet \\(\\sigma_n=\\pm 1\\) with probabilities given by Equation 5.\nRepeat 1-3 many times\n\n\nWhat happens if we try and come up with more complicated proposals, flipping many spins at once? For Metropolis, the problem is that without a cleverly designed proposal we will be suggesting moves that are likely to be rejected. For the heat bath algorithm, the more spins we flip, the more complicated the evaluation of the corresponding probabilities (\\(2^n\\) outcomes if we flip \\(n\\) spins).\nThe good news is that we can do better — much better — than the above algorithms. The Wolff algorithm is one example. This proposes a cluster of spins of the same orientation to be flipped by adding adjacent spins to an initially random chosen spin with probability \\(p_\\text{add}\\). It turns out that for the nearest neighbour Ising model with Ferromagnetic coupling \\(J<0\\) the “magic” value \\(p_\\text{add}=1-e^{2\\beta J}\\) is rejection free: the probability to flip the whole cluster is always one. This makes for an extremely fast algorithm that is not subject to the usual critical slowing down at phase transitions.\n\n\nIsing model code\nclass IsingModel:\n    def __init__(self, L):\n        self.L = L\n        self.spins = np.random.choice(a=[1, -1], size=(L, L))\n        stagger = np.empty(self.L, dtype = bool)\n        stagger[::2] = True\n        stagger[1::2] = False\n        self.mask = np.logical_xor(stagger[:, np.newaxis], stagger[np.newaxis, :])\n\n    def gibbs_update(self, beta, sublattice):\n        fields = np.roll(self.spins, 1, 0) + np.roll(self.spins, -1, 0) + np.roll(self.spins, 1, 1) + np.roll(self.spins, -1, 1)\n        delta_E = 2 * fields\n        spin_up_probabilities = 1 / (1 + np.exp(- beta * delta_E))\n        new_spins = 2 * (np.random.rand(self.L, self.L) < spin_up_probabilities) - 1\n        self.spins = np.choose(np.logical_xor(sublattice, self.mask), [self.spins, new_spins])\n\n    def glauber_update(self, beta):\n        x, y = np.random.randint(self.L, size=2)\n        fields = 0\n        for neighbour in [((x + 1) % self.L, y), ((x - 1) % self.L, y), (x, (y + 1) % self.L), (x, (y - 1) % self.L)]:\n            fields += self.spins[neighbour]\n        delta_E = 2 * fields\n        spin_up_probability = 1 / (1 + np.exp(- beta * delta_E))        \n        if np.random.rand() < spin_up_probability:\n            self.spins[x, y] = 1\n        else:\n            self.spins[x, y] = -1\n\n    def wolff_update(self, beta):\n        initial_x, initial_y = np.random.randint(self.L, size=2)\n        initial_spin = self.spins[initial_x, initial_y]\n        cluster = deque([(initial_x, initial_y)])\n        add_prob = 1 - np.exp(-2 * beta)\n\n        while len(cluster) != 0:\n            x, y = cluster.popleft()\n            if self.spins[x, y] == initial_spin:\n                self.spins[x, y] *= -1\n                for neighbour in (((x + 1) % self.L, y), ((x - 1) % self.L, y), (x, (y + 1) % self.L), (x, (y - 1) % self.L)):\n                    if self.spins[neighbour] == initial_spin:\n                        if np.random.rand() < add_prob:\n                            cluster.append(neighbour)\n\n\n\n\n\n\n\n\n\n\nFigure 1: Glauber dynamics, Block Gibbs sampling and Wolff updates compared. Change the temperature using the slider. The centre of the slider corresponds to the critical temperature \\(k_\\text{B}T = 2|J|/\\log(1+\\sqrt{2})\\sim 2.269|J|\\)."
  },
  {
    "objectID": "notes/numbers.html",
    "href": "notes/numbers.html",
    "title": "Floating point and all that",
    "section": "",
    "text": "Since physics is all about numbers we had better develop some understanding of how computers represent them, and the limitations of this representation. Hopefully this example is sufficiently motivating:\nAh…"
  },
  {
    "objectID": "notes/numbers.html#sec-fp-numpy",
    "href": "notes/numbers.html#sec-fp-numpy",
    "title": "Floating point and all that",
    "section": "2.1 Floating point numbers in NumPy",
    "text": "2.1 Floating point numbers in NumPy\nIf this all a bit theoretical you can just get NumPy’s finfo function to tell all about the machine precision\n\nnp.finfo(np.float64)\n\nfinfo(resolution=1e-15, min=-1.7976931348623157e+308, max=1.7976931348623157e+308, dtype=float64)\n\n\nNote that \\(2^{-52}=2.22\\times 10^{-16}\\) which accounts for the value \\(10^{-15}\\) of the resolution. This can be checked by finding when a number is close enough to treated as 1.0.\n\nx=1.0\nwhile 1.0 + x != 1.0:\n    x /= 1.01 \nprint(x)\n\n1.099427563084686e-16\n\n\nFor binary32 we have a resolution of \\(10^{-6}\\).\n\nnp.finfo(np.float32)\n\nfinfo(resolution=1e-06, min=-3.4028235e+38, max=3.4028235e+38, dtype=float32)\n\n\nOne lesson from this is that taking small differences between numbers is a potential source of rounding error, as in this somewhat mean exam question\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSolution: \\(x-x'=x(1-\\gamma^{-1})\\sim x\\beta^2/2\\sim 4.2\\text{mm}\\).\n\nimport numpy as np\nfrom scipy.constants import c\nbeta = 384400e3 / (76 * 3600) / c\ngamma = 1/np.sqrt(1 - beta**2)\nprint(1 - np.float32(1/gamma), 1 - np.float64(1/gamma))\n\n0.0 1.0981660025777273e-11"
  },
  {
    "objectID": "notes/numbers.html#the-dreaded-nan",
    "href": "notes/numbers.html#the-dreaded-nan",
    "title": "Floating point and all that",
    "section": "2.2 The dreaded NaN",
    "text": "2.2 The dreaded NaN\nAs well as a floating point system, IEEE 754 defines Infinity and NaN (Not a Number)\n\nnp.array([1, -1, 0]) / 0\n\n/var/folders/xs/y8sn45v943s2_62flnxw0p940000gn/T/ipykernel_38352/2604490398.py:1: RuntimeWarning:\n\ndivide by zero encountered in true_divide\n\n/var/folders/xs/y8sn45v943s2_62flnxw0p940000gn/T/ipykernel_38352/2604490398.py:1: RuntimeWarning:\n\ninvalid value encountered in true_divide\n\n\n\narray([ inf, -inf,  nan])\n\n\nThey behave as you might guess\n\n2 * np.inf, 0 * np.inf, np.inf > np.nan\n\n(inf, nan, False)\n\n\nNaNs propagate through subsequent operations\n\n2 * np.nan\n\nnan\n\n\nwhich means that if you get a NaN somewhere in your calculation, you’ll probably end up seeing it somewhere in the output (which is the idea)."
  },
  {
    "objectID": "notes/intro.html",
    "href": "notes/intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Science is what we understand well enough to explain to a computer. Art is everything else we do.\nDonald Knuth\n\nComputation saturates every corner of physics these days, much as it saturates every corner of everything. Even if we restrict ourselves to the senses most relevant to physicists, the word computation covers a terrific variety of ideas. From the prosaic to the lofty, I could be talking about:\n\nThe tools we use to do computation. Physical hardware, editors, notebooks, etc.\nThe languages we use to write code in order to perform computations.\nThe use of tools to generate documents (e.g. using \\(\\LaTeX\\)) or disseminate knowledge (online).\nThe automated gathering and analysis of experimental data.\nThe numerical techniques that we use to solve particular problems in theoretical physics and mathematics.\nThe limits of what we can achieve with finite resources including time and space (memory). That is, how hard — or complex — are the computational tasks we wish to perform? Can we quantify this?\nThe question of whether physical processes are really the same things as computations. That is: are all processes that happen in the physical universe computable in principal (perhaps on a quantum computer)? This is roughly what is meant by (physical) Church–Turing thesis. This brings us full circle to the first item on the list.\n\nIn this course we’ll have to touch on all of these, except the last one (it’s only eight lectures). Most of the concrete techniques we’ll look at will come from computational physics (i.e. mathematical modelling of physical processes), rather than data analysis, but that’s mostly because of my background.\nFor theoretical physics, computation is used to deal with the awkward fact that physical theories are generally not tractable. You can’t solve Maxwell’s equations, the Navier–Stokes equation, or Schrödinger’s equation in any but the simplest situations. To be blunt, this means that your knowledge of physics, while very nice, is not all that useful unless you can write a program to solve more complicated problems. Sorry.\nOn the plus side — as the above quote from Donald Knuth suggests — thinking about how to put a piece of physics you think you know into functioning code is a fantastic way to deepen your understanding of the physics itself. Every symbol and every operation has to mean and do exactly what it should for you to succeed.\nIt’s important to understand that this need to apply our mathematical descriptions of nature in more general settings was the principal driving force behind the invention of the computer in the first place. If you’d like to learn more about the early history of electronic computers I’d recommend Dyson (2012).\nIf you’d like to get into the theory of computation more deeply, I can’t recommend Moore and Mertens (2011) highly enough.\n\n\n\n\nReferences\n\nDyson, George. 2012. Turing’s Cathedral: The Origins of the Digital Universe. Vintage.\n\n\nMoore, Cristopher, and Stephan Mertens. 2011. The Nature of Computation. Oxford University Press."
  },
  {
    "objectID": "notes/complexity.html",
    "href": "notes/complexity.html",
    "title": "Algorithms and computational complexity",
    "section": "",
    "text": "How hard is it to multiply numbers? The bigger they are, the harder it is, as you well know. You also know that computers are very good at multiplying, so once you’ve switched from multiplying numbers yourself to multiplying them on a computer, you may well be tempted to forget about how hard it is. Nevertheless, computers find big numbers harder than small numbers. How much harder?\nIf you remember how you learnt to multiply numbers at school, it probably went something like this: 1\n\nMultiplying two 3 digit numbers\n\n\n\n\n\n\n\n\n\n\n×\n1\n3\n2\n2\n3\n1\n\n\n\n\n_\n_\n3\n_\n2\n6\n1\n4\n9\n2\n6\n3\n\n\n3\n9\n4\n8\n3\n\n\n\nFor \\(n\\) digits we have to perform \\(n^2\\) single digit multiplications. We then have to add together the \\(n\\) resulting \\(n\\)-digit numbers. This is another \\(n^2\\) operations. Thus the overall number of operations is proportional to \\(n^2\\): doubling the number of digits will make the problem four times harder.\nExactly how long this takes to perform in your head or on a computer will depend on many things, such as how long it takes you to multiply two digits, or get the previous values out of memory (or read them of the page), but you can’t get away from the basic quadratic scaling law of this algorithm."
  },
  {
    "objectID": "notes/complexity.html#best-worst-average",
    "href": "notes/complexity.html#best-worst-average",
    "title": "Algorithms and computational complexity",
    "section": "2.1 Best / worst / average",
    "text": "2.1 Best / worst / average\nEven when we focus on a problem in the above sense we still have to be careful in defining the complexity of an algorithm. In general we can characterize three complexities: best case, worse case, and average case. To see the difference between these three consider search, the very simple problem of finding an item in an (unordered) list of length \\(n\\). How hard is this? You have to check every item until you find the one you are looking for, so this suggests the complexity is \\(O(n)\\). You could be lucky and get it first try, however, or within the first ten tries. This means the best case complexity of search is \\(O(1)\\): it doesn’t increase with the size of the problem. The worst thing that could happen is that the sought item is last: the worst case complexity is \\(O(n)\\). On average, you’ll find your item in the middle of the list on attempt \\(\\sim n/2\\), so the average case complexity is \\(O(n/2)\\). But this is the same as \\(O(n)\\) (constants don’t matter)\nThus for linear search we have:\n\n\n\n\nComplexity\n\n\n\n\nBest case\n\\(O(1)\\)\n\n\nWorst case\n\\(O(n)\\)\n\n\nAverage case\n\\(O(n)\\)\n\n\n\nWe can check the average case performance experimentally by using randomly chosen lists: 2\n\ndef linear_search(x, val):\n    \"Return True if val is in x, otherwise return False\"\n    for item in x:\n        if item == val:\n            return True\n    return False\n\n\nimport numpy as np\n# Create array of problem sizes n we want to test (powers of 2)\nN = 2**np.arange(2, 20)\n\n# Generate the array of integers for the largest problem to use in plotting times\nx = np.arange(N[-1])\n\n# Initialise an empty array to stores times for plotting\ntimes = []\n\n# Time the search for each problem size\nfor n in N:\n\n    # Time search function (repeating 3 times) to find a random integer in x[:n]\n    t = %timeit -q -n4 -r1 -o linear_search(x[:n], np.random.randint(0, n))\n\n    # Store best case time (best on a randomly chosen problem)\n    times.append(t.best)\n\n\n\nCode for plot\nimport matplotlib.pyplot as plt\n# Plot and label the time taken for linear search\nplt.loglog(N, times, marker='o')\nplt.xlabel('$n$')\nplt.ylabel('$t$ (s)')\n\n# Show a reference line of O(n)\nplt.loglog(N, 1e-6*N, label='$O(n)$')\n\n# Add legend\nplt.legend(loc=0)\nplt.title(\"Experimental complexity of linear search\")\n\nplt.show()\n\n\n\n\n\nThe “experimental noise” in these plots arises because we don’t have full control over exactly what our computer is doing at any moment: there are lots of other processes running. Also, it takes a while to reach the linear regime: there is an overhead associated with starting the program that represents a smaller fraction of the overall run time as \\(n\\) increases."
  },
  {
    "objectID": "notes/complexity.html#polynomial-complexity",
    "href": "notes/complexity.html#polynomial-complexity",
    "title": "Algorithms and computational complexity",
    "section": "2.2 Polynomial complexity",
    "text": "2.2 Polynomial complexity\nSince you’ve already learnt a lot of algorithms in mathematics (even if you don’t think of them this way) it’s very instructive to revisit them through the lens of computational complexity.\nMultiplying a \\(n\\)-dimensional vector by a \\(n\\times n\\) matrix?\n\\[\n\\begin{align}\n\\sum_{j=1}^n M_{ij}v_j\n\\end{align}\n\\]\nThe sum contains \\(n\\) terms, and we have to perform \\(n\\) such sums. Thus the complexity of this operation is \\(O(n^2)\\).\nLikewise, multiplying two \\(n\\times n\\) matrices\n\\[\n\\sum_{j} A_{ij}B_{jk}\n\\]\ninvolves \\(n\\) terms for each of the \\(n^2\\) assignments of \\(i\\) and \\(k\\). Complexity: \\(O(n^3)\\)\nThus, if you have to calculate something like \\(M_1 M_2\\cdots M_n \\mathbf{v}\\), you should not calculate the matrix product first, but instead do it like this\n\\[\nM_1\\left(M_2\\cdots \\left(M_n \\mathbf{v}\\right)\\right)\n\\]\nWikipedia has a nice summary of computational complexity of common mathematical operations.\nIf an algorithm has complexity \\(O(n^p)\\) for some \\(p\\) it is generally described as having polynomial complexity. A useful heuristic (which is not 100% reliable) is that if you have \\(p\\) nested loops that range over \\(\\sim n\\), the complexity is \\(O(n^p)\\) (think how you would implement matrix-vector and matrix-matrix multiplication)."
  },
  {
    "objectID": "notes/complexity.html#better-than-linear",
    "href": "notes/complexity.html#better-than-linear",
    "title": "Algorithms and computational complexity",
    "section": "2.3 Better than linear?",
    "text": "2.3 Better than linear?\nIt seems obvious that for search you can’t do better than linear: you have to look at roughly half the items before you should expect to find the one you’re looking for 3. What if the list is ordered? Any order will do: numerical for numbers, or lexicographic for strings. This extra structure allows us to use an algorithm called binary search that you may have seen before 4. The idea is pretty intuitive: look in the middle of the list and see if the item you seek should be in the top half or bottom half. Take the relevant half and divide it in half again to determine which quarter of the list your item is in, and so on. Here’s how it looks in code:\n\ndef binary_search(x, val):\n    \"\"\"Peform binary search on x to find val. If found returns position, otherwise returns None.\"\"\"\n\n    # Intialise end point indices\n    lower, upper = 0, len(x) - 1\n\n    # If values is outside of interval, return None \n    if val < x[lower] or val > x[upper]:\n        return None\n\n    # Perform binary search\n    while True:\n                \n        # Compute midpoint index (integer division)\n        midpoint = (upper + lower)//2\n\n        # Check which side of x[midpoint] val lies, and update midpoint accordingly\n        if val < x[midpoint]:\n            upper = midpoint - 1\n        elif val > x[midpoint]:\n            lower = midpoint + 1\n        elif val == x[midpoint]:  # found, so return\n            return midpoint\n       \n        # In this case val is not in list (return None)\n        if upper < lower:\n            return None\n\nAnd here’s the performance\n\n\nCode for plot\n# Create array of problem sizes we want to test (powers of 2)\nN = 2**np.arange(2, 24)\n\n# Creat array and sort\nx = np.arange(N[-1])\nx = np.sort(x)\n\n# Initlise an empty array to capture time taken\ntimes = []\n\n# Time search for different problem sizes\nfor n in N:\n    # Time search function for finding '2'\n    t = %timeit -q -n5 -r2 -o binary_search(x[:n], 2)\n\n    # Store average\n    times.append(t.best)\n\n# Plot and label the time taken for binary search\nplt.semilogx(N, times, marker='o')\nplt.xlabel('$n$')\nplt.ylabel('$t$ (s)')\n\n# Change format on y-axis to scientific notation\nplt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\nplt.title(\"Experimental complexity of binary search\")\nplt.show()\n\n\n\n\n\nNote the axes: the plot is linear-log, so the straight line indicates logarithmic growth of complexity. This makes sense: if the length is a power of 2 i.e. \\(n=2^p\\), we are going to need \\(p\\) bisections to locate our value. The complexity is \\(O(\\log n)\\) (we don’t need to specify the base as overall constants don’t matter).\nHere’s another example of logarithm scaling. Exponentiation is the problem of raising a number \\(b\\) (the base) to the \\(n\\)th power. The obvious way is to multiply the number by itself \\(n\\) times. Linear scaling, right? But there’s a quicker way, since\n\\[\n\\begin{align}\nb^2 &= b\\cdot b\\\\\nb^4 &= b^2\\cdot b^2\\\\\nb^4 &= b^4\\cdot b^4\n\\end{align}\n\\]\nWe only have to do three multiplications! Exponentiation by this method (called exponentiation by squaring) is \\(O(\\log n)\\). To handle powers that aren’t a power of \\(2\\), we do the following\n\\[\nb^n = \\begin{cases}\n    b^{n/2} \\cdot b^{n/2} & \\text{if $n$ even} \\\\\n    b \\cdot b^{n-1} & \\text{if $n$ odd}\n\\end{cases}\n\\]\nHere’s a way to implement this in code.\n\ndef exp(b, n):\n    if n == 0:\n        return 1\n    elif n % 2 == 0:\n        return exp(b, n // 2)**2\n    else:\n        return b * exp(b, n - 1) \n\nexp(2, 6)\n\n64\n\n\nThis implementation is recursive: the function exp(b, n) calls itself. If this seems a bit self-referential, notice that it only calls itself with lower values of the exponent \\(n\\). This process continues until we hit \\(n=0\\), and 1 is returned by the first part of the if ... else. Any recursive function has to have such a base case to avoid an infinite regress. You’ll know if you haven’t provided one correctly:\n\ndef exp_no_base_case(b, n):\n    if n % 2 == 0:\n        return exp_no_base_case(b, n // 2)**2\n    else:\n        return b * exp_no_base_case(b, n - 1) \n\nexp_no_base_case(2, 6)\n\nRecursionError: maximum recursion depth exceeded in comparison\n\n\nOne interesting thing about exponentiation is that while it can be done efficiently, the inverse — finding the logarithm — cannot. To make this more precise one has to work with modular arithmetic e.g. do all operations modulo some prime number \\(p\\). Then for \\(b, y=0,\\ldots p-1\\) we are guaranteed that there is some number \\(x\\) such that \\(b^x=y\\) (this is called the discrete logarithm). Finding this number is hard: there is no known method for computing it efficiently. Certain public-key cryptosystems are based on the difficulty of the discrete log (for carefully chosen \\(b\\), \\(p\\) and \\(y\\))."
  },
  {
    "objectID": "notes/complexity.html#exponential-complexity",
    "href": "notes/complexity.html#exponential-complexity",
    "title": "Algorithms and computational complexity",
    "section": "2.4 Exponential complexity",
    "text": "2.4 Exponential complexity\nWhile we’re on the subject of recursion, here’s an example that’s often used to introduce the topic: calculating the Fibonacci numbers. Remember that the Fibonacci numbers are this sequence\n\\[\n0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233 ...\n\\]\nwhere each new term is obtained by adding together the previous two\n\\[\n\\text{Fib}(n) = \\text{Fib}(n-1) + \\text{Fib}(n-2)\n\\]\nThe fact that the value \\(\\text{Fib}(n)\\) is defined in terms of lower values of \\(n\\) makes a recursive definition possible\n\ndef fib(n):\n    if n == 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fib(n - 1) + fib(n - 2)\n\nfib(13)\n\n233\n\n\nThe first two terms are the base cases (we need two because the recursion refers to two earlier values). While this looks quite cool it’s actually a terrible way of calculating \\(\\text{Fib}(n)\\). Look at the picture below which illustrates the function calls that are made during the evaluation of \\(\\text{Fib}(5)\\). There are huge amounts of duplication!\n\n\n\nThe recursive tree for calculating Fibonacci numbers. Source: SICP\n\n\nThe complexity of this algorithm actually grows exponentially with \\(n\\). Because of the branching structure the algorithm is \\(O(2^n)\\). Calculating the Fibonacci number the sensible way (i.e. the way you do it in your head) gives an \\(O(n)\\) algorithm. Formalizing this approach gives rise to the matrix recurrence relation:\n\\[\n\\begin{pmatrix}\n\\text{Fib}(n+2) \\\\\n\\text{Fib}(n+1)\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n\\text{Fib}(n+1) \\\\\n\\text{Fib}(n)\n\\end{pmatrix}.\n\\]\nThis suggests an even better way of calculating the Fibonacci numbers, since\n\\[\n\\begin{pmatrix}\n\\text{Fib}(n+1) \\\\\n\\text{Fib}(n)\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & 0\n\\end{pmatrix}^n\n\\begin{pmatrix}\n1 \\\\\n0\n\\end{pmatrix},\n\\]\nfinding the \\((n+1)\\)th number involves the \\(n\\)th power of a matrix. But now we could use exponentiation by squaring, yielding an \\(O(\\log n)\\) algorithm!\nIt would be nice if exponential complexity were only ever the result of poor choices of algorithm. Unfortunately, this is not the case. It’s possible to come up with problems that definitely can’t be solved faster than exponentially: the Towers of Hanoi is one famous example. Closer to the hearts of physicists, the simulation of a quantum system with \\(n\\) qubits (a qubit — or quantum bit — is just a computer sciencey word for a spin-1/2) is believed to have complexity \\(O(2^n)\\), which is a big part of the hype surrounding quantum computers.\nThere are problems whose solution, once found, is easy to check. The discrete logarithm we mentioned above is one example. Checking involves exponentiation, and exponentiation is \\(O(\\log n)\\) in the size of the numbers, or \\(O(n)\\) in the number of digits. The question of whether efficient (i.e. polynomial) algorithms always exist for problems which are easy to check is in fact the outstanding problem in computer science: it’s called P vs NP, where P is the class of problems with polynomial time algorithms and NP is the class whose solution can be checked in polynomial time. The question is: are these two classes the same or do they differ? That is, are there problems in NP that aren’t in P? I think it’s fair to say that most people with an opinion on the matter think so, but the proof is lacking (and worth a million dollars)."
  },
  {
    "objectID": "notes/complexity.html#bubble-sort",
    "href": "notes/complexity.html#bubble-sort",
    "title": "Algorithms and computational complexity",
    "section": "3.1 Bubble sort",
    "text": "3.1 Bubble sort\nThis one is very simple: repeatedly pass through the array, comparing neighbouring pairs of elements and switching them if they are out of order. After the first pass the largest element is in the rightmost position (largest index). That means the second pass can finish before reaching the last element, as we know that it is already in place. After the second pass the final two elements are correctly ordered. Continue in this way, stopping one place earlier each time, until the array is sorted.\nHere’s a simple implementation: 5\n\ndef bubble_sort(A):\n    \"Sort A and return\"\n    A = A.copy()\n    n = len(A)\n    while n > 0:\n        for i in range(n - 1):\n            # Swap data if in wrong order\n            if A[i] > A[i + 1]:\n                A[i + 1], A[i] = A[i], A[i + 1]\n        n = n - 1\n\n    return A\n\nWhat is the complexity of this algorithm? From the code we can see immediately that there are two nested loops: one to implement each pass and one to loop over the \\(n-1\\) passes. This suggests that the complexity is quadratic i.e. \\(O(n^2)\\). A numerical check verifies this:\n\n\nCode for plot\n# Create array of problem sizes we want to test (powers of 2)\nN = 2**np.arange(2, 10)\n\n# Create an array of random numbers\nx = np.random.rand(N[-1])\n\n# Time bubble sort on arrays of different lengths  \ntimes = []\nfor n in N:\n    t = %timeit -q -n2 -r2 -o bubble_sort(x[:n])\n    times.append(t.best)\n\n# Plot bubble sort timing\nplt.loglog(N, times, marker='o', label='bubble sort')\n\n# Show reference line of O(n^2)\nplt.loglog(N, 1e-6*N**2, label='$O(n^2)$')\n\n# Add labels and legend\nplt.xlabel('$n$')\nplt.ylabel('$t$ (s)')\nplt.legend(loc=0)\n\nplt.show()\n\n\n\n\n\nIf you watch the animation of bubble sort you might get a bit bored, as it slowly carries the next largest element to the end. Can we do better?"
  },
  {
    "objectID": "notes/complexity.html#quicksort",
    "href": "notes/complexity.html#quicksort",
    "title": "Algorithms and computational complexity",
    "section": "3.2 Quicksort",
    "text": "3.2 Quicksort\nYes! It turns out that there are number of algorithms that do considerably better than quadratic. How fast could a sorting algorithm be? It’s clear that it can’t be faster than \\(O(n)\\): at the very least one has to look at each element. While one can’t actually achieve linear scaling, there are many algorithms which achieve the next best thing: \\(O(n\\log n)\\).\nQuicksort is one such algorithm. It uses two key ideas:\n\nIt is possible in \\(O(n)\\) steps to partition an array into those elements larger (or equal) and those elements smaller than a given value (called the pivot).\nActing recursively on each partition requires only \\(O(\\log n)\\) partitions to completely sort the array.\n\nHere’s an implementation. See this discussion of the partitioning scheme for more information.\n\ndef quicksort(A, lo=0, hi=None):\n    \"Sort A and return sorted array\"\n\n    # Initialise data the first time function is called    \n    if hi is None:\n        hi = len(A) - 1\n        A = A.copy()\n\n    # Sort    \n    if lo < hi:\n        p = partition(A, lo,  hi)\n        quicksort(A, lo, p - 1)\n        quicksort(A, p + 1, hi)\n    return A\n\n\ndef partition(A, lo, hi):\n    \"Partitioning function for use in quicksort\"\n    pivot = A[hi]\n    i = lo\n    for j in range(lo,  hi):\n        if A[j] <= pivot:\n            A[i], A[j] = A[j], A[i]\n            i += 1\n    A[i], A[hi] = A[hi], A[i]\n    return i\n\nAnd here’s a demonstration of the \\(O(n\\log n)\\) performance\n\n\nCode for plot\n# Create array of problem sizes we want to test (powers of 2)\nN = 2**np.arange(2, 14)\n\n# Create an array of random numbers\nx = np.random.rand(N[-1])\n\n# Time quicksort on arrays of different lengths\ntimes = []\nfor n in N:\n    t = %timeit -n1 -r1 -o -q quicksort(x[:n])\n    times.append(t.best)\n\n# Plot quicksort timings\nplt.loglog(N, times, marker='o', label='quicksort')\n\n# Show reference line of O(n*log(n))\nplt.loglog(N, 1e-6*N*np.log(N), label='$O(n\\log\\, n)$')\n\n# Add labels\nplt.xlabel('$n$')\nplt.ylabel('$t$ (s)')\nplt.legend(loc=0)\n\nplt.show()\n\n\n\n\n\nQuicksort provides an interesting example of the differences between best, worst and average case complexities. They are:\n\nBest case: \\(O(n\\log n)\\)\nWorst case: \\(O(n^2)\\)\nAverage case: \\(O(n\\log n)\\)\n\nFunnily enough, the worst case occurs when the array is already sorted. Because the pivot is chosen as the last element of the array, one of the partitions is always empty. Thus, instead of the problem being cut roughly in half at each stage, it is merely reduced in size by 1.\nNumPy’s sort uses quicksort, whereas Python’s sorted uses a hybrid algorithm called Timsort, which also has \\(O(n\\log n)\\) average case performance.\n\n\nCode for plot\n# Create array of problem sizes we want to test (powers of 2)\nN = 2**np.arange(2, 14)\n \n# Create an array of random numbers, and make read-only so we don't accidentally sort it    \nx = np.random.rand(N[-1])\nx.flags.writeable = False\n\n# Time the different implementations\npy_times = []\nnp_times = []\nfor n in N:\n    # Time Python built-in sort\n    t = %timeit -n3 -q -o sorted(x[:n])\n    py_times.append(t.best)\n\n    # Time NumPy sort\n    t = %timeit -n3 -q -o np.sort(x[:n], kind='quicksort')\n    np_times.append(t.best)\n\n\n# Plot time taken for built-in sort\nplt.loglog(N, py_times, marker='o', label='Python (timsort)')\nplt.loglog(N, np_times, marker='o', label='NumPy (quicksort)')\n\n# Show reference lines of O(n*log(n)) and  O(n^2)\nplt.loglog(N, 1e-6*N*np.log(N), '--', label=r'$O(n\\log n)$')\nplt.loglog(N, 1e-6*N**2, '--', label=r'$O(n^2$)')\n\n# Show legend\nplt.legend(loc=0);\n\n# Add label and legend\nplt.xlabel('$n$')\nplt.ylabel('$t$ (s)')\n\nplt.show()"
  },
  {
    "objectID": "notes/autodiff.html",
    "href": "notes/autodiff.html",
    "title": "Part II Computational Physics",
    "section": "",
    "text": "Intro to backpropagation"
  },
  {
    "objectID": "notes/divide.html",
    "href": "notes/divide.html",
    "title": "Part II Computational Physics",
    "section": "",
    "text": "# Divide and Conquer\nFFT. Use split step as illustration Matrix multiplication"
  },
  {
    "objectID": "notes/linear.html",
    "href": "notes/linear.html",
    "title": "Part II Computational Physics",
    "section": "",
    "text": "1 Linear algebra\nhttps://twitter.com/nicholdav/status/1621180520901001220\nKrylov subspaces\nSVD and quantum mechanics. Quantum entanglement.\nImage compression using SVD\nhttp://timbaumann.info/svd-image-compression-demo/\nTrebst has nice Einstein example\nPCA for big data\nhttps://www.tensors.net/exact-diagonalization\nPower method\nPagerank\nhttps://www.programcreek.com/python/?code=MKLab-ITI%2Freveal-graph-embedding%2Freveal-graph-embedding-master%2Freveal_graph_embedding%2Fembedding%2Fimplicit.py\nhttp://pi.math.cornell.edu/~web6140/TopTenAlgorithms/PageRank.html\nhttp://infolab.stanford.edu/~ullman/mmds/ch5.pdf"
  },
  {
    "objectID": "notes/fourier.html",
    "href": "notes/fourier.html",
    "title": "Fast Fourier transform",
    "section": "",
    "text": "You’ve met Fourier in the context of Fourier series, where a periodic function is represented as an infinite series, and Fourier transforms, where a non-periodic function is represented as an integral. Both representations are infinite, involving all the points on a real interval or an infinite number of terms in a series, so not really suitable for representation on a computer. For that we need a third thing: the discrete Fourier transform (DFT). After defining it, we’ll see there is a divide and conquer type algorithm for calculating it efficiently called the fast Fourier transform (FFT), which opened up an enormous number of applications across science and engineering."
  },
  {
    "objectID": "notes/fourier.html#ntoinfty-limit",
    "href": "notes/fourier.html#ntoinfty-limit",
    "title": "Fast Fourier transform",
    "section": "1.1 \\(N\\to\\infty\\) limit",
    "text": "1.1 \\(N\\to\\infty\\) limit\nIn this limit the \\(\\eta_n\\) values become dense in the range \\((-\\pi,\\pi]\\), with separation \\(\\Delta \\eta = 2\\pi/N\\), and we replace the sum in the inverse DFT Equation 3 by an integral according to the prescription \\[\n\\sum_{n=0}^{N-1} \\left(\\cdots\\right) \\xrightarrow{N\\to\\infty} N \\int_{0}^{2\\pi} \\frac{d\\eta}{2\\pi}\\left(\\cdots\\right),\n\\] giving \\[\nf_j = \\int_{0}^{2\\pi} \\frac{d\\eta}{2\\pi}\\,F(\\eta) e^{i\\eta j}.\n\\]"
  },
  {
    "objectID": "notes/fourier.html#ntoinfty-with-f_j-fjln",
    "href": "notes/fourier.html#ntoinfty-with-f_j-fjln",
    "title": "Fast Fourier transform",
    "section": "1.2 \\(N\\to\\infty\\) with \\(f_j = f(jL/N)\\)",
    "text": "1.2 \\(N\\to\\infty\\) with \\(f_j = f(jL/N)\\)\nAlternatively, regard the \\(N\\to\\infty\\) limit as sampling a function \\(f(x)\\) ever more finely in the range (0,L]. Now it’s the DFT, rather than the inverse, that becomes an integral \\[\n\\hat f(k) \\equiv \\int_0^L f(x) e^{-ik_n x}\\,dx,\n\\] where \\(k_n =2\\pi n/L\\). Note that \\(k_n x = \\eta_n j\\). The pair of transformations is now \\[\n\\begin{align}\n\\hat f_k &= \\int_0^L f(x) e^{-ik_n x}\\,dx\\nonumber\\\\\nf(x) &= \\frac{1}{L}\\sum_k \\hat f_k e^{ik_n x}\n\\end{align}\n\\tag{4}\\] This is the conventional form of the Fourier series for a function with period \\(L\\).\nWith this definition \\(\\hat f_k\\) has an extra dimension of distance (on account of the integral), which gets removed by the \\(1/L\\) in the inverse transform.\nThe analog of the identity Equation 2 is \\[\n\\frac{1}{L}\\sum_k e^{ik x} = \\delta_L(x),\n\\] where \\(\\delta_L(x)\\) is an \\(L\\)-periodic version of the \\(\\delta\\)-function."
  },
  {
    "objectID": "notes/fourier.html#ltoinfty",
    "href": "notes/fourier.html#ltoinfty",
    "title": "Fast Fourier transform",
    "section": "1.3 \\(L\\to\\infty\\)",
    "text": "1.3 \\(L\\to\\infty\\)\nFinally we arrive at the Fourier transform, where we take \\(L\\to\\infty\\), so that the inverse transform in Equation 4 becomes an integral too \\[\n\\begin{align}\n\\hat{f}(k) & = \\int_{-\\infty}^\\infty f(x) e^{-ik_n x}\\,dx\\nonumber\\\\\nf(x) &= \\int_{-\\infty}^\\infty \\hat f(k) e^{ik_n x}\\,\\frac{dk}{2\\pi}.\n\\label{coll_FTTrans}\n\\end{align}\n\\]"
  },
  {
    "objectID": "notes/fourier.html#sec-properties",
    "href": "notes/fourier.html#sec-properties",
    "title": "Fast Fourier transform",
    "section": "1.4 Some important properties",
    "text": "1.4 Some important properties\nHere are some properties that hold for all of the above.\n\nIf \\(f_j\\) is real then \\(F_n = \\left[F_{-n}\\right]^*\\).\nIf \\(f_j\\) is even (odd) 2 , \\(F_n\\) is even (odd).\n(Ergo) if \\(f_j\\) is real and even, so is \\(F_n\\)."
  },
  {
    "objectID": "notes/fourier.html#higher-dimensions",
    "href": "notes/fourier.html#higher-dimensions",
    "title": "Fast Fourier transform",
    "section": "1.5 Higher dimensions",
    "text": "1.5 Higher dimensions\nThe DFT generalizes to higher dimensions straightforwardly. Suppose we have data living in \\(d\\) dimensions with \\(N_i\\) datapoints along dimension \\(i=1,\\dots d\\), then the DFT is \\[\nF_{\\mathbf{n}} = \\sum_{\\mathbf{n}} f_\\mathbf{j}e^{-i \\boldsymbol{\\eta}_\\mathbf{n}\\cdot \\mathbf{j}},\n\\] where \\(\\mathbf{j}=(j_1,\\ldots j_{d})\\) with \\(j_i = 0,\\ldots N_i - 1\\) and likewise \\(\\boldsymbol{\\eta}_\\mathbf{n} = 2\\pi (n_1 / N_1, \\ldots n_d/ N_d)\\) \\(n_i = 0,\\ldots N_i - 1\\)"
  },
  {
    "objectID": "notes/fourier.html#complexity",
    "href": "notes/fourier.html#complexity",
    "title": "Fast Fourier transform",
    "section": "2.1 Complexity",
    "text": "2.1 Complexity\nIt should be clear that the FFT is going to beat the naive approach, but let’s analyze the complexity more carefully. If \\(T(N)\\) is time (or number of steps) required to compute the DFT for size \\(N\\) inputs, then calculating \\(F^\\text{e}_{n'}\\) and \\(F^\\text{0}_{n'}\\) takes time \\(2T(N/2)\\). Using Equation 5 to evaluate \\(F_n\\) is a further \\(N\\) steps, so we have 3\n\\[\nT(N) = 2T(N/2) +\\alpha N\n\\]\nfor some \\(\\alpha\\). This implies \\(T(N)=\\Theta(N\\log N)\\).\nWhat happens when \\(N\\) isn’t a power of 2? You can still use the divide and conquer strategy for any other factor \\(p\\) of \\(N\\). If the largest prime factor of \\(N\\) is bounded i.e. doesn’t grow with \\(N\\) 4, then this still yields \\(T(N)=\\Theta(N\\log N)\\). If \\(N\\) is prime you have to use something else.\nAs a practical matter it’s probably best to try and ensure that \\(N\\) is a power of two e.g. by choosing the size of your simulation appropriately or padding your data with zeros until its length is a power of 2."
  },
  {
    "objectID": "notes/fourier.html#history",
    "href": "notes/fourier.html#history",
    "title": "Fast Fourier transform",
    "section": "2.2 History",
    "text": "2.2 History\nThe modern invention of the FFT is credited to Cooley and Tukey (1965). Certainly they were the first to discuss its complexity. The divide and conquer approach was however anticipated by Danielson and Lanczos (1942), who had applications in crystallography in mind, and by unpublished work of Carl Friedrich Gauss in 1805 (predating even Fourier) in his astronomical studies. See Cooley, Lewis, and Welch (1967) for more on the historical background."
  },
  {
    "objectID": "notes/fourier.html#fft-in-python",
    "href": "notes/fourier.html#fft-in-python",
    "title": "Fast Fourier transform",
    "section": "2.3 FFT in Python",
    "text": "2.3 FFT in Python\nAs usual, you don’t have to go implementing this yourself. The FFT is available in both NumPy (in the numpy.fft module) and SciPy (in scipy.fft), the latter with a more comprehensive set of functions.\nEquation 1 and Equation 3 are the default definitions used in these modules, though you should always check the conventions used in any library implementation. The NumPy documentation provides a careful discussion of the positive and negative frequency components and there are several helper functions available to make your life easier, such as:\n\nnp.fft.fftfreq(n, d), which returns the frequencies (not the angular frequencies) for input size \\(n\\) and sample spacing \\(d\\).\nnp.fft.fftshift(A) shifts data so that the zero frequency is in the centre.\nnp.fft.ifftshift(A) inverts this.\n\nHere are some simple examples of their use, applied to a very simple signal consisting of two sinusoids at 12 Hz and 34 Hz:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\ndt=0.01\nfftsize=256\nt=np.arange(fftsize)*dt\n#Generate some fake data at 12 Hz and 34 Hz\ny=np.cos(2*np.pi*12*t)+0.5*np.sin(2*np.pi*34*t)\nplt.plot(t,y)\nplt.xlabel(\"Time\")\n\nText(0.5, 0, 'Time')\n\n\n\n\n\nNow we can take the FFT and plot vs array index, or against the real frequency (with the given sample spacing of \\(dt=0.01\\))\n\nY=np.fft.fft(y)\n# Plot FFT modulus versus array index\nplt.subplot(2,1,1); plt.plot(abs(Y))\n# Now use the correct frequency coordinates\nf=np.fft.fftfreq(fftsize, dt)\n# Reordering makes a tidier plot...\nY=np.fft.fftshift(Y)\nf=np.fft.fftshift(f)\nplt.subplot(2,1,2); plt.plot(f, abs(Y))\nplt.xlabel(\"Frequency / Hz\")\nplt.show()\n\n\n\n\nAs we discussed in Section 1.4, the Fourier transform of real valued data has the property \\(F_n = \\left[F_{-n}\\right]^*\\):\n\nplt.subplot(2,1,1); plt.plot(f,Y.real)\nplt.subplot(2,1,2); plt.plot(f,Y.imag)\nplt.xlabel(\"Frequency / Hz\")\nplt.show()"
  },
  {
    "objectID": "notes/fourier.html#windowing",
    "href": "notes/fourier.html#windowing",
    "title": "Fast Fourier transform",
    "section": "2.4 Windowing",
    "text": "2.4 Windowing\nNote that, even though our signal consists of just a pair of sinusoids, the FFT does not just consist of \\(\\delta\\)-functions, which is more obvious on a log-scale:\n\nplt.magnitude_spectrum(y, Fs=100, scale='dB')\nplt.show()\n\n\n\n\nThe reason, as you may guess from your knowledge of Fourier transforms, is that our data is of finite length. This windowing causes the FFT to have non-zero values outside the frequencies present in the signal, a phenomenon called spectral leakage. In our case a sharp window means that the FFT is effectively convolved with the Fourier transform of a top hat function i.e. a sinc function. If the window happens to contain a whole number of wavelengths of the signals present, spectral leakage does not occur.\nThe effects of windowing can be mitigated by choosing different window functions — with smooth edges for example — to multiply our data, depending on what you are looking for. The rectangular / top hat window has low dynamic range: it is not great at distinguishing contributions of different amplitude even when their frequencies differ, as leakage from the large peak may obscure the other, smaller ones. On the other hand it has high resolution, meaning that it is good at resolving peaks of similar amplitude that are close in frequency. Expect to be presented with several options (Hamming, Tukey (him again), etc.) when using library functions that perform spectral analysis.\n\n\n\nImportance of windowing LIGO data, from Abbott et al. (2020). Without appropriate windowing, the entire power spectrum is dominated by spectral leakage following a \\(1/f^2\\) scaling."
  },
  {
    "objectID": "notes/fourier.html#signal-processing",
    "href": "notes/fourier.html#signal-processing",
    "title": "Fast Fourier transform",
    "section": "3.1 Signal processing",
    "text": "3.1 Signal processing\nAs an example, let’s look at some of the stages in the analysis of time series data from the LIGO and Virgo experiments on gravitational wave detection that led to the 2017 Nobel prize in physics. I’m following Abbott et al. (2020) closely here, and you should check it out for further details, as well as the accompanying notebook that describes how the analysis is performed in Python.\nAs illustrated in Figure 1 uncovering the signal in the raw data involves a number of processing steps designed to eliminate noise, mostly carried out in the Fourier domain. The guiding principle is that the noise is stationary — meaning that it is described by a random process that does not change in time — while the signal is transient. This idea can be used to reduce noise in the data even though low frequency noise completely dominates the raw measurement (top panel).\n\n\n\nFigure 1: Stages in the analysis of LIGO strain data,from Abbott et al. (2020). Note the scale on the \\(y\\)-axis!\n\n\nThe first step is windowing, which we have already discussed, designed to reduce spectral leakage. Next, the data is whitened, meaning that the Fourier spectrum is normalized by the spectral density (the power spectrum is made to resemble the power spectrum of white noise) \\[\n\\tilde d(f)\\longrightarrow \\frac{\\tilde d(f)}{S_n^{1/2}(f)}.\n\\]\nThe idea behind this step is to prevent high amplitude noise in certain parts of the spectrum from swamping the signal. After this step (third panel, red trace), the low frequency noise has been greatly reduced.\nFinally, the data are bandpass filtered with a pass band [35 Hz, 350 Hz], which removes low frequency seismic noise and high frequency (quantum) noise from the detector. Filtering is the Fourier analog of windowing i.e. multiplying by a function to discard certain parts of the signal. At this point, a transient is revealed in the data (bottom panel).\nThe next step is to fit this transient with a model that describes the graviational wave physics. An important check on the correctness of this approach is to then analyze the residual — the difference between the data and the model — and to check whether it is described by a stationary noise process (see Figure 2). In such a process the phases of the Fourier components are random and uncorrelated, for example.\n\n\n\nFigure 2: Residuals from the modelled signal, from Abbott et al. (2020)\n\n\nHopefully this short summary has emphasized the vital role played throughout by the processing of signals in the Fourier domain, and therefore the importance of the FFT in analyzing time series data."
  },
  {
    "objectID": "notes/fourier.html#partial-differential-equations",
    "href": "notes/fourier.html#partial-differential-equations",
    "title": "Fast Fourier transform",
    "section": "3.2 Partial differential equations",
    "text": "3.2 Partial differential equations\nWe’ll illustrate the role of the FFT in the numerical solution of partial differential equations (PDEs) using one that you all know well: the time-dependent Schrödinger equation\n\\[\ni\\hbar \\frac{\\partial \\psi}{\\partial t} = -\\frac{\\hbar^2}{2m}\\nabla^2 \\psi + V(\\mathbf{r})\\psi.\n\\tag{6}\\]\nWhen the potential is absent, the solutions are plane waves\n\\[\n\\Psi(\\mathbf{r}, t) = \\exp\\left[-i\\frac{\\hbar^2 \\mathbf{k}^2 t}{2m} +i\\mathbf{k}\\cdot\\mathbf{r}\\right].\n\\tag{7}\\]\nOn the other hand, if the first term on the right hand side of Equation 6 — representing the kinetic energy — were absent, then the evolution of the wavefunction would be\n\\[\n\\Psi(\\mathbf{r}, t) = \\Psi(\\mathbf{r}, 0)\\exp\\left[-iV(\\mathbf{r})t/\\hbar\\right].\n\\tag{8}\\]\nThe idea behind the split-step method is that the time evolution can be approximated by alternating the two kinds of time evolution represented by Equation 7 and Equation 8. In more formal terms, write Equation 6 in operator form as\n\\[\ni\\hbar \\frac{\\partial \\ket{\\psi}}{\\partial t} = H\\ket{\\psi} = (T+V)\\ket{\\psi}\n\\]\nThe solution is \\(\\ket{\\psi(t)} = \\exp(-iHt/\\hbar)\\ket{\\psi}(0)\\). The exponential of an operator sum \\(A+B\\) obeys the Lie product formula\n\\[\ne^{A+B} = \\lim_{n\\to\\infty}\\left( e^{A/n}e^{B/n}\\right)^n.\n\\]\nThe logic behind this formula is that when the exponent is small, the failure of \\(A\\) and \\(B\\) to commute can be neglected. More precisely, \\[\ne^{xA}e^{xB} = e^{x(A+B) + O(x^2)}.\n\\]\nIt turns out that a more accurate approximation is given by the Suzuki—Trotter formula\n\\[\ne^{xA/2}e^{xB}e^{xA/2} = e^{x(A+B) + O(x^3)}.\n\\]\nIn any case, the practical algorithm suggested by these formulas is implemented for the Schrödinger equation by switching between real space and Fourier space, where the two kinds of evolution are implemented. Here’s a simple 1D example:\n\ndef split_step_schrodinger(psi_0, dx, dt, V, N, x_0 = 0., k_0 = None, m = 1.0, non_linear = False):\n\n    len_x = psi_0.shape[0]\n\n    x = x_0 + dx*np.arange(len_x)\n\n    dk_x = (2*np.pi)/(len_x*dx)\n    if k_0 == None:\n        k_0 = -np.pi/dx\n    k_x = k_0+dk_x*np.arange(len_x)\n\n    psi_x = np.zeros((len_x,N), dtype = np.complex128)\n    psi_k = np.zeros((len_x,N), dtype = np.complex128)\n    psi_mod_x = np.zeros((len_x), dtype = np.complex128)\n    psi_mod_k = np.zeros((len_x), dtype = np.complex128)\n    psi_x[:,0] = psi_0\n\n    if not non_linear:\n        V_n = V(x)\n    else:\n        V_n = V(x,psi_0)\n\n    def _compute_psi_mod(j):\n        return (dx/np.sqrt(2*np.pi))*psi_x[:,j]*np.exp(-1.0j*k_x[0]*x)\n\n    def _compute_psi(j):\n        psi_x[:,j] = (np.sqrt(2*np.pi)/dx)*psi_mod_x*np.exp(1.0j*k_x[0]*x)\n        psi_k[:,j] = psi_mod_k*np.exp(-1.0j*x[0]*dk_x*np.arange(len_x))\n\n    def _x_half_step(j,ft = True):\n        if ft == True:\n            psi_mod_x[:] = np.fft.ifft(psi_mod_k[:])\n        if non_linear:\n            V_n[:] = V(x,psi_x[:,j])\n        psi_mod_x[:] = psi_mod_x[:]*np.exp(-1.0j*(dt/2.)*V_n)   \n\n    def _k_full_step():\n        psi_mod_k[:] = np.fft.fft(psi_mod_x[:])\n        psi_mod_k[:] = psi_mod_k[:]*np.exp(-1.0j*k_x**2*dt/(2.*m))      \n\n    def _main_loop():\n        psi_mod_x[:] = _compute_psi_mod(0)\n\n        for i in range(N-1):\n            _x_half_step(i,ft = False)\n            _k_full_step()\n            _x_half_step(i)\n            _compute_psi(i+1)\n\n    _main_loop()\n\n    return psi_x,psi_k,k_x\n\n\ndef oneD_gaussian(x,mean,std,k0):\n    return np.exp(-((x-mean)**2)/(4*std**2)+ 1j*x*k0)/(2*np.pi*std**2)**0.25\n\ndef V(x):\n    V_x = np.zeros_like(x)\n    V_x[np.where(abs(x) < 0.5)] = 1.5\n    return V_x\n\n\nN_x = 2**11\ndx = 0.05\nx = dx * (np.arange(N_x) - 0.5 * N_x)\n\ndt = 0.01\nN_t = 2000\n\np0 = 2.0\nd = np.sqrt(N_t*dt/2.)\n\npsi_0 = oneD_gaussian(x,x.max()-10.*d, d, -p0)\n\npsi_x,psi_k,k = split_step_schrodinger(psi_0, dx, dt, V, N_t, x_0 = x[0])\n\n\n\nAnimation code\nfrom matplotlib.animation import FuncAnimation\n\nreal_psi = np.real(psi_x)\nimag_psi = np.imag(psi_x)\nabsl_psi = np.absolute(psi_x)\nabs_psik = np.absolute(psi_k)\n\nfig = plt.figure(figsize = (10,10))\nax1 = plt.subplot(211)\nline1_R = ax1.plot(x,real_psi[:,0],'b')[0]\nline1_I = ax1.plot(x,imag_psi[:,0],'r')[0]\nline1_A = ax1.plot(x,absl_psi[:,0],'k')[0]\nline_V = ax1.plot(x,0.5*V(x),'k',alpha=0.5)[0]\nax1.set_ylim((real_psi.min(),real_psi.max()))\nax1.set_xlim((x.min(),x.max()))\n\nax2 = plt.subplot(212)\nline2 = ax2.plot(k,abs_psik[:,1],'k')[0]\nax2.set_ylim((abs_psik.min(),abs_psik.max()))\nax2.set_xlim((-10,10))\n\ndef nextframe(arg):\n    line1_R.set_data(x,real_psi[:,10*arg])\n    line1_I.set_data(x,imag_psi[:,10*arg])\n    line1_A.set_data(x,absl_psi[:,10*arg])\n    line2.set_data(k,abs_psik[:,10*arg])\n    \nanimate = FuncAnimation(fig, nextframe, frames = int(N_t/10), interval = 50, repeat = False)\nplt.show()\n\n\n\n\nVideo\nWavepacket colliding with a top hat barrier. Black line is the modulus, while red and blue are the real and imaginary parts."
  },
  {
    "objectID": "notes/ode.html",
    "href": "notes/ode.html",
    "title": "Solving differential equations with SciPy",
    "section": "",
    "text": "Newton’s fundamental discovery, the one which he considered necessary to keep secret and published only in the form of an anagram, consists of the following: Data aequatione quotcunque fluentes quantitates involvente, fluxiones invenire; et vice versa. In contemporary mathematical language, this means: “It is useful to solve differential equations”.\nVladimir Arnold, Geometrical Methods in the Theory of Ordinary Differential Equations\nWhile Arnold (and Newton) are of course right the problem is that solving differential equations is not possible in general. Even the simplest example of a first order ordinary differential equation (ODE) in a single variable\n\\[\n\\frac{dx}{dt} = f(x, t)\n\\tag{1}\\]\ncannot be solved for general \\(f(x,t)\\) 1. Of course, formulating a physical (or whatever) system in terms of differential equations represents a nontrivial step on the road to understanding it, but a lot remains to be done.\nNumerical analysis of differential equations is a colossal topic in applied mathematics and we are barely going to scratch the surface. The important thing is to be able to access existing solvers (and implement your own if necessary) and crucially to understand their limitations."
  },
  {
    "objectID": "notes/ode.html#truncation-error",
    "href": "notes/ode.html#truncation-error",
    "title": "Solving differential equations with SciPy",
    "section": "1.1 Truncation error",
    "text": "1.1 Truncation error\nIn making the approximation Equation 2 we make an \\(O(h^2)\\) local truncation error. To integrate for a fixed time the number of steps required is proportional to \\(h^{-1}\\), which means that the worst case error at fixed time (the global truncation error) is \\(O(h)\\). For this reason Euler’s method is called first order. More sophisticated methods are typically higher order: the SciPy function scipy.integrate.solve_ivp uses a fifth order method by default.\nThe midpoint method is a simple example of a higher order integration scheme\n\\[\n\\begin{align}\nk_1 &\\equiv h f(x_j,t_j) \\\\\nk_2 &\\equiv h f(x_i + k_1/2, t_j + h/2) \\\\\nx_{j+1} &= x_j + k_2 +O(h^3)\n\\end{align}\n\\]\nThe \\(O(h^2)\\) error cancels! The downside is that we have two function evaluations to perform per step, but this is often worthwhile."
  },
  {
    "objectID": "notes/ode.html#rounding-error",
    "href": "notes/ode.html#rounding-error",
    "title": "Solving differential equations with SciPy",
    "section": "1.2 Rounding error",
    "text": "1.2 Rounding error\nIf you had unlimited computer time you might think you could make the step size \\(h\\) ever smaller in order to make the updates more accurate. This ignores the machine precision \\(\\epsilon\\). The rounding error is roughly \\(\\epsilon x_j\\), and if the \\(N\\propto h^{-1}\\) errors in successive steps can be treated as independent random variables, the relative total rounding error will be \\(\\propto \\sqrt{N}\\epsilon=\\frac{\\epsilon}{\\sqrt{h}}\\) and will dominate for \\(h\\) small."
  },
  {
    "objectID": "notes/ode.html#stability",
    "href": "notes/ode.html#stability",
    "title": "Solving differential equations with SciPy",
    "section": "1.3 Stability",
    "text": "1.3 Stability\nApart from the relatively low accuracy that comes from using a first order method, the Euler method may additionally be unstable, depending on the equation. This can be demonstrated for the linear equation\n\\[\n\\frac{dx}{dt} = kx\n\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef euler(h, t_max, k=1):\n    \"\"\"\n    Solve the equation x' = k x, with x(0) = 1 using\n    the Euler method. \n\n    Integrate from t=0 to t=t_max using stepsize h for\n    num_steps = t_max / h.\n    \n    Returns two arrays of length num_steps: t, the time coordinate, and x_0, the position.\n    \"\"\"\n    num_steps = int(t_max / h)\n    # Allocate return arrays\n    x = np.zeros(num_steps, dtype=np.float32)\n    t = np.zeros(num_steps, dtype=np.float32)\n    x[0] = 1.0  # Initial condition\n    for i in range(num_steps - 1):\n        x[i+1] = x[i] + k * x[i] * h\n        t[i+1] = t[i] + h  # Time step\n    return t, x\n\nk = -2.3\nt_max = 5\nt, x = euler(1, t_max, k)\nplt.plot(t, x, label=\"h=1 Euler\")\nt, x = euler(0.7, t_max, k)\nplt.plot(t, x, label=\"h=0.7 Euler\")\nt = np.linspace(0, t_max, 100)\nplt.plot(t, np.exp(k * t), label=\"exact solution\")\nplt.title(\"k=-2.3\")\nplt.legend()\nplt.show()\n\n\n\n\nFor a linear equation the Euler update Equation 3 is a simple rescaling\n\\[\nx_{j+1} = x_j(1 + hk)\n\\]\nso the region of stability is \\(|1 + hk|\\leq 1\\). You can check that the backward Euler method Equation 4 eliminates the instability for \\(k<0\\)."
  },
  {
    "objectID": "notes/autograd.html",
    "href": "notes/autograd.html",
    "title": "Part II Computational Physics",
    "section": "",
    "text": "1 Automatic Differentiation\nKarpathy’s micrograd Michael Nielsen’s book"
  },
  {
    "objectID": "notes/references.html",
    "href": "notes/references.html",
    "title": "Part II Computational Physics",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "notes/numpy.html",
    "href": "notes/numpy.html",
    "title": "NumPy and friends",
    "section": "",
    "text": "The NumPy package is the key building block of the Python scientific ecosystem.\nIn this chapter we introduce a few of the key concepts. You should refer to the documentation for details. As with any mature software ecosystem, you should first assume that what you want to achieve can be achieved in a highly optimised way within the existing framework, and only resort to creating your own solution if and when you satisfy yourself that this is not the case.\nThere are a huge number of resources for learning NumPy online. Here are a couple of good ones:"
  },
  {
    "objectID": "notes/numpy.html#example-playing-with-images",
    "href": "notes/numpy.html#example-playing-with-images",
    "title": "NumPy and friends",
    "section": "6.1 Example: playing with images",
    "text": "6.1 Example: playing with images\nPixels in an image are encoded as a triple of RGB values in the range [0,255] i.e. 8 bits of type uint8 (the “u” is for “unsigned”). Tinting an image gives us a nice example of broadcasting\n\nimg = plt.imread('../assets/lucian.jpeg')\n\nimg_tinted = img * [1, 0.55, 1]\n\n# Show the original image\nplt.subplot(1, 2, 1)\nplt.imshow(img)\nplt.title(\"Lucian\")\n\n# Show the tinted image\nplt.subplot(1, 2, 2)\nplt.title(\"Pink Panther\")\n# Having multiplied by floats, \n# we must cast the image to uint8 before displaying it.\nplt.imshow(np.uint8(img_tinted))\n\nplt.show()\nimg.shape, img.dtype\n\n\n\n\n((4032, 3024, 3), dtype('uint8'))\n\n\nThis is a standard 12 megapixel image."
  },
  {
    "objectID": "notes/dynamic.html",
    "href": "notes/dynamic.html",
    "title": "Part II Computational Physics",
    "section": "",
    "text": "1 Dynamic Programming"
  }
]