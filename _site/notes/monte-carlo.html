<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Part II Computational Physics - Monte Carlo methods</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notes/complexity.html" rel="next">
<link href="../notes/ode.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Monte Carlo methods</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Part II Computational Physics</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Notes</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Course outline</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/intro.html" class="sidebar-item-text sidebar-link">Introduction</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/getting-going.html" class="sidebar-item-text sidebar-link">Getting going</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/numpy.html" class="sidebar-item-text sidebar-link">NumPy and friends</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/numbers.html" class="sidebar-item-text sidebar-link">Floating point and all that</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/ode.html" class="sidebar-item-text sidebar-link">Solving differential equations with SciPy</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/monte-carlo.html" class="sidebar-item-text sidebar-link active">Monte Carlo methods</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/complexity.html" class="sidebar-item-text sidebar-link">Algorithms and computational complexity</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/fourier.html" class="sidebar-item-text sidebar-link">Fast Fourier transform</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/autodiff.html" class="sidebar-item-text sidebar-link">Automatic differentiation and neural networks</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Slides</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/getting-going.html" class="sidebar-item-text sidebar-link">Getting going</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/numpy.html" class="sidebar-item-text sidebar-link">NumPy and friends</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/numbers-and-odes.html" class="sidebar-item-text sidebar-link">Floating point and ODEs</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/monte-carlo.html" class="sidebar-item-text sidebar-link">Monte Carlo methods</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/complexity.html" class="sidebar-item-text sidebar-link">Algorithms and computational complexity</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/fourier.html" class="sidebar-item-text sidebar-link">Fast Fourier transform</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/autodiff.html" class="sidebar-item-text sidebar-link">Autodiff and neural nets</a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#sampling-from-a-distribution" id="toc-sampling-from-a-distribution" class="nav-link active" data-scroll-target="#sampling-from-a-distribution"><span class="toc-section-number">1</span>  Sampling from a distribution</a></li>
  <li><a href="#the-monte-carlo-method" id="toc-the-monte-carlo-method" class="nav-link" data-scroll-target="#the-monte-carlo-method"><span class="toc-section-number">2</span>  The Monte Carlo method</a>
  <ul class="collapse">
  <li><a href="#monte-carlo-integration" id="toc-monte-carlo-integration" class="nav-link" data-scroll-target="#monte-carlo-integration"><span class="toc-section-number">2.1</span>  Monte Carlo integration</a></li>
  <li><a href="#importance-sampling" id="toc-importance-sampling" class="nav-link" data-scroll-target="#importance-sampling"><span class="toc-section-number">2.2</span>  Importance sampling</a></li>
  <li><a href="#sec-mcmc" id="toc-sec-mcmc" class="nav-link" data-scroll-target="#sec-mcmc"><span class="toc-section-number">2.3</span>  Markov chain Monte Carlo</a></li>
  </ul></li>
  <li><a href="#sec-statmech" id="toc-sec-statmech" class="nav-link" data-scroll-target="#sec-statmech"><span class="toc-section-number">3</span>  Statistical mechanics</a>
  <ul class="collapse">
  <li><a href="#mcmc-updates-for-the-ising-model" id="toc-mcmc-updates-for-the-ising-model" class="nav-link" data-scroll-target="#mcmc-updates-for-the-ising-model"><span class="toc-section-number">3.1</span>  MCMC updates for the Ising model</a></li>
  </ul></li>
  <li><a href="#the-universe-of-monte-carlo-methods" id="toc-the-universe-of-monte-carlo-methods" class="nav-link" data-scroll-target="#the-universe-of-monte-carlo-methods"><span class="toc-section-number">4</span>  The universe of Monte Carlo methods</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">Monte Carlo methods</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Many physical phenomena, notably those falling within the domains of statistical mechanics and quantum theory, depend in an essential way on <em>randomness</em>. The simulation of these phenomena therefore requires algorithms that incorporate random (or pseudo-random) elements in the most efficient way.</p>
<section id="sampling-from-a-distribution" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Sampling from a distribution</h1>
<p>Let’s suppose that we have a source of samples of a real valued random variable <span class="math inline">\(X\)</span> that follows a particular probability density function <span class="math inline">\(p_X\)</span> <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. This means that the probability of drawing a sample in the region <span class="math inline">\([x, x+dx]\)</span> is <span class="math inline">\(p_X(x)dx\)</span>. If we now map the samples using a function <span class="math inline">\(f\)</span>, what is the probability density <span class="math inline">\(p_Y\)</span> of <span class="math inline">\(y=f(x)\)</span>? The new probability density is defined in just the same way: the probability of <span class="math inline">\(y\)</span> lying in the region <span class="math inline">\([y, y+dy]\)</span> is <span class="math inline">\(p_Y(y)dy\)</span>. Since <span class="math inline">\(x\)</span> is being mapped deterministically to <span class="math inline">\(y\)</span> these two probabilities are therefore the same</p>
<p><span class="math display">\[
p_X(x)dx = p_Y(y)dy
\]</span></p>
<p>or</p>
<p><span class="math display">\[
p_Y(y)=p_X(x)\Bigg\lvert \frac{dx}{dy}\Bigg\rvert= \frac{p_X(x)}{|f'(x)|},\qquad x=f^{-1}(y)
\]</span></p>
<p>This formula shows that we can create samples from an arbitrary probability distribution by choosing an invertible map <span class="math inline">\(f\)</span> appropriately. If <span class="math inline">\(p_X\)</span> is a <a href="https://en.wikipedia.org/wiki/Continuous_uniform_distribution">standard uniform distribution</a> on <span class="math inline">\([0,1]\)</span> then <span class="math inline">\(f(x)\)</span> is the inverse of the cummulative probability distribution of <span class="math inline">\(Y\)</span> i.e.</p>
<p><span class="math display">\[
f^{-1}(y) = \int^y_{-\infty} p_Y(y')dy'
\]</span></p>
<p>The same approach works in higher dimensions: <span class="math inline">\(\big\lvert \frac{dx}{dy}\big\rvert\)</span> is replaced by the inverse of the Jacobian determinant.</p>
<p>The <a href="https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform">Box–Muller transform</a> is one example of this idea. Take two independent samples from a standard uniform distribution <span class="math inline">\(u_{1,2}\)</span> and form</p>
<p><span class="math display">\[
\begin{align}
x &amp;= \sqrt{-2\log u_1}\cos(2\pi u_2)\\
y &amp;= \sqrt{-2\log u_1}\sin(2\pi u_2).
\end{align}
\]</span></p>
<p><span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are independent samples from a <a href="https://en.wikipedia.org/wiki/Standard_normal_distribution">standard normal distribution</a>.</p>
<p>Various functions are available in the <a href="https://numpy.org/doc/stable/reference/random/index.html#module-numpy.random"><code>numpy.random</code></a> module to generate random arrays drawn from a variety of distributions. Box–Muller has now been retired in favour of the <a href="https://en.wikipedia.org/wiki/Ziggurat_algorithm">Ziggurat algorithm</a>.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy.random <span class="im">as</span> random</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>mu, sigma <span class="op">=</span> <span class="dv">0</span>, <span class="fl">0.1</span> <span class="co"># mean and standard deviation</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> random.normal(mu, sigma, size<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>count, bins, ignored <span class="op">=</span> plt.hist(s, <span class="dv">30</span>, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>plt.plot(bins, <span class="dv">1</span><span class="op">/</span>(sigma <span class="op">*</span> np.sqrt(<span class="dv">2</span> <span class="op">*</span> np.pi)) <span class="op">*</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>               np.exp( <span class="op">-</span> (bins <span class="op">-</span> mu)<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> sigma<span class="op">**</span><span class="dv">2</span>) ),</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>         linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Value"</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Frequency"</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="monte-carlo_files/figure-html/cell-2-output-1.png" width="589" height="422"></p>
</div>
</div>
<p>For complex multivariate (i.e.&nbsp;high dimensional) distributions there is no general recipe to construct an appropriate <span class="math inline">\(f\)</span>. One very recent application of these ideas is in machine learning models called <a href="https://arxiv.org/abs/1908.09257">normalizing flows</a> that use a mapping <span class="math inline">\(f\)</span> parameterized by a neural network. The workhorse for sampling from complicated distributions is Markov chain Monte Carlo, as we discuss in <a href="#sec-mcmc">Section&nbsp;2.3</a>.</p>
</section>
<section id="the-monte-carlo-method" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> The Monte Carlo method</h1>
<p><em>Monte Carlo</em> is the general prefix applied to variety of numerical methods that use randomness in some way. Two of the main classes of problem encountered in physics that come under this heading are:</p>
<ol type="1">
<li><p>Interpret a numerical evaluation as an expectation value of some random variable and use sampling to estimate it. <a href="https://en.wikipedia.org/wiki/Monte_Carlo_integration">Monte Carlo integration</a> is an example of this idea.</p></li>
<li><p>Sampling from a complex probability distribution (which may include taking expectation values). Example: <a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">Markov chain Monte Carlo</a>.</p></li>
</ol>
<section id="monte-carlo-integration" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="monte-carlo-integration"><span class="header-section-number">2.1</span> Monte Carlo integration</h2>
<p>The technique is exemplified by the following fairly dumb way of estimating <span class="math inline">\(\pi\)</span></p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>max_samples <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>inside <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>areas <span class="op">=</span> []</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> sample <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, max_samples <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> random.uniform(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> random.uniform(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> x <span class="op">**</span> <span class="dv">2</span> <span class="op">+</span> y <span class="op">**</span> <span class="dv">2</span> <span class="op">&lt;=</span> <span class="dv">1</span>:</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        inside <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    areas.append(<span class="dv">4</span> <span class="op">*</span> inside <span class="op">/</span> sample)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>plt.plot(np.arange(<span class="dv">1</span>, max_samples <span class="op">+</span> <span class="dv">1</span>), areas)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>plt.plot(np.arange(<span class="dv">1</span>, max_samples <span class="op">+</span> <span class="dv">1</span>), np.pi <span class="op">*</span> np.ones(max_samples), linestyle<span class="op">=</span><span class="st">'dashed'</span>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="monte-carlo_files/figure-html/cell-3-output-1.png" width="579" height="404"></p>
</div>
</div>
<p>In terms of integration, you can think of this as a way to compute the integral of a function which is one inside the unit disc, and zero outside it.</p>
<p>Although it’s a silly method, this does illustrate one important feature of Monte Carlo methods in general: that the relative error with <span class="math inline">\(N\)</span> samples is typically <span class="math inline">\(\propto N^{-1/2}\)</span> (thus at the 1% level for <span class="math inline">\(10^4\)</span> samples) because the variance of a sum of <span class="math inline">\(N\)</span> <a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables">iid</a> variables is <span class="math inline">\(\propto N^{1/2}\)</span>.</p>
<p>The general setting of Monte Carlo integration is as follows. Suppose we have a multidimensional integral to evaluate over some domain <span class="math inline">\(D\)</span></p>
<p><span class="math display">\[
I(f,D) = \int_D f(\mathbf{x}) d\mathbf{x}
\]</span></p>
<p>If we can sample points uniformly within <span class="math inline">\(D\)</span>, then an estimate for the integral is</p>
<p><span class="math display">\[
I(f,D) = \frac{V_D}{N}\sum_{i=1}^N f(\mathbf{x}_i)
\]</span></p>
<p>where <span class="math inline">\(N\)</span> is the number of samples and <span class="math inline">\(V_D\)</span> is the (hyper-)volume of <span class="math inline">\(D\)</span>. Why does this work? Because the uniform distribution has constant probability density <span class="math inline">\(1/V_D\)</span> so the average of <span class="math inline">\(f(\mathbf{x}_i)\)</span> with respect to this uniform distribution is simply related to the integral we are trying to calculate</p>
<p><span class="math display">\[
\bar f = \frac{1}{V_D}\int f(\mathbf{x})d\mathbf{x}.
\]</span></p>
<p>By taking many samples and averaging <span class="math inline">\(f(\mathbf{x}_i)\)</span> we can estimate this average. In the simple example that we started with <span class="math inline">\(f(\mathbf{x})\)</span> would be a “top hat” function that is one inside the circle. As in that example, the relative error is <span class="math inline">\(\propto N^{-1/2}\)</span>, whatever the dimension.</p>
<p>For this reason Monte Carlo integration comes into its own for high dimensional problems. For low dimensional integrals the quadrature methods in <a href="https://docs.scipy.org/doc/scipy/tutorial/integrate.html"><code>scipy.integrate</code></a> are preferable:</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> integrate</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>integrate.quadrature(np.cos, <span class="dv">0</span>, np.pi <span class="op">/</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>(0.9999999999999536, 3.9611425250996035e-11)</code></pre>
</div>
</div>
<p>As for ODE solvers, there is a lot of detail in the implementation to do with how intervals are chosen, and so on.</p>
</section>
<section id="importance-sampling" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="importance-sampling"><span class="header-section-number">2.2</span> Importance sampling</h2>
<p>Monte Carlo integration is not restricted to sampling from the uniform distribution. If our function <span class="math inline">\(f(\mathbf{x})\)</span> has regions where it is very small, there is not much point in sampling its value there. If there is a distribution we can sample from where samples tend to fall in the region where <span class="math inline">\(f(\mathbf{x})\)</span> is large, it will probably be better to use that. In this case we calculate the weighted average using the probability density <span class="math inline">\(p_\text{sample}(\mathbf{x})\)</span> from where the samples are drawn</p>
<p><span class="math display">\[
I(f,D, p_\text{sample}) = \frac{1}{N}\sum_{i=1}^N \frac{f(\mathbf{x}_i)}{p_\text{sample}(\mathbf{x}_i)}
\]</span></p>
<p>The reason this works is that the average of one of the terms in the sum is just the integral we want</p>
<p><span class="math display">\[
\overline{\frac{f(\mathbf{x})}{p_\text{sample}(\mathbf{x})}} = \int \frac{f(\mathbf{x})}{p_\text{sample}(\mathbf{x})} p_\text{sample}(\mathbf{x})d\mathbf{x} = \int f(\mathbf{x})d\mathbf{x}
\]</span></p>
<p>The benefit of this approach is that it can lead to a drastic reduction in the variance of the estimator. To take an extreme example: if <span class="math inline">\(f(\mathbf{x})\propto p_\text{sample}(\mathbf{x})\)</span>, and even a single sample leads to perfect estimate with no uncertainty! This observation is not useful, if you knew <span class="math inline">\(p_\text{sample}(\mathbf{x})\)</span> you would know the constant factor by which <span class="math inline">\(f(\mathbf{x})\)</span> differs, but it illustrates the point about variance reduction.</p>
<p>This general technique is called <a href="https://en.wikipedia.org/wiki/Importance_sampling">Importance sampling</a>. To apply the above approach, one needs both an explicit form for <span class="math inline">\(p_\text{sample}(\mathbf{x})\)</span> and the ability to generate samples, which is rather restrictive. There are many elaborations of the basic idea, however, including multiple distributions as well as adaptive sampling to “discover” the right region for sampling.</p>
</section>
<section id="sec-mcmc" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="sec-mcmc"><span class="header-section-number">2.3</span> Markov chain Monte Carlo</h2>
<p>Suppose you want to generate configurations at random (i.e.&nbsp;with a uniform distribution) from a “gas” of hard disks <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../assets/hard-spheres.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Coins in a shoe box (gas of hard disks). From <span class="citation" data-cites="krauth1998introduction">Krauth (<a href="#ref-krauth1998introduction" role="doc-biblioref">1998</a>)</span></figcaption><p></p>
</figure>
</div>
<p>It’s harder than it looks! The first guess you might have is to start adding coins at random, and if you get an overlap, try again until you don’t. Obviously this will become inefficient as the box fills up, and most attempts fail. <em>Worse, it doesn’t in fact yield a uniform distribution!</em> <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>Here’s an approach that works:</p>
<div id="exm-metropolis" class="theorem example">
<p><span class="theorem-title"><strong>Example 1 (Metropolis algorithm for hard disks) </strong></span>&nbsp;</p>
<ol type="1">
<li>Fix the number of disks and an initial configuration (some regular lattice configuration, say).</li>
<li>Pick a disk at random and attempt (or <em>propose</em>) to move it by a small random amount (i.e.&nbsp;random direction; random small magnitude).</li>
<li>If this results in the moved disk intersecting another, <em>reject</em> the move, leaving the disk where it is. Otherwise, <em>accept</em> the move.</li>
<li>Repeat 2. and 3. many times.</li>
</ol>
</div>
<p><img src="../assets/metropolis.png" class="img-fluid" alt="Accepted and rejected moves for hard disks. From">.</p>
<p>This is the simplest example of the <a href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm">Metropolis–Hastings algorithm</a>, the first Markov chain Monte Carlo (MCMC) algorithm.</p>
<p>More generally, the goal of MCMC is to come up with a sequential random process (a <strong>Markov chain</strong>) that generates (usually after many steps) a sample from a particular distribution.</p>
<p>You’ve all heard of a <a href="https://en.wikipedia.org/wiki/Random_walk">random walk</a>, perhaps as a model for diffusion. At each step you make a move in a random direction, independently of your earlier moves. After many steps these random moves gives rise to a distribution of possible locations. A random walk is the simplest example of a Markov chain.</p>
<p>More generally, a <a href="https://en.wikipedia.org/wiki/Markov_chain">Markov chain</a> is a sequence of random variables <span class="math inline">\(X_n\)</span> with each having a distribution that is is conditional on the value of the previous one, and so is defined in terms of <strong>transition probabilities</strong> <span class="math inline">\(p(X_{n}=x_n|X_{n-1}=x_{n-1})\)</span> (hence they form a “chain”). I’m going to immediately drop this cumbersome notation in favour of <span class="math inline">\(p(x_n|x_{n-1})\)</span>, a function of <span class="math inline">\(x_n\)</span> and <span class="math inline">\(x_{n-1}\)</span>, but in general the function giving the transition probabilities can be different at each step (the random variables could all be different).</p>
<p>The probability of a particular sequence <span class="math inline">\(X_1=x_1\ldots X_n=x_n\)</span> is therefore</p>
<p><span class="math display">\[
p(x_n|x_{n-1})p(x_{n-1}|x_{n-2})\cdots p(x_2|x_{1})p^{(1)}(x_1)
\]</span></p>
<p><span class="math inline">\(X_1\)</span> has no “parent” so is not conditional on any other value.</p>
<p>Suppose we don’t care about the earlier values and just want to know the <strong>marginal distribution</strong> <span class="math inline">\(p^{(n)}(x_n)\)</span> of the final variable. For a random walk this is easy, as <span class="math inline">\(x_n\)</span> typically represents a displacement that is a sum of iid increments. In general this is not the case, however, as the marginal distribution is</p>
<p><span class="math display">\[
p^{(n)}(x_n)=\sum_{x_{n-1},\ldots x_1}p(x_n|x_{n-1})p(x_{n-1}|x_{n-2})\cdots p(x_2|x_{1})p^{(1)}(x_1)
\]</span></p>
<p>(I’m writing all these expressions for discrete random variables, but the continuous version involving probability density functions is straightforward)</p>
<p>The sums are over all possible values that the random variables might take in the <strong>state space</strong> of the problem. These could be finite or infinite in number.</p>
<p>Things are not as bad as they appear, however, as the marginal distribution can be interpreted as the result of acting <span class="math inline">\(n-1\)</span> times on the vector of values of <span class="math inline">\(p^{(1)}_j\equiv p^{(1)}(j)\)</span> with the <strong>transition matrix</strong> with elements <span class="math inline">\(\mathsf{P}_{jk}=p(j|k)\)</span></p>
<p><span class="math display">\[
\mathbf{p}^{(n)} = \mathsf{P}^{n-1}\mathbf{p}^{(1)}.
\]</span></p>
<p>In a single step the marginal probabilities are updated as</p>
<p><span class="math display">\[
\mathbf{p}^{(n)} = \mathsf{P}^{n}\mathbf{p}^{(n-1)}.
\]</span></p>
<p><span class="math inline">\(\mathsf{P}\)</span> has some structure. The matrix elements are positive, as they represent probabilities, and each row sums to one</p>
<p><span class="math display">\[
\sum_j \mathsf{P}_{jk} = 1.
\]</span></p>
<p>Such matrices are called <a href="https://en.wikipedia.org/wiki/Stochastic_matrix">stochastic</a>.</p>
<p>Although <span class="math inline">\(p^{(n)}\)</span> — the probability distribution at the <span class="math inline">\(n\)</span>th step — changes from step to step, you might expect that after many steps it tends to converge to a <strong>stationary distribution</strong> <span class="math inline">\(p^{(n)}\to\boldsymbol{\pi}\)</span>. If it exists, this distribution must satisfy</p>
<p><span id="eq-stat"><span class="math display">\[
\boldsymbol{\pi} = \mathsf{P}\boldsymbol{\pi}.
\tag{1}\]</span></span></p>
<p>In other words, it is an eigenvector of <span class="math inline">\(\mathsf{P}\)</span> with eigenvalue one. This property is guaranteed by the <a href="https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem">Perron–Frobenius theorem</a> <a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</p>
<p>Thus <span class="math inline">\(\mathsf{P}\)</span> determines <span class="math inline">\(\boldsymbol{\pi}\)</span>. MCMC turns this idea on its head and asks: if there is some <span class="math inline">\(\boldsymbol{\pi}\)</span> that I would like to generate samples from, can I find a <span class="math inline">\(\mathsf{P}\)</span> that has it as a stationary distribution?</p>
<p>There is a trivial answer to this question. Sure, take <span class="math inline">\(\mathsf{P}_{jk}=\boldsymbol{\pi}_j\)</span>. That is, jump straight to the stationary distribution no matter what the starting state. But we are interested in highly complicated distributions over large state spaces (think the Boltzmann distribution for a statistical mechanical system comprised of billions of particles). Thus what we really want is to be able to approach such a complicated distribution by making many transitions with <em>simple</em> distributions.</p>
<p>One more idea is useful before returning to concrete algorithms. The quantity</p>
<p><span class="math display">\[
\mathsf{P}_{jk}\pi_k = p(j|k)\pi_k = p(j,k)
\]</span></p>
<p>is the joint distribution of seeing state <span class="math inline">\(k\)</span> followed by state <span class="math inline">\(j\)</span> in the stationary distribution. A <em>reversible</em> Markov chain is one where <span class="math inline">\(p(j,k)=p(k,j)\)</span>. Roughly, you can’t tell the direction of time because any transition is equally likely to happen forward in time as backward. Random physical processes that respect time reversal symmetry are often modeled as reversible Markov processes.</p>
<p>Combining reversibility with the definition of the stationary state yields the condition of <a href="https://en.wikipedia.org/wiki/Detailed_balance">detailed balance</a></p>
<p><span id="eq-detailed"><span class="math display">\[
\mathsf{P}_{jk}\pi_k = \pi_j\mathsf{P}_{kj}.
\tag{2}\]</span></span></p>
<p>This condition is stronger than the condition <a href="#eq-stat">Equation&nbsp;1</a> for a stationary state. This makes it easier to check: you don’t have to do a sum over a state space index. The Metropolis algorithm <a href="#exm-metropolis">Example&nbsp;1</a> for the hard disk problem satisfies detailed balance for a stationary distribution that is constant when disks don’t intersect and zero when they do.</p>
<p>When the stationary distribution <span class="math inline">\(\boldsymbol{\pi}\)</span> has more structure, designing an appropriate transition matrix is harder. The idea is to generalize the hard disk approach by separating the transition into a <em>proposal</em> distribution <span class="math inline">\(p_\text{prop}(j|k)\)</span> and an <em>acceptance</em> distribution <span class="math inline">\(p_\text{acc}(a=0,1|j\leftarrow k)\)</span> that gives the probability of a move from <span class="math inline">\(k\)</span> to <span class="math inline">\(j\)</span> being accepted (<span class="math inline">\(a=1\)</span>) or rejected (<span class="math inline">\(a=0\)</span>). The probability of moving from <span class="math inline">\(k\)</span> to <span class="math inline">\(j\)</span> is then</p>
<p><span class="math display">\[
p(j|k) = p_\text{acc}(a=1|j\leftarrow k) p_\text{prop}(j|k).
\]</span></p>
<p>Substituting this into the detailed balance condition <a href="#eq-detailed">Equation&nbsp;2</a> gives <span class="math display">\[
\frac{p_\text{acc}(a=1|j\leftarrow k)}{p_\text{acc}(a=1|k\leftarrow j)} = \frac{\pi_j}{\pi_k}\frac{p_\text{prop}(k|j)}{p_\text{prop}(j|k)}.
\]</span></p>
<p>Any <span class="math inline">\(p_\text{acc}\)</span> that satisfies this relation for all <span class="math inline">\(j\)</span> and <span class="math inline">\(k\)</span> will do the job. The Metropolis choice is</p>
<p><span id="eq-metropolis"><span class="math display">\[
p_\text{acc}(a=1|j \leftarrow k) = \min\left(1,  \frac{\pi_j}{\pi_k}\frac{p_\text{prop}(k|j)}{p_\text{prop}(j|k)}\right).
\tag{3}\]</span></span></p>
<p>This gives an extremely general algorithm, one of the top ten in applied mathematics, according to <a href="https://nhigham.com/2016/03/29/the-top-10-algorithms-in-applied-mathematics/">one list</a>:</p>
<div id="exm-metropolis-gen" class="theorem example">
<p><span class="theorem-title"><strong>Example 2 (Metropolis algorithm) </strong></span>&nbsp;</p>
<ol type="1">
<li>Starting from state <span class="math inline">\(k\)</span> sample a next state <span class="math inline">\(j\)</span> from the proposal distribution <span class="math inline">\(p_\text{prop}(j|k)\)</span>.</li>
<li>Accept the proposal with probability <span class="math inline">\(p_\text{acc}(a=1|j \leftarrow k)\)</span> and move to state <span class="math inline">\(j\)</span>. Otherwise reject the proposal and stay in state <span class="math inline">\(k\)</span>.</li>
<li>Repeat 1. and 2. many times.</li>
</ol>
</div>
<p>MCMC has the benefit of being <a href="https://en.wikipedia.org/wiki/Embarrassingly_parallel">embarrassingly parallel</a>. If you want to average something over <span class="math inline">\(\boldsymbol{\pi}\)</span>, just run the algorithm many times independently and average the results. This is perfect for parallel computing.</p>
<p>The Metropolis algorithm has an Achilles’ heel, however. To perform a move one has to sample from <span class="math inline">\(p_\text{prop}(j|k)\)</span> and from <span class="math inline">\(p_\text{acc}(a|j \leftarrow k)\)</span>. The proposal therefore has to be tractable, like the small shift in position for the hard disk case. This may however, mean that that many of the <span class="math inline">\(j\)</span>s suggested correspond to very small <span class="math inline">\(\pi_j\)</span>, and therefore a very low acceptance probability (c.f. <a href="#eq-metropolis">Equation&nbsp;3</a>). For example, in the hard disk case at high density many proposed moves will give rise to overlap of disks and be rejected. This means that many steps are required to have one successful update of the simulation. This kind of slowdown is a common feature of MCMC methods applied to complex distributions.</p>
<p>We’ll see some more examples of MCMC algorithms for statistical mechanical problems in <a href="#sec-statmech">Section&nbsp;3</a>, and ways in which this problem can be avoided.</p>
<!-- ## Relaxation to equilibrium

TODO

Eigenvalues

Master equation

Transition matrix -->
</section>
</section>
<section id="sec-statmech" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Statistical mechanics</h1>
<p>Statistical mechanics is a natural source of such complex distributions in physics. Remember the fundamental principle that the probability of finding a statistical mechanical system in a microstate <span class="math inline">\(\mathbf{x}\)</span> <a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> with energy <span class="math inline">\(\mathcal{E}(\mathbf{x})\)</span> is</p>
<p><span id="eq-boltzmann"><span class="math display">\[
p(\mathbf{x})=\frac{\exp\left[-\beta \mathcal{E}(\mathbf{x})\right]}{Z},
\tag{4}\]</span></span></p>
<p>where <span class="math inline">\(Z\)</span> is a normalizing constant called the partition function and <span class="math inline">\(\beta=1/k_\text{B}T\)</span>, where <span class="math inline">\(T\)</span> is the temperature and <span class="math inline">\(k_\text{B}\)</span> is Boltzmann’s constant.</p>
<p>The <em>central problem</em> of statistical mechanics is computing ensemble averages of physical quantities, and the <em>principal difficulty</em> is the intractability of those averages for large systems. For example, if we are dealing with a classical gas, the configuration space point <span class="math inline">\(\mathbf{x}\)</span> corresponds to the positions of each of the gas molecules <span class="math inline">\(\mathbf{x}=(\mathbf{x}_1,\ldots \mathbf{x}_N)\)</span> and an average is a <span class="math inline">\(3N\)</span>-dimensional integral. The only situation in which this integral is tractable is when the gas is noninteracting (ideal), in which case the energy function takes the form</p>
<p><span class="math display">\[
\mathcal{E}(\mathbf{x}) = \sum_{n=1}^N \mathcal{E}_1(\mathbf{x}_n)
\]</span></p>
<p>where <span class="math inline">\(\mathcal{E}_1(\mathbf{x})\)</span> is the single particle energy. In this case the integral factorizes. As soon as we introduce interactions between particles of the form</p>
<p><span class="math display">\[
\mathcal{E}(\mathbf{x}) = \sum_{n&lt;m}^N \mathcal{E}_2(\mathbf{x}_n,\mathbf{x}_m)
\]</span></p>
<p>things get a lot harder. The same issue arises in models involving discrete random variables. The canonical example is the <a href="https://en.wikipedia.org/wiki/Ising_model">Ising model</a>, in which a configuration corresponds to fixing the values of <span class="math inline">\(N\)</span> “spins” <span class="math inline">\(\sigma_n=\pm 1\)</span> with an energy function of the form</p>
<p><span class="math display">\[
\mathcal{E}(\sigma)=\sum_n h_n\sigma_n + \sum_{m&lt;n} J_{mn}\sigma_m\sigma_n.
\]</span></p>
<p>The two terms correspond to a (magnetic) field that acts on each spin and a coupling between spins. As in the gas, it’s the latter that causes problems / interest.</p>
<p>The Ising model comes in a great many flavours according to how the fields and couplings are chosen. They may reflect a lattice structure: <span class="math inline">\(J_{mn}\neq 0\)</span> for nearest neighbours, say, or longer range. They may be fixed or random, defining an ensemble of models.</p>
<p>The most pessimistic assessment is that to calculate an average we are going to have sum over <span class="math inline">\(2^N\)</span> configurations. Computing the partition function <span class="math inline">\(Z\)</span> that normalizes the average (or which gives the free energy via <span class="math inline">\(F=-k_\text{B}T\log Z\)</span>) is another such sum.</p>
<p>Monte Carlo simulation is a much more attractive alternative. MCMC can be used to generate samples from <span class="math inline">\(p(\sigma)\)</span> which are then used to estimate the averages of interest (e.g.&nbsp;average energy <span class="math inline">\(\langle\mathcal{E}(\sigma)\rangle\)</span>, average magnetization <span class="math inline">\(\langle\sum_n \sigma_n\rangle\)</span>, etc.).</p>
<section id="mcmc-updates-for-the-ising-model" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="mcmc-updates-for-the-ising-model"><span class="header-section-number">3.1</span> MCMC updates for the Ising model</h2>
<p>How does MCMC work in practice for the Ising model? To apply the Metropolis alogorithm <a href="#exm-metropolis-gen">Example&nbsp;2</a> we can use a simple proposal: pick each spin in turn in some order and try to flip it.</p>
<p>The form of <span class="math inline">\(p(\sigma)\)</span> means that, although we cannot compute the probabilities explicitly, we can calculate <em>ratios</em>, which is all we need for Metropolis. For two configurations that differ only by <span class="math inline">\(\sigma_n=\pm 1\)</span> we have</p>
<p><span class="math display">\[
\begin{align}
\frac{p(\sigma_n=1|\sigma_{m\neq n})}{p(\sigma_n=-1|\sigma_{m\neq n})} &amp;= \exp\left[-2\beta \left(h_n+\sum_{m\neq n} J_{mn}\sigma_m\right)\right]\\
&amp;\equiv \exp\left[-\beta\Delta \mathcal{E}\right],
\end{align}
\]</span></p>
<p>where <span class="math inline">\(\Delta \mathcal{E}\)</span> is the energy difference between two configurations.</p>
<p>One alternative to Metropolis is the <strong>Heat bath algorithm</strong> (or <a href="https://en.wikipedia.org/wiki/Glauber_dynamics">Glauber dynamics</a> or <a href="https://en.wikipedia.org/wiki/Gibbs_sampling">Gibbs sampling</a>) <a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>. The idea behind the name is that, since we can calculate the influence of the spin’s environment (the “bath”), we can just choose the spin’s orientation with the corresponding probabilities. Since there are only two probabilities the ratio is all we need and we get</p>
<p><span id="eq-heat-bath"><span class="math display">\[
p(\sigma_n=\pm 1|\sigma_{m\neq n}) = \frac{1}{1+ e^{\pm\beta \Delta \mathcal{E}}}.
\tag{5}\]</span></span></p>
<p>The algorithm is then:</p>
<div id="exm-heat-bath" class="theorem example">
<p><span class="theorem-title"><strong>Example 3 (Heat bath algorithm) </strong></span>&nbsp;</p>
<ol type="1">
<li>Pick a spin <span class="math inline">\(n\)</span>. <a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></li>
<li>Compute <span class="math inline">\(\Delta E\)</span>, the energy difference between <span class="math inline">\(\sigma_n=\pm 1\)</span>.</li>
<li>Set <span class="math inline">\(\sigma_n=\pm 1\)</span> with probabilities given by <a href="#eq-heat-bath">Equation&nbsp;5</a>.</li>
<li>Repeat 1-3 many times</li>
</ol>
</div>
<p>What happens if we try and come up with more complicated proposals, flipping many spins at once? For Metropolis, the problem is that without a cleverly designed proposal we will be suggesting moves that are likely to be rejected. For the heat bath algorithm, the more spins we flip, the more complicated the evaluation of the corresponding probabilities (<span class="math inline">\(2^n\)</span> outcomes if we flip <span class="math inline">\(n\)</span> spins).</p>
<p>The good news is that we <em>can</em> do better — much better — than the above algorithms. The <a href="https://en.wikipedia.org/wiki/Wolff_algorithm">Wolff algorithm</a> is one example. This proposes a cluster of spins of the same orientation to be flipped by adding adjacent spins to an initially random chosen spin with probability <span class="math inline">\(p_\text{add}\)</span>. It turns out that for the nearest neighbour Ising model with Ferromagnetic coupling <span class="math inline">\(J&lt;0\)</span> the “magic” value <span class="math inline">\(p_\text{add}=1-e^{2\beta J}\)</span> is <em>rejection free</em>: the probability to flip the whole cluster is always one. This makes for an extremely fast algorithm that is not subject to the usual <em>critical slowing down</em> at phase transitions.</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Ising model code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> IsingModel:</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, L):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.L <span class="op">=</span> L</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.spins <span class="op">=</span> np.random.choice(a<span class="op">=</span>[<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>], size<span class="op">=</span>(L, L))</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        stagger <span class="op">=</span> np.empty(<span class="va">self</span>.L, dtype <span class="op">=</span> <span class="bu">bool</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        stagger[::<span class="dv">2</span>] <span class="op">=</span> <span class="va">True</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        stagger[<span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> <span class="va">False</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mask <span class="op">=</span> np.logical_xor(stagger[:, np.newaxis], stagger[np.newaxis, :])</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> gibbs_update(<span class="va">self</span>, beta, sublattice):</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        fields <span class="op">=</span> np.roll(<span class="va">self</span>.spins, <span class="dv">1</span>, <span class="dv">0</span>) <span class="op">+</span> np.roll(<span class="va">self</span>.spins, <span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>) <span class="op">+</span> np.roll(<span class="va">self</span>.spins, <span class="dv">1</span>, <span class="dv">1</span>) <span class="op">+</span> np.roll(<span class="va">self</span>.spins, <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        delta_E <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> fields</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        spin_up_probabilities <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span> beta <span class="op">*</span> delta_E))</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        new_spins <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> (np.random.rand(<span class="va">self</span>.L, <span class="va">self</span>.L) <span class="op">&lt;</span> spin_up_probabilities) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.spins <span class="op">=</span> np.choose(np.logical_xor(sublattice, <span class="va">self</span>.mask), [<span class="va">self</span>.spins, new_spins])</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> glauber_update(<span class="va">self</span>, beta):</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        x, y <span class="op">=</span> np.random.randint(<span class="va">self</span>.L, size<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        fields <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> neighbour <span class="kw">in</span> [((x <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="va">self</span>.L, y), ((x <span class="op">-</span> <span class="dv">1</span>) <span class="op">%</span> <span class="va">self</span>.L, y), (x, (y <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="va">self</span>.L), (x, (y <span class="op">-</span> <span class="dv">1</span>) <span class="op">%</span> <span class="va">self</span>.L)]:</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>            fields <span class="op">+=</span> <span class="va">self</span>.spins[neighbour]</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>        delta_E <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> fields</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        spin_up_probability <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span> beta <span class="op">*</span> delta_E))        </span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.random.rand() <span class="op">&lt;</span> spin_up_probability:</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.spins[x, y] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.spins[x, y] <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> wolff_update(<span class="va">self</span>, beta):</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>        initial_x, initial_y <span class="op">=</span> np.random.randint(<span class="va">self</span>.L, size<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>        initial_spin <span class="op">=</span> <span class="va">self</span>.spins[initial_x, initial_y]</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>        cluster <span class="op">=</span> deque([(initial_x, initial_y)])</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>        add_prob <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> np.exp(<span class="op">-</span><span class="dv">2</span> <span class="op">*</span> beta)</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> <span class="bu">len</span>(cluster) <span class="op">!=</span> <span class="dv">0</span>:</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>            x, y <span class="op">=</span> cluster.popleft()</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.spins[x, y] <span class="op">==</span> initial_spin:</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.spins[x, y] <span class="op">*=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> neighbour <span class="kw">in</span> (((x <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="va">self</span>.L, y), ((x <span class="op">-</span> <span class="dv">1</span>) <span class="op">%</span> <span class="va">self</span>.L, y), (x, (y <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="va">self</span>.L), (x, (y <span class="op">-</span> <span class="dv">1</span>) <span class="op">%</span> <span class="va">self</span>.L)):</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> <span class="va">self</span>.spins[neighbour] <span class="op">==</span> initial_spin:</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">if</span> np.random.rand() <span class="op">&lt;</span> add_prob:</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>                            cluster.append(neighbour)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<script src="https://cdn.jsdelivr.net/npm/p5@1.4.1/lib/p5.js"></script>
<script src="https://cdn.jsdelivr.net/pyodide/v0.22.0/full/pyodide.js"></script>
<script src="../assets/ising.js"></script>
<div id="fig-ising" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<div id="ising-simulation" data-align="center">

</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: Glauber dynamics, Block Gibbs sampling and Wolff updates compared. Change the temperature using the slider. The centre of the slider corresponds to the critical temperature <span class="math inline">\(k_\text{B}T = 2|J|/\log(1+\sqrt{2})\sim 2.269|J|\)</span>.</figcaption><p></p>
</figure>
</div>
</section>
</section>
<section id="the-universe-of-monte-carlo-methods" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> The universe of Monte Carlo methods</h1>
<p>Monte Carlo simulation is a vast field with practitioners and specialists across the natural sciences, engineering, machine learning, and statistics. In this section I’ll mention a few important topics to give a taste of what’s out there. For much more detail take a look at <span class="citation" data-cites="krauth2006statistical">Krauth (<a href="#ref-krauth2006statistical" role="doc-biblioref">2006</a>)</span> and / or <span class="citation" data-cites="mackay2003information">MacKay (<a href="#ref-mackay2003information" role="doc-biblioref">2003</a>)</span>. The recent set of lectures <a href="https://hef.ru.nl/~tbudd/mct/intro.html">Monte Carlo Techniques</a> by Timothy Budd also look fantastic.</p>
<!-- Probably the biggest single issue is: how do you kow when your MCMC simulation has reached the stationary distribution $\boldsymbol{\pi}$? The pragmatic approach is to monitor the averages of interest (magnetization, say, in the case of the Ising model) over different simulations or over a time interval and see when they stop changing. 


We've touched on the issue of the [mixing time](https://en.wikipedia.org/wiki/Markov_chain_mixing_time) in a Markov chain.

1. Finite size effects
2. Approach to equilibrium
2. Critical slowing down / loss of ergodicity
3. Bias of estimators. Importance sampling

Exact sampling

[Hamiltonian Monte Carlo](https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo).


Multispin encoding: 32 or 64 simulations @jacobs1981multi

https://en.wikipedia.org/wiki/Gibbs_sampling

Other updates


A huge topic, see @krauth2006statistical for much more

Also Chapter 29 of @mackay2003information


https://hef.ru.nl/~tbudd/mct/intro.html
looks nice and deals with the queue issue

Comment at the end about typicality



MCMC in Bayesian inference

Relation to Ising models. Community detection. Why not?

https://arxiv.org/pdf/cond-mat/0005264.pdf

Bayesian inference

## Random number generators {#sec-rng}

Computers are deterministic 

This is covered in some detail in the Nature of Computation

This is a subject dealt with already

RNGs in Trebst?

Further reading: refer to [Krauth notes](https://arxiv.org/pdf/cond-mat/9612186.pdf) or book

Other suggestions from Twitter

https://roomno308.github.io/blog/MCMC.html
https://maximilianrohde.com/posts/code-breaking-with-metropolis/ -->



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-krauth1998introduction" class="csl-entry" role="doc-biblioentry">
Krauth, Werner. 1998. <span>“Introduction to Monte Carlo Algorithms.”</span> In <em>Advances in Computer Simulation</em>, 1–35. Springer.
</div>
<div id="ref-krauth2006statistical" class="csl-entry" role="doc-biblioentry">
———. 2006. <em>Statistical Mechanics: Algorithms and Computations</em>. Vol. 13. OUP Oxford.
</div>
<div id="ref-mackay2003information" class="csl-entry" role="doc-biblioentry">
MacKay, David JC. 2003. <em>Information Theory, Inference and Learning Algorithms</em>. Cambridge university press.
</div>
<div id="ref-metropolis1953equation" class="csl-entry" role="doc-biblioentry">
Metropolis, Nicholas, Arianna W Rosenbluth, Marshall N Rosenbluth, Augusta H Teller, and Edward Teller. 1953. <span>“Equation of State Calculations by Fast Computing Machines.”</span> <em>The Journal of Chemical Physics</em> 21 (6): 1087–92.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>A common shorthand notation is <span class="math inline">\(x\sim p_X\)</span>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>This is in fact the original motivation for the development of the technique, see <span class="citation" data-cites="metropolis1953equation">Metropolis et al. (<a href="#ref-metropolis1953equation" role="doc-biblioref">1953</a>)</span>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>For a discussion of what goes wrong in a simple discrete model of rods in 1D, see Section 2.2.1 of <span class="citation" data-cites="krauth2006statistical">Krauth (<a href="#ref-krauth2006statistical" role="doc-biblioref">2006</a>)</span>. <!-- TODO Why not? See @widom1966random for an explanation --><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>There is an important caveat. If there are two or more subsets of the state space that are not connected by finite transition probabilities, the probability distribution in each subset evolves independently and there is not a unique stationary distribution. When there <em>is</em>, we say that the Markov chain is <strong>ergodic</strong> and the corresponding transition matrix is <strong>irreducible</strong>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>For a classical gas of point particles this would correspond to specifying all the positions and velocities, for example.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Multiple names are sign that a technique was re-discovered by different communities who don’t talk to each other.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>This can be done deterministically (e.g.&nbsp;sequentially or in alternating blocks when the model is defined on a <a href="https://en.wikipedia.org/wiki/Bipartite_graph">bipartite graph</a>) — which is what is normally called Gibbs sampling — or at random, which corresponds to Glauber dynamics.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../notes/ode.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Solving differential equations with SciPy</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../notes/complexity.html" class="pagination-link">
        <span class="nav-page-text">Algorithms and computational complexity</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>