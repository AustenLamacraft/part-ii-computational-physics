<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Automatic differentiation and neural networks – Part II Computational Physics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notes/linear.html" rel="next">
<link href="../notes/fourier.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../index.html">Notes</a></li><li class="breadcrumb-item"><a href="../notes/autodiff.html">Automatic differentiation and neural networks</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Part II Computational Physics</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Notes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Course outline</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/getting-going.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting going</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/numpy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">NumPy and friends</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/numbers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Floating point and all that</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/ode.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Solving differential equations with SciPy</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/monte-carlo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Monte Carlo methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/complexity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Algorithms and computational complexity</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/fourier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fast Fourier transform</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/autodiff.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Automatic differentiation and neural networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notes/linear.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Linear algebra</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Slides</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/getting-going.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting going</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/numpy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">NumPy and friends</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/numbers-and-odes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Floating point and ODEs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/monte-carlo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Monte Carlo methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/complexity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Algorithms and computational complexity</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/fourier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fast Fourier transform</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/autodiff.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Autodiff and neural nets</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/linear.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Linear algebra</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Exercises and Projects</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../exercises/exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#background" id="toc-background" class="nav-link active" data-scroll-target="#background"><span class="header-section-number">1</span> Background</a>
  <ul class="collapse">
  <li><a href="#sec-cost" id="toc-sec-cost" class="nav-link" data-scroll-target="#sec-cost"><span class="header-section-number">1.1</span> The cost function</a></li>
  <li><a href="#sec-grad" id="toc-sec-grad" class="nav-link" data-scroll-target="#sec-grad"><span class="header-section-number">1.2</span> Gradient descent</a></li>
  <li><a href="#sec-nn" id="toc-sec-nn" class="nav-link" data-scroll-target="#sec-nn"><span class="header-section-number">1.3</span> The network</a>
  <ul class="collapse">
  <li><a href="#why-neural" id="toc-why-neural" class="nav-link" data-scroll-target="#why-neural"><span class="header-section-number">1.3.1</span> Why neural?</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-backprop" id="toc-sec-backprop" class="nav-link" data-scroll-target="#sec-backprop"><span class="header-section-number">2</span> Automatic differentiation and backpropagation</a>
  <ul class="collapse">
  <li><a href="#evaluating-the-derivatives" id="toc-evaluating-the-derivatives" class="nav-link" data-scroll-target="#evaluating-the-derivatives"><span class="header-section-number">2.1</span> Evaluating the derivatives</a></li>
  <li><a href="#forward-accumulation" id="toc-forward-accumulation" class="nav-link" data-scroll-target="#forward-accumulation"><span class="header-section-number">2.2</span> Forward accumulation</a></li>
  <li><a href="#backpropagation" id="toc-backpropagation" class="nav-link" data-scroll-target="#backpropagation"><span class="header-section-number">2.3</span> Backpropagation</a></li>
  </ul></li>
  <li><a href="#implementation" id="toc-implementation" class="nav-link" data-scroll-target="#implementation"><span class="header-section-number">3</span> Implementation</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">4</span> Further reading</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../index.html">Notes</a></li><li class="breadcrumb-item"><a href="../notes/autodiff.html">Automatic differentiation and neural networks</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Automatic differentiation and neural networks</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>In this lecture we are going to look at the algorithms that underlie the training of <a href="https://en.wikipedia.org/wiki/Neural_network">neural networks</a>, which are the dominant model in the field of machine learning.</p>
<section id="background" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Background</h1>
<p>At a very high level a neural network is just a complicated function with a lot of parameters — so really a <em>family</em> of functions — into which data can be passed. “Data” in this case could be anything: in popular applications in machine learning it could represent images (represented as pixel values), text (represented as strings), audio signals (represented as a time series), and so on. From now on just regard data as a big vector <span class="math inline">\(\mathbf{x}\in\mathbb{R}^{N_D}\)</span>, where <span class="math inline">\(N_D\)</span> is the dimensionality of the data (e.g.&nbsp;number of pixels in an image, <span class="math inline">\(\times 3\)</span> if it’s colour and there are three RGB values). In a common setting — called <em>supervised learning</em> — the purpose of this function is to map the data <span class="math inline">\(\mathbf{x}\)</span> to some output <span class="math inline">\(\mathbf{y}\)</span> that represents a set of labels that correspond (for example) to the different kinds of objects that might appear in the images. Denoting the function by <span class="math inline">\(\mathsf{NN}_\theta\)</span>, where <span class="math inline">\(\theta\)</span> represents the parameters, we have:</p>
<p><span class="math display">\[
\mathbf{y} = \mathsf{NN}_\theta(\mathbf{x})
\]</span></p>
<p>Different choices of <span class="math inline">\(\theta\)</span> give different functions. The idea is that if our family of functions is “big enough” then somewhere in that family will be a function that does a “good job”. In the previous sentence “big enough” is a somewhat vague idea that implies both a large number of parameters and a variety of functions accessible by varying them. “Good job” is a bit less vague: it means that if we try out our function on a set of data where the labels already exist (normally because a human as labelled them, either voluntarily or because they are confronted with <a href="https://en.wikipedia.org/wiki/ReCAPTCHA">reCAPTCHA</a>), the answer will be correct most of the time. The process of finding the “right” <span class="math inline">\(\theta\)</span> is called <em>training</em> the neural network. Since making a correct prediction can be quantified, training is really a matter of <em>optimizing</em> an appropriate function of the output <span class="math inline">\(\mathbf{y}\)</span>, so techniques of optimization can be applied.</p>
<p>I should say at the outset that the conceptual ideas behind the training of neural networks are not particularly deep, with the possible exception of the backpropagation algorithm that we will describe here. What has made for the revolutionary success of this approach — essentially putting all other forms of machine learning (e.g.&nbsp;symbolic) out of business <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> — is:</p>
<ol type="1">
<li>The free availability of large datasets of labelled data.</li>
<li>The free availability of open source languages, libraries, and models.</li>
<li>The <del>free</del> wide availability of the necessary computing power to train models.</li>
</ol>
<section id="sec-cost" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="sec-cost"><span class="header-section-number">1.1</span> The cost function</h2>
<p>Let’s discuss the idea of training as optimization in a bit more detail. We suppose that we have a large dataset of size <span class="math inline">\(N\)</span> that consists of data <span class="math inline">\(\mathbf{x}_i=1,\ldots N\)</span> together with labels <span class="math inline">\(l_i\)</span>. The first step is to encode our labels in vectors <span class="math inline">\(\mathbf{y}_i\)</span> that can be compared with the output of the neural network. A popular choice is <em>one hot</em> encoding. <span class="math inline">\(\mathbf{y}_i\)</span> is an <span class="math inline">\(N_L\)</span> dimensional vector, where <span class="math inline">\(N_L\)</span> is the number of labels, and label <span class="math inline">\(n\)</span> is encoded as <span class="math inline">\((0,0,\ldots, 1, \ldots, 0)\)</span>, with the <span class="math inline">\(1\)</span> in the <span class="math inline">\(n\)</span>th place.</p>
<p>We would like to train the network (choose the parameters <span class="math inline">\(\theta\)</span>) so that <span class="math inline">\(\mathsf{NN}_\theta(\mathbf{x}_i)\)</span> is close to the corresponding <span class="math inline">\(\mathbf{y}_i\)</span> that represents the label. In order to quantify this we introduce a <em>cost</em> or <em>loss function</em> that quantifies the difference. A simple example is the quadratic cost</p>
<p><span id="eq-cost"><span class="math display">\[
\mathcal{C}(\theta) = \frac{1}{2N}\sum_{i=1}^N \lVert\mathbf{y}_i-\mathsf{NN}_\theta(\mathbf{x}_i)\rVert^2.
\tag{1}\]</span></span></p>
<p>In other words, we use the usual square norm in <span class="math inline">\(\mathbb{R}^{N_L}\)</span> of the distance between the network output and encoded label. Note also that we <em>average</em> over the training data, because sometimes our network may not perform so well, confusing different labels:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../assets/dog-or-food.png" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>Muffin or chihuahua?</figcaption>
</figure>
</div>
<p>The idea is now to minimize <span class="math inline">\(\mathcal{C}(\theta)\)</span> over the parameters of the network. The rest of this lecture concerns the practicalities of how this is done. When it comes to <em>using</em> the model for identifying previously unseen data, we need a procedure for turning the output <span class="math inline">\(\mathsf{NN}_\theta(\mathbf{x})\)</span> — an <span class="math inline">\(N_L\)</span> dimensional vector — into a discrete label. If the network has been defined so that the components of the output are non-negative, and recalling that the labels were encoded as one hot vectors, the simplest way to do this is to find the maximum component and make the prediction that the corresponding label is the correct one. This is written as</p>
<p><span class="math display">\[
l_* = \underset{l}{\operatorname{argmax}} \left[\mathsf{NN}_\theta(\mathbf{x})\right]_l.
\]</span></p>
<p>When evaluating the performance of a machine learning model there is a standard protocol that involves splitting the dataset into <em>training set</em> and a <em>test set</em>, where the former is used for training the model and the latter for evaluating it. After training the model it should perform well on the training set, but will generally perform less well on the test set, which contains data that the model has never seen. The difference between the cost function evaluated on the test set and the training set is a measure of how well the model generalizes to new inputs and is known as the <a href="https://en.wikipedia.org/wiki/Generalization_error">generalization error</a>.</p>
<p>A particular risk when using large neural networks with many parameters is the problem of <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a>. A sufficiently flexible model is capable of effectively “memorizing” the dataset, without “understanding” the labelling, leading to poor generalization.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../assets/overfitting.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>Simple illustration of overfitting, from <a href="https://en.wikipedia.org/wiki/Overfitting">Wikipeda</a>. The black line represents a “reasonable” model that does a decent job of distinguishing between the two labels in the data (red and blue), while the green line represents an “unreasonable” model that does a better job.</figcaption>
</figure>
</div>
<p>A particularly vivid example appears in <span class="citation" data-cites="zhang2021understanding">Zhang et al. (<a href="#ref-zhang2021understanding" role="doc-biblioref">2021</a>)</span>. They showed that popular computer vision models can be trained on <em>randomly</em> labelled data (where the labels have no connection to the image) to achieve perfect accuracy on the training set. Of course, the resulting performance on the test set was no better than random guessing. This is a natural consequence of <em>overparameterization</em> — having more parameters than data points in your training data — and shows that much of the success in training models with good generalization is down to the details of how the training is done (for example, by stopping before the training error gets too low).</p>
</section>
<section id="sec-grad" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="sec-grad"><span class="header-section-number">1.2</span> Gradient descent</h2>
<p>Leaving these questions aside, the basic idea underlying training is an extremely simple algorithm called <a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a>. If our network <span class="math inline">\(\mathsf{NN}_\theta\)</span> is designed appropriately, our cost function <a href="#eq-cost" class="quarto-xref">Equation&nbsp;1</a> is a differentiable function of the parameters <span class="math inline">\(\theta\)</span>. The minimum cost that we seek therefore corresponds to a stationary point where <span class="math inline">\(\nabla_\theta \mathcal{C}(\theta)|_{\theta_*}=0\)</span>. The idea of gradient descent is to take steps “downhill” i.e.&nbsp;in the direction <span class="math inline">\(-\mathcal{C}(\theta)\)</span> in the high dimensional space of all the parameters, where each step corresponds to an update of the parameters according to</p>
<p><span id="eq-grad-descent"><span class="math display">\[
\theta_i\longrightarrow \theta'_i = \theta_i - \eta \frac{\partial\mathcal{C}}{\partial \theta_i}
\tag{2}\]</span></span></p>
<p>where <span class="math inline">\(\eta\)</span> is a hyperparameter<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> called the <em>learning rate</em>. Choosing the learning rate is an important part of the craft of training models: too large and the first order approximation underlying <a href="#eq-grad-descent" class="quarto-xref">Equation&nbsp;2</a> breaks down and the cost may end up increasing; too small and the network will take too long to train. Often a <em>learning rate schedule</em> is used where the rate is adjusted during training to optimize convergence. You might guess that starting off with a large learning rate and then reducing it is the right way to go, and this is correct, but people do all sort of exotic things.</p>
<p>You might find it surprising that such a simple approach plays such an important role in machine learning. All of the sophistication lies in how the model is defined (<a href="#sec-nn" class="quarto-xref">Section&nbsp;1.3</a>) and how the gradients are calculated (<a href="#sec-backprop" class="quarto-xref">Section&nbsp;2</a>): for a complicated function with many parameters <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> this is a highly nontrivial task. While there are plenty of more sophisticated optimization methods they often involve more information about the model’s dependence on its parameters, and this is more costly to evaluate. For example <a href="https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization">Newton’s method</a> — which you may have encountered before — requires knowledge of first <em>and second</em> derivatives at each step, and this is normally less practical.</p>
<p>Another issue that relates to scale concerns the definition of our cost function <a href="#eq-cost" class="quarto-xref">Equation&nbsp;1</a> as an average over the dataset. For large datasets consisting of high dimensional data (e.g.&nbsp;images) it is usually not practical to calculate the gradient of the cost using the entire dataset. The usual procedure is then to split the data up into batches (usually called <em>minibatches</em>, confusingly), and perform each step of gradient descent by evaluating the gradient only on the batch, moving on to a new batch at the next step. Eventually this will lead to all the data in the dataset being used, which is usually known as one <em>epoch</em> of training. Training a model can involve many epochs (passes through the dataset).</p>
<p>Because each step only uses part of the data, the gradients calculated are going to be more “noisy” than the “true” gradients involving the whole dataset. Because of this, training by gradient descent with minibatches is known as <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">stochastic gradient descent</a>. It is generally thought that the noise introduced by minibatching plays a role in improving the generalization performance of neural networks.</p>
</section>
<section id="sec-nn" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="sec-nn"><span class="header-section-number">1.3</span> The network</h2>
<p>So far we have said nothing at all about <span class="math inline">\(\mathsf{NN}_\theta\)</span> except that it is a function <span class="math inline">\(\mathsf{NN}_\theta:\mathbb{R}^{N_D}\longrightarrow \mathbb{R}^{N_L}\)</span> from the space of data to the space of labels, and it has lots of parameters. What is this function, and why is it called a “neural network”? In this section we’ll define <span class="math inline">\(\mathsf{NN}_\theta\)</span> and say something about the origins of the idea in neuroscience.</p>
<p>Leaving biology aside for the moment, we know that <span class="math inline">\(\mathsf{NN}_\theta\)</span> must be <em>complicated</em>. We want it to take high dimensional inputs and somehow interpret them, outputting a label which synthesizes lots of high-level features in the data (e.g.&nbsp;in images the network must detect edges, shapes, and their relation). How can we make a <em>complicated</em> function?</p>
<p>The answer is that we do it by <em>composing</em> lots of simpler functions</p>
<p><span id="eq-composed"><span class="math display">\[
\mathsf{NN}_\theta = f_\theta^{(L)} \circ f_\theta^{(L-1)} \cdots \circ f_\theta^{(2)} \circ f_\theta^{(1)}
\tag{3}\]</span></span></p>
<p>The function <span class="math inline">\(f^{(1)}\)</span> is a map <span class="math inline">\(f^{(1)}:\mathbb{R}^{N_D}\longrightarrow \mathbb{R}^{h_{1}}\)</span>, where <span class="math inline">\(h_1\)</span> is usually called the <em>width</em> of the first <em>hidden layer</em> (we’ll shortly draw a picture to show where this terminology comes from). <span class="math inline">\(f^{(j)}\)</span> are maps <span class="math inline">\(f^{(j)}:\mathbb{R}^{h_{j-1}}\longrightarrow \mathbb{R}^{h_{j}}\)</span> for <span class="math inline">\(j=2, \ldots L-1\)</span>, and finally the <em>output layer</em> is <span class="math inline">\(f^{(L)}:\mathbb{R}^{h_{L-1}}\longrightarrow \mathbb{R}^{N_L}\)</span>. The dimensions <span class="math inline">\(h_j\)</span> are hyperparameters of the model.</p>
<p>We now have to define the intermediate functions <span class="math inline">\(f^{(j)}\)</span>. Although we have said they should be simple, we still want to allow the possibility that each of the output components depends on all of the input components. The usual recipe is</p>
<p><span id="eq-fdef"><span class="math display">\[
\left[f(\mathbf{x})\right]_\alpha = \phi\left(\sum_{\beta=1}^{N_\text{in}} w_{\alpha\beta}x_\beta + b_\alpha\right),\qquad \alpha = 1,\ldots N_\text{out}
\tag{4}\]</span></span></p>
<p>Here the matrix <span class="math inline">\(w\in \mathbb{R}^{N_\text{out}\times N_\text{in}}\)</span> contains the <em>weights</em> and the vector <span class="math inline">\(\mathbf{b}\in\mathbb{R}^{N_\text{out}}\)</span> contains the <em>biases</em>. These are the parameters of (this layer of) the network and will be modified during training. The function <span class="math inline">\(\phi:\mathbb{R}\longrightarrow\mathbb{R}\)</span> is called the <em>activation function</em>. There are several popular choices, including the <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid</a></p>
<p><span class="math display">\[
\sigma(x) = \frac{1}{1+e^{-x}},
\]</span></p>
<p>probably more familiar to you as the Fermi-Dirac distribution, and the <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a> function <span class="math inline">\(\max(0,x)\)</span> <a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</p>
<p><a href="#eq-fdef" class="quarto-xref">Equation&nbsp;4</a> is sometimes written more compactly as</p>
<p><span class="math display">\[
f(\mathbf{x}) = \phi(w\cdot\mathbf{x} + \mathbf{b}).
\]</span></p>
<p>You should understand this expression in the sense of vectorized functions (like NumPy’s ufuncs): <span class="math inline">\(\phi()\)</span> is applied to each element of <span class="math inline">\(w\cdot\mathbf{x} + \mathbf{b}\)</span>.</p>
<p>Activation functions should be differentiable but <em>nonlinear</em>. A linear function would mean that the function compositions in <a href="#eq-composed" class="quarto-xref">Equation&nbsp;3</a> collapse into matrix multiplications, producing a single overall weight matrix and bias vector. There wouldn’t be any point having separate functions: one would do the same job.</p>
<p>The result of composing functions like this many times, each with their own set of weights and biases, is a highly complex function. The term <em>deep learning</em>, which you will often here these days in the context of neural networks, refers to models with many function applications, or <em>layers</em>, which is the source of the networks’ expressiveness.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../assets/loss-landscape.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>A two dimensional section through the cost function (“loss landscape”) of a neural network from <span class="citation" data-cites="li2018visualizing">Li et al. (<a href="#ref-li2018visualizing" role="doc-biblioref">2018</a>)</span>.</figcaption>
</figure>
</div>
<p>The network that we have described so far is called <em>fully connected</em>, because in <a href="#eq-fdef" class="quarto-xref">Equation&nbsp;4</a> the matrix of weights means that every input dimension is coupled to every output dimension. Most of the innovation in neural networks over the past decade has been in creating and refining architectures that exploit the structure of the data in some way. For example, in a network to be used for computer vision (image recognition), it makes sense that the model “knows” that two input pixels are near or far from each other. This means that the input dimensions corresponding to two nearby pixels should be treated differently at the outset (i.e.&nbsp;before training) than two separated pixels. Also, the way the network responds to an image should not be strongly dependent on translations of that image. This implies that the weight matrices <span class="math inline">\(w\)</span> should have some structure to them. In the case of vision this lead to the <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">convolutional neural network</a>, where the <span class="math inline">\(w\)</span>’s act like convolutions, exploiting translational invariance but still retaining many parameters. We won’t go any further into these different architectures in this lecture.</p>
<section id="why-neural" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1" class="anchored" data-anchor-id="why-neural"><span class="header-section-number">1.3.1</span> Why neural?</h3>
<p>You may still be wondering where the network is, let alone the neurons. I’ve decide to present the neural network model in an ahistorical way, but if you look at other presentations you will tend to see the function <a href="#eq-fdef" class="quarto-xref">Equation&nbsp;4</a> represented graphically as</p>
<div id="fig-neuron" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-neuron-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/nielsen-single-neuron.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-neuron-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: A single artifical neuron. From <span class="citation" data-cites="nielsen2015neural">Nielsen (<a href="#ref-nielsen2015neural" role="doc-biblioref">2015</a>)</span>.
</figcaption>
</figure>
</div>
<p>which reflects the dependence of the output on the inputs (and not much else). The result of composing several such functions then looks like this:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../assets/nielsen-nn.png" class="img-fluid figure-img"></p>
<figcaption>An artifical neural network. From <span class="citation" data-cites="nielsen2015neural">Nielsen (<a href="#ref-nielsen2015neural" role="doc-biblioref">2015</a>)</span>.</figcaption>
</figure>
</div>
<p><em>This</em> is the network we have been talking about all along! Specifically, it is a network called a <a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">directed acylic graph</a> (DAG), meaning that the connections have a direction to them (input to output) and there are no loops.</p>
<p>Neural networks have long been used as a model for what goes on in the brain, with <a href="#fig-neuron" class="quarto-xref">Figure&nbsp;1</a> playing the role of the neuron. There are many differences, however, including the absence of any particular role for time<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>, and the fact that (real) neural networks are not DAGs! This latter property plays a decisive role in the training of artificial neural networks, as we’ll see in the next section. In general, I feel that neural networks have outgrown their biological inspiration, which is the reason I’ve downplayed it here.</p>
</section>
</section>
</section>
<section id="sec-backprop" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Automatic differentiation and backpropagation</h1>
<p>The ingredients we have assembled so far are:</p>
<ol type="1">
<li>The cost function: <span class="math display">\[
\mathcal{C}(\theta) = \frac{1}{2N}\sum_{i=1}^N \lVert\mathbf{y}_i-\mathsf{NN}_\theta(\mathbf{x}_i)\rVert^2
\]</span></li>
<li>Training by gradient descent: <span class="math display">\[
\theta_i\longrightarrow \theta'_i = \theta_i - \eta \frac{\partial\mathcal{C}}{\partial \theta_i}
\]</span></li>
<li>A neural network expressed as a composition of functions: <span id="eq-composed2"><span class="math display">\[
\mathsf{NN}_\theta = f_\theta^{(L)} \circ f_\theta^{(L-1)} \cdots \circ f_\theta^{(2)} \circ f_\theta^{(1)}
\tag{5}\]</span></span></li>
</ol>
<p>In order to perform a gradient step in training a neural network we have to calculate the gradients <span class="math inline">\(\partial\mathcal{C}/\partial \theta_i\)</span>. In this section we are going to see how the structure in <a href="#eq-composed2" class="quarto-xref">Equation&nbsp;5</a> allows these gradients to be calculated efficiently, using an algorithm called <a href="https://en.wikipedia.org/wiki/Backpropagation">backpropagation</a>.</p>
<p>Backpropagation is an example of <a href="https://en.wikipedia.org/wiki/Automatic_differentiation">automatic differentiation</a> (AD): the algorithmic evaluation of derivatives of a function. When people first hear about AD they sometimes guess that it must be something like this</p>
<p><span id="eq-nd"><span class="math display">\[
\frac{\partial\mathcal{C}}{\partial \theta_i} \approx \frac{\mathcal{C}(\theta_i+\Delta\theta_i)- \mathcal{C}(\theta_i)}{\Delta \theta_i}
\tag{6}\]</span></span></p>
<p>i.e.&nbsp;the numerical evaluation of a derivative. This is called <a href="https://en.wikipedia.org/wiki/Numerical_differentiation">numerical differentiation</a> and it’s what you would be forced to do if you only had access to <span class="math inline">\(\mathcal{C}(\theta)\)</span> as a <a href="https://en.wikipedia.org/wiki/Black_box">black box</a> function: one you can evaluate but otherwise have no knowledge of. AD is a different beast altogether: it uses knowledge about how the function <span class="math inline">\(\mathcal{C}\)</span> is formed by composing many simpler functions, together with the (analytic) derivatives of those functions, to find the overall gradient.</p>
<p>As well as being a bit more elegant than the brute force approach, there is another reason why AD is preferred over the (conceptually simpler) approach <a href="#eq-nd" class="quarto-xref">Equation&nbsp;6</a>. That approach would require us to vary each of the parameters in the network separately: think of those 175 billion parameters of ChatGPT! AD uses the network structure to simplify things drastically, as we’ll now see.</p>
<section id="evaluating-the-derivatives" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="evaluating-the-derivatives"><span class="header-section-number">2.1</span> Evaluating the derivatives</h2>
<p>Let’s just plow on and evaluate <span class="math inline">\(\partial\mathcal{C}/\partial \theta_i\)</span>. Remember that the parameters are the set of weights and biases in each layer:</p>
<p><span class="math display">\[
\theta = (w^{(1)}, \mathbf{b}^{(1)},\ldots, w^{(L)}, \mathbf{b}^{(L)}).
\]</span></p>
<p>The function <span class="math inline">\(\mathsf{NN}_\theta\)</span> can therefore more precisely be written as</p>
<p><span id="eq-composed3"><span class="math display">\[
\mathsf{NN}_\theta = f_{w^{(L)}, \mathbf{b}^{(L)}}^{(L)} \circ f_{w^{(L-1)}, \mathbf{b}^{(L-1)}}^{(L-1)} \cdots \circ f_{w^{(2)}, \mathbf{b}^{(2)}}^{(2)} \circ f_{w^{(1)}, \mathbf{b}^{(1)}}^{(1)}.
\tag{7}\]</span></span></p>
<p>Evaluating the derivative with respect to weights and biases in layer <span class="math inline">\(l\)</span> is therefore going to involve applying the chain rule to <a href="#eq-composed3" class="quarto-xref">Equation&nbsp;7</a>. In the following we’re going to denote the input to the <span class="math inline">\(l\)</span>th layer as</p>
<p><span class="math display">\[
\mathbf{z}^{(l)} \equiv w^{(l)} \cdot \mathbf{x}^{(l)} + \mathbf{b}^{(l)}
\]</span></p>
<p>and the output as <span class="math inline">\(\mathbf{a}^{(l)}\)</span> (“a” for activation). Thus the layer is written in vectorized form as</p>
<p><span class="math display">\[
\mathbf{a}^{(l)} = \phi(\mathbf{z}^{(l)}).
\]</span></p>
<p>Let’s evaluate the derivative of <span class="math inline">\(\mathsf{NN}_\theta(\mathbf{x})\)</span> with some fixed input with respect to the biases <span class="math inline">\(\mathbf{b}^{(l)}\)</span> in the <span class="math inline">\(l\)</span>th layer. One thing to note in passing is that the cost functions we consider are simple sums (averages) over different data <span class="math inline">\(\mathbf{x}_i\)</span>, so the derivatives are too.</p>
<p>A straightforward application of the chain rule gives</p>
<p><span id="eq-bias-chain"><span class="math display">\[
\frac{\partial \mathsf{NN}_\theta(\mathbf{x})}{\partial \mathbf{b}^{(l)}} = \frac{\partial f^{(L)}}{\partial \mathbf{x}^{(L)}}\cdot \frac{\partial f^{(L-1)}}{\partial \mathbf{x}^{(L-1)}} \cdots  \frac{\partial f^{(l)}}{\partial \mathbf{z}^{(l)}}
\tag{8}\]</span></span></p>
<p>because <span class="math inline">\(d\mathbf{z}^{(l)}=d\mathbf{b}^{(l)}\)</span>. In this expression</p>
<p><span id="eq-jac"><span class="math display">\[
\frac{\partial f_j^{(l')}}{\partial x_k^{(l')}} = \phi'(z^{(l')}_j)w^{(l')}_{jk} \qquad l'=l+1,\ldots L
\tag{9}\]</span></span></p>
<p>is the Jacobian matrix of each layer and the final factor is</p>
<p><span class="math display">\[
\frac{\partial f_j^{(l)}}{\partial z_k^{(l)}} =  \phi'(z^{(l)}_j)\delta_{jk}.
\]</span></p>
<p>We find a similar expression for the derivative with respect to the weights in the <span class="math inline">\(l\)</span>th layer. These expressions all involve the deriviative of the activation function. When AD is implemented in code, the definition of any function is always supplemented with the derivative of that function.</p>
<p>In <a href="#eq-bias-chain" class="quarto-xref">Equation&nbsp;8</a> the matrices are composed by matrix multiplication. How should they be evaluated? There are two possibilities:</p>
</section>
<section id="forward-accumulation" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="forward-accumulation"><span class="header-section-number">2.2</span> Forward accumulation</h2>
<p>We go from right to left. Starting from the input <span class="math inline">\(\mathbf{x}\)</span>, we evaluate <span class="math inline">\(\mathbf{z}^{(l)}\)</span> by evaluating the first <span class="math inline">\(l-1\)</span> functions, passing the output from each to the input of the next. Once we reach <span class="math inline">\(f^{(l)}\)</span>, we have to start keeping track of a matrix as well as the values <span class="math inline">\(\mathbf{z}^{(l')}\)</span>. This matrix is initialized with components <span class="math inline">\(\phi'(\mathbf{z}^{(l)})\delta_{jk}\)</span>. It is then acted on by each of the Jacobians in <a href="#eq-jac" class="quarto-xref">Equation&nbsp;9</a> until we get to the final layer. This procedure is called <em>forward accumulation</em>.</p>
<p>The advantage of forward accumulation is that during evaluation we only have to store the current <span class="math inline">\(\mathbf{z}^{(l')}\)</span> and the corresponding matrix. The disadvantage is that we are dealing with <em>matrix multiplication</em>. For matrices <span class="math inline">\(M_1\in \mathbb{R}^{N_1\times N_2}\)</span> and <span class="math inline">\(M_2\in \mathbb{R}^{N_2\times N_3}\)</span> matrix multiplication <span class="math inline">\(M_1\cdot M_2\)</span> is <span class="math inline">\(O(N_1 N_2 N_3)\)</span>. Since we are interested in models with large numbers of parameters in each layer, this is a problem.</p>
</section>
<section id="backpropagation" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="backpropagation"><span class="header-section-number">2.3</span> Backpropagation</h2>
<p>The alternative is that we go from left to right. Instantly we see a problem: we have to have evaluate <em>and store</em> all the <span class="math inline">\(\mathbf{z}^{(l')}\)</span> with <span class="math inline">\(l'=1,\ldots L\)</span> before we can do anything, as the Jacobians depend on these values. This is called the <em>forward pass</em>.</p>
<p>Now remember that we’re <em>actually</em> interested in calculating <span class="math inline">\(\partial\mathcal{C}/\partial w^{(l)_k}\)</span>. For a single data point our cost function is</p>
<p><span id="eq-cost-single"><span class="math display">\[
C_i(\theta) \equiv \frac{1}{2}\lVert\mathbf{y}_i-\mathsf{NN}_\theta(\mathbf{x}_i)\rVert^2,
\tag{10}\]</span></span></p>
<p>and so</p>
<p><span class="math display">\[
\frac{\partial C_i}{\partial b^{(l)}_k} = -\left(\mathbf{y_i} - \mathsf{NN}_\theta(\mathbf{x}_i)\right) \cdot \frac{\partial \mathsf{NN}_\theta(\mathbf{x_i})}{\partial b^{(l)}_k}.
\]</span></p>
<p>This means that going from left to right involves only <em>matrix-vector</em> multiplications rather than matrix-matrix mutiplications. We start with the (row) vector <span class="math inline">\(\left(\mathbf{y_i} - \mathsf{NN}_\theta(\mathbf{x}_i)\right)^T\)</span> and act on the right with the Jacobians. This reduces the complexity of the evaluation by a factor equal to the number of biases in the <span class="math inline">\(l\)</span>th layer.</p>
<p>In AD this is known as <em>backward accumulation</em>. For the special case of neural networks it’s usually called <em>backpropagation</em>. Going backwards reduces the time complexity in favour additional space (i.e.&nbsp;memory) complexity, as we have to store <span class="math inline">\(\mathbf{z}^{l'}\)</span> for each layer. This trade-off is usually worth it, and essentially all large neural networks these days are trained using backpropagation.</p>
</section>
</section>
<section id="implementation" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Implementation</h1>
<p>These days, getting started with training neural networks is easy due to the availability of many libraries that implement the common building blocks of popular neural architectures, as well as the automatic differentiation required to train them by gradient descent. Popular libraries include <a href="https://pytorch.org/">PyTorch</a>, <a href="https://www.tensorflow.org/">TensorFlow</a>, and <a href="https://jax.readthedocs.io/en/latest/">Jax</a>. As usual, you don’t have to do it yourself.</p>
<p>Still, it’s fun to take a look at how backpropagation is actually implemented in code. Among the simple versions of backpropgation you can find online I can recommend <span class="citation" data-cites="nielsen2015neural">Nielsen (<a href="#ref-nielsen2015neural" role="doc-biblioref">2015</a>)</span> and <a href="https://github.com/karpathy/micrograd">micrograd</a> by Andrej Karpathy. He also has a <a href="https://www.youtube.com/watch?v=VMj-3S1tku0">YouTube video</a> where he explains how it works in detail.</p>
</section>
<section id="further-reading" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Further reading</h1>
<p>There are a huge number of resources online. For a really beautiful gentle introduction try <span class="citation" data-cites="nielsen2015neural">Nielsen (<a href="#ref-nielsen2015neural" role="doc-biblioref">2015</a>)</span>. For much more detail try <span class="citation" data-cites="goodfellow2016deep">Goodfellow, Bengio, and Courville (<a href="#ref-goodfellow2016deep" role="doc-biblioref">2016</a>)</span>, though the field moves so fast that the latter parts are probably already a bit dated.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-goodfellow2016deep" class="csl-entry" role="listitem">
Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. <em>Deep Learning</em>. MIT press.
</div>
<div id="ref-li2018visualizing" class="csl-entry" role="listitem">
Li, Hao, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. 2018. <span>“Visualizing the Loss Landscape of Neural Nets.”</span> <em>Advances in Neural Information Processing Systems</em> 31.
</div>
<div id="ref-nielsen2015neural" class="csl-entry" role="listitem">
Nielsen, Michael A. 2015. <em>Neural Networks and Deep Learning</em>. Vol. 25. Determination press San Francisco, CA, USA. <a href="http://neuralnetworksanddeeplearning.com">http://neuralnetworksanddeeplearning.com</a>.
</div>
<div id="ref-zhang2021understanding" class="csl-entry" role="listitem">
Zhang, Chiyuan, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. 2021. <span>“Understanding Deep Learning (Still) Requires Rethinking Generalization.”</span> <em>Communications of the ACM</em> 64 (3): 107–15.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>This is a slight exaggeration, but what people often do these days is augment symbolic approaches with neural approaches.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>The term <em>parameter</em> is normally reserved for the <span class="math inline">\(\theta\)</span>’s that appear in the definition of the model. Numbers like the learning rate that describe how the model is trained are usually referred to as <em>hyperparameters</em>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>GPT-3, the model from OpenAI underlying ChatGPT, has 175 <em>billion</em> parameters!<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Note that you don’t have to use the same activation function throughout. Typically the output layer of a network used for classification uses the <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax function</a> to output probabilities over the labels.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><a href="https://en.wikipedia.org/wiki/Spiking_neural_network">Spiking neural networks</a> are a model in which time plays a more serious role.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/computational-physics\.tripos\.org");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../notes/fourier.html" class="pagination-link" aria-label="Fast Fourier transform">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Fast Fourier transform</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../notes/linear.html" class="pagination-link" aria-label="Linear algebra">
        <span class="nav-page-text">Linear algebra</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>