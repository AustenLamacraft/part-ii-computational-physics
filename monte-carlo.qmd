# Monte Carlo methods

Many physical phenomena, notably those falling within the domains of statistical mechanics and quantum theory, depend in an essential way on _randomness_. The simulation of these phenomena therefore requires algorithms that incorporate random (or pseudo-random) elements in the most efficient way. 



## Sampling from a distribution

Let's suppose that we have a source of samples of a real valued random variable $X$ that follows a particular probability density function $p_X$ [^1] (more about where they might come from in @sec-rng). This means that the probability of drawing a sample in the region $[x, x+dx]$ is $p_X(x)dx$. If we now map the samples using a function $f$, what is the probability density $p_Y$ of $y=f(x)$? The new probability density is defined in just the same way: the probability of $y$ lying in the region $[y, y+dy]$ is $p_Y(y)dy$. Since $x$ is being mapped deterministically to $y$ these two probabilities are therefore the same

[^1]: A common shorthand notation is $x\sim p_X$.

$$
p_X(x)dx = p_Y(y)dy
$$

or

$$
p_Y(y)=p_X(x)\Bigg\lvert \frac{dx}{dy}\Bigg\rvert= \frac{p_X(x)}{|f'(x)|},\qquad x=f^{-1}(y)
$$

This formula shows that we can create samples from an arbitrary probability distribution by choosing an invertible map $f$ appropriately. If $p_X$ is a [standard uniform distribution](https://en.wikipedia.org/wiki/Continuous_uniform_distribution) on $[0,1]$ then $f(x)$ is the inverse of the cummulative probability distribution of $Y$ i.e.

$$
f^{-1}(y) = \int^y_{-\infty} p_Y(y')dy'
$$

The same approach works in higher dimensions: $\big\lvert \frac{dx}{dy}\big\rvert$ is replaced by the inverse of the Jacobian determinant. 

The [Box–Muller transform](https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform) is one example of this idea. Take two independent samples from a standard uniform distribution $u_{1,2}$ and form

$$
\begin{align}
x &= \sqrt{-2\log u_1}\cos(2\pi u_2)\\
y &= \sqrt{-2\log u_1}\sin(2\pi u_2).
\end{align}
$$

$x$ and $y$ are independent samples from a [standard normal distribution](https://en.wikipedia.org/wiki/Standard_normal_distribution). 

Various functions are available in the [`numpy.random`](https://numpy.org/doc/stable/reference/random/index.html#module-numpy.random) module to generate random arrays drawn from a variety of distributions. Box–Muller has now been retired in favour of the [Ziggurat algorithm](https://en.wikipedia.org/wiki/Ziggurat_algorithm).

```{python}
import numpy.random as random
import numpy as np
import matplotlib.pyplot as plt

mu, sigma = 0, 0.1 # mean and standard deviation
s = random.normal(mu, sigma, size=10000)
count, bins, ignored = plt.hist(s, 30, density=True)
plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *
               np.exp( - (bins - mu)**2 / (2 * sigma**2) ),
         linewidth=2, color='r')
plt.xlabel("Value")
plt.ylabel("Frequency")
plt.show()
```

For complex multivariate (i.e. high dimensional) distributions there is no general recipe to construct an appropriate $f$. One very recent application of these ideas is in machine learning models called [normalizing flows](https://arxiv.org/abs/1908.09257) that use a mapping $f$ parameterized by a neural network. The workhorse for sampling from complicated distributions is Markov chain Monte Carlo, as we discuss in @sec-mcmc.

## The Monte Carlo method

_Monte Carlo_ is the general prefix applied to variety of numerical methods that use randomness in some way. Two of the main classes of problem encountered in physics that come under this heading are:

1. Interpret a numerical evaluation as an expectation value of some random variable and use sampling to estimate it. [Monte Carlo integration](https://en.wikipedia.org/wiki/Monte_Carlo_integration) is an example of this idea.

2. Sampling from a complex probability distribution (which may include taking expectation values). Example: [Markov chain Monte Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo).

### Monte Carlo integration

The technique is exemplified by the following fairly dumb way of estimating $\pi$

```{python}
max_samples = 10000
inside = 0
areas = []
for sample in range(1, max_samples + 1):
    x = random.uniform(-1, 1)
    y = random.uniform(-1, 1)
    
    if x ** 2 + y ** 2 <= 1:
        inside += 1
    areas.append(4 * inside / sample)

plt.plot(np.arange(1, max_samples + 1), areas)
plt.plot(np.arange(1, max_samples + 1), np.pi * np.ones(max_samples), linestyle='dashed')
plt.show()
```

In terms of integration, you can think of this as a way to compute the integral of a function which is one inside the unit disc, and zero outside it.

Although it's a silly method, this does illustrate one important feature of Monte Carlo methods in general: that the relative error with $N$ samples is typically $\propto N^{-1/2}$ (thus at the 1% level for $10^4$ samples) because the variance of a sum of $N$ [iid](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) variables is $\propto N^{1/2}$.

Monte Carlo integration comes into its own for high dimensional problems. For low dimensional integrals the quadrature methods in [`scipy.integrate`](https://docs.scipy.org/doc/scipy/tutorial/integrate.html) are preferable.

### Markov chain Monte Carlo {#sec-mcmc}

Suppose you want to generate configurations at random (i.e. with a uniform distribution) from a "gas" of hard disks [^2]. 

[^2]: This is in fact the original motivation for the development of the technique, see @metropolis1953equation.

![Coins in a shoe box (gas of hard disks). From @krauth1998introduction](assets/hard-spheres.png)

It's harder than it looks! The first guess you might have is to start adding coins at random, and if you get an overlap, try again until you don't. Obviously this will become inefficient as the box fills up, and most attempts fail. _Worse, it doesn't in fact yield a uniform distribution!_ 

TODO Why not? See @widom1966random for an explanation

Here's an approach that works:

::: {#exm-metropolis}
1. Fix the number of disks and an initial starting configuration (some regular lattice configuration, say).
2. Pick a disk at random and attempt to move it by a small random amount.
3. If this results in the moved disk intersecting another, _reject_ the move, leaving the disk where it is. Otherwise, _accept_ the move.
4. Repeat 2. and 3. many times.
:::

![Accepted and rejected moves for hard disks. From @kapfer2013sampling](assets/metropolis.png).

This is the simplest example of the [Metropolis–Hastings algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm), the first Markov chain Monte Carlo (MCMC) algorithm. 

More generally, the goal of MCMC is to come up with a sequential random process (a __Markov chain__) that generates (usually after many steps) a sample from a particular distribution.

You've all heard of a [random walk](https://en.wikipedia.org/wiki/Random_walk), perhaps as a model for diffusion. At each step you make a move in a random direction, independently of your earlier moves. After many steps these random moves gives rise to a distribution of possible locations. A random walk is the simplest example of a Markov chain. 

More generally, a [Markov chain](https://en.wikipedia.org/wiki/Markov_chain) is a sequence of random variables $X_n$ with each having a distribution that is is conditional on the value of the previous one, and so is defined in terms of __transition probabilities__ $p(X_{n}=x_n|X_{n-1}=x_{n-1})$ (hence they form a "chain"). I'm going to immediately drop this cumbersome notation in favour of $p(x_n|x_{n-1})$, a function of $x_n$ and $x_{n-1}$, but in general the function giving the transition probabilities can be different at each step (the random variables could all be different).

The probability of a particular sequence $X_1=x_1\ldots X_n=x_n$ is therefore

$$
p(x_n|x_{n-1})p(x_{n-1}|x_{n-2})\cdots p(x_2|x_{1})p^{(1)}(x_1)
$$

$X_1$ has no "parent" so is not conditional on any other value. 

Suppose we don't care about the earlier values and just want to know the __marginal distribution__ $p^{(n)}(x_n)$ of the final variable. For a random walk this is easy, as $x_n$ typically represents a displacement that is a sum of iid increments. In general this is not the case, however, as the marginal distribution is

$$
p^{(n)}(x_n)=\sum_{x_{n-1},\ldots x_1}p(x_n|x_{n-1})p(x_{n-1}|x_{n-2})\cdots p(x_2|x_{1})p^{(1)}(x_1)
$$

(I'm writing all these expressions for discrete random variables, but the continuous version involving probability density functions is straightforward) 

The sums are over all possible values that the random variables might take in the __state space__ of the problem. These could be finite or infinite in number.

Things are not as bad as they appear, however, as the marginal distribution can be interpreted as the result of acting $n-1$ times on the vector of values of $p^{(1)}_j\equiv p^{(1)}(j)$ with the __transition matrix__ with elements $\mathsf{P}_{jk}=p(j|k)$

$$
\mathbf{p}^{(n)} = \mathsf{P}^{n-1}\mathbf{p}^{(1)}.
$$

In a single step the marginal probabilities are updated as

$$
\mathbf{p}^{(n)} = \mathsf{P}^{n}\mathbf{p}^{(n-1)}.
$$

$\mathsf{P}$ has some structure. The matrix elements are positive, as they represent probabilities, and each row sums to one

$$
\sum_j \mathsf{P}_{jk} = 1.
$$

Such matrices are called [stochastic](https://en.wikipedia.org/wiki/Stochastic_matrix).

Although $p^{(n)}$ – the probability distribution at the $n$th step – changes from step to step, you might expect that after many steps it tends to converge to a __stationary distribution__ $p^{(n)}\to\boldsymbol{\pi}$. If it exists, this distribution must satisfy

$$
\boldsymbol{\pi} = \mathsf{P}\boldsymbol{\pi}.
$${#eq-stat}  

In other words, it is an eigenvector of $\mathsf{P}$ with eigenvalue one. This property is guaranteed by the [Perron–Frobenius theorem](https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem) [^3]. 

[^3]: There is an important caveat. If there are two or more subsets of the state space that are not connected by finite transition probabilities, the probability distribution in each subset evolves independently and there is not a unique stationary distribution. When there _is_, we say that the Markov chain is __ergodic__ and the corresponding transition matrix is __irreducible__.

Thus $\mathsf{P}$ determines $\boldsymbol{\pi}$. MCMC turns this idea on its head and asks: if there is some $\boldsymbol{\pi}$ that I would like to generate samples from, can I find a $\mathsf{P}$ that has it as a stationary distribution? 

There is a trivial answer to this question. Sure, take $\mathsf{P}_{jk}=\boldsymbol{\pi}_j$. That is, jump straight to the stationary distribution no matter what the starting state. But we are interested in highly complicated distributions over large state spaces (think the Boltzmann distribution for a statistical mechanical system comprised of billions of particles). Thus what we really want is to be able to approach such a complicated distribution by making many transitions with _simple_ distributions. 

One more idea is useful before returning to concrete algorithms. The quantity 

$$
\mathsf{P}_{jk}\pi_k = p(j|k)\pi_k = p(j,k)
$$

is the joint distribution of seeing state $k$ followed by state $j$ in the stationary distribution. A _reversible_ Markov chain is one where $p(j,k)=p(k,j)$. Roughly, you can't tell the direction of time because any transition is equally likely to happen forward in time as backward. Random physical processes that respect time reversal symmetry are often modeled as reversible Markov processes. 

Combining reversibility with the definition of the stationary state yields the condition of [detailed balance](https://en.wikipedia.org/wiki/Detailed_balance)

$$
 \mathsf{P}_{jk}\pi_k = \pi_j\mathsf{P}_{kj}.
$$

This condition is stronger than the condition @eq-stat for a stationary state. This makes it easier to check: you don't have to do a sum over a state space index. The Metropolis algorithm @exm-metropolis for the hard disk problem is easily seen to satisfy detailed balance for a stationary distribution that is constant when disks don't intersect and zero when they do.

We'll see some more examples of MCMC algorithms for statistical mechanical problems in @sec-statmech.


### Relaxation to equilibrium

Eigenvalues

Master equation

Transition matrix

Marginalise over earlier variables

### Importance sampling

MC methods can be elaborated in lots of ways

## Statistical mechanics {#sec-statmech}

Original paper treated hard spheres:

https://aip.scitation.org/doi/10.1063/1.1699114

How to you get thermodynamics out of hard sphere samples?

Statistical mechanics is a natural source of such complex distributions in physics. Remember the fundamental principle that the probability of finding a statistical mechanical system in a microstate $x$ [^4] with energy $E(x)$ is

$$
p_X(x) = \frac{\exp(-\beta E(x))}{Z}
$$

where $Z$ is a normalizing constant called the partition function and $\beta=1/k_\text{B}T$, where $T$ is the temperature and $k_\text{B}$ is Boltzmann's constant.  

[^4]: For a classical gas of point particles this would correspond to specifying all the positions and velocities, for example.

Metropolis m

### The Ising model

Background on Markov processes. Transition kernel. Stochastic matrices

Example of SEP

Simplest examples...

How to estimate errors 



Some curve fitting here to extract something?

Google pagerank

MCMC in Bayesian inference

Relation to Ising models. Community detection. Why not?

https://arxiv.org/pdf/cond-mat/0005264.pdf

## Random number generators {#sec-rng}

Computers are deterministic 


This is a subject dealt with already

RNGs in Trebst?

Further reading: refer to [Krauth notes](https://arxiv.org/pdf/cond-mat/9612186.pdf) or book

Other suggestions from Twitter

https://roomno308.github.io/blog/MCMC.html
https://maximilianrohde.com/posts/code-breaking-with-metropolis/